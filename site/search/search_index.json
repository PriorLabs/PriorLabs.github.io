{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>OVERWRITE!</p>"},{"location":"aup/","title":"ACCEPTABLE USE POLICY","text":"<p>Effective Date: January 8<sup>th</sup> 2025</p> <p>This Acceptable Use Policy (\"AUP\") applies to the use of PriorLabs' Services. Where this AUP uses terms that are defined in the General Terms and Conditions (\"GTC\"), those terms shall have the meaning ascribed to them in the GTC.</p> <p>PriorLabs reserves the right to change this AUP in accordance with the GTC at https://www.priorlabs.ai/aup.</p>"},{"location":"aup/#1-what-type-of-activity-is-prohibited","title":"1. What type of activity is prohibited?","text":"<p>Customer shall not use, and encourage or allow any other person or entity to use the Services in prohibited manners, including but not limited to the following:</p> <ul> <li> <p>Customer may not upload any personal data within the meaning of the GDPR to the Contract Software or the Services.</p> </li> <li> <p>Customer may not upload any material to the Contract Software or the Services that infringes the intellectual property rights or other rights of third parties, including but not limited to trademarks, copyrights, trade secrets, rights of publicity, or otherwise violating, infringing or misappropriating the rights of any third party.</p> </li> <li> <p>Customer may not misappropriate, reverse-engineer, copy, disassemble, decompile, extract source code, trade secrets, or know-how, including PriorLabs' models, algorithms or artificial intelligence systems, or otherwise misuse or manipulate the Contract Software or Services or any part thereof.</p> </li> <li> <p>Customer may not use the Services or the Contract Software in a way that imposes an unreasonable or disproportionately large load on PriorLabs' infrastructure, which adversely impacting the availability, reliability or stability of PriorLabs' Services.</p> </li> <li> <p>Customer may not upload any viruses, spam, trojan horses, worms or any other malicious, harmful, or deleterious programs or code, including prompt-based manipulation or scraping behaviors, to the Contract Software or the Services.</p> </li> <li> <p>Customer may not attempt to use the Services and Contract Software in a manner that compromises, circumvents, or tests the vulnerability of any of PriorLabs' technical safeguards or other security measures.</p> </li> <li> <p>Customer may not use PriorLabs' Services or the Contract Software in any manner that may subject PriorLabs or any third party to liability, damages or danger.</p> </li> <li> <p>Customer shall not use the Contract Software improperly or allow it to be used improperly, and in particular shall not use or upload to the Contract Software any content that is illegal or immoral and/or such content that serves to incite hatred, hate speech, illicit deep fakes, or fake news, or incites criminal acts or glorifies or trivializes violence, is sexually offensive or pornographic, is capable of seriously endangering children or young people morally or impairing their well-being or may damage the reputation of PriorLabs, and shall not refer to such content.</p> </li> </ul> <p>This list of prohibited uses is provided by way of example and should not be considered exhaustive.</p>"},{"location":"aup/#2-who-is-prohibited-from-using-the-services","title":"2. Who is prohibited from using the Services?","text":"<p>Consumers within the meaning of Section 13 German Civil Code may not use PriorLabs' Services.</p>"},{"location":"cla/","title":"Contributor Agreement","text":""},{"location":"cla/#individual-contributor-exclusive-license-agreement","title":"Individual Contributor Exclusive License Agreement","text":""},{"location":"cla/#including-the-traditional-patent-license-option","title":"(including the Traditional Patent License OPTION)","text":"<p>Thank you for your interest in contributing to PriorLabs's TabPFN (\"We\" or \"Us\").</p> <p>The purpose of this contributor agreement (\"Agreement\") is to clarify and document the rights granted by contributors to Us. To make this document effective, please follow the instructions at https://www.priorlabs.ai/sign-cla.</p>"},{"location":"cla/#how-to-use-this-contributor-agreement","title":"How to use this Contributor Agreement","text":"<p>If You are an employee and have created the Contribution as part of your employment, You need to have Your employer approve this Agreement or sign the Entity version of this document. If You do not own the Copyright in the entire work of authorship, any other author of the Contribution should also sign this \u2013 in any event, please contact Us at noah.homa@gmail.com</p>"},{"location":"cla/#1-definitions","title":"1. Definitions","text":"<p>\"You\" means the individual Copyright owner who Submits a Contribution to Us.</p> <p>\"Contribution\" means any original work of authorship, including any original modifications or additions to an existing work of authorship, Submitted by You to Us, in which You own the Copyright.</p> <p>\"Copyright\" means all rights protecting works of authorship, including copyright, moral and neighboring rights, as appropriate, for the full term of their existence.</p> <p>\"Material\" means the software or documentation made available by Us to third parties. When this Agreement covers more than one software project, the Material means the software or documentation to which the Contribution was Submitted. After You Submit the Contribution, it may be included in the Material.</p> <p>\"Submit\" means any act by which a Contribution is transferred to Us by You by means of tangible or intangible media, including but not limited to electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Us, but excluding any transfer that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\"</p> <p>\"Documentation\" means any non-software portion of a Contribution.</p>"},{"location":"cla/#2-license-grant","title":"2. License grant","text":""},{"location":"cla/#21-copyright-license-to-us","title":"2.1 Copyright license to Us","text":"<p>Subject to the terms and conditions of this Agreement, You hereby grant to Us a worldwide, royalty-free, Exclusive, perpetual and irrevocable (except as stated in Section 8.2) license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul>"},{"location":"cla/#22-moral-rights","title":"2.2 Moral rights","text":"<p>Moral Rights remain unaffected to the extent they are recognized and not waivable by applicable law. Notwithstanding, You may add your name to the attribution mechanism customary used in the Materials you Contribute to, such as the header of the source code files of Your Contribution, and We will respect this attribution when using Your Contribution.</p>"},{"location":"cla/#23-copyright-license-back-to-you","title":"2.3 Copyright license back to You","text":"<p>Upon such grant of rights to Us, We immediately grant to You a worldwide, royalty-free, non-exclusive, perpetual and irrevocable license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul> <p>This license back is limited to the Contribution and does not provide any rights to the Material.</p>"},{"location":"cla/#3-patents","title":"3. Patents","text":""},{"location":"cla/#31-patent-license","title":"3.1 Patent license","text":"<p>Subject to the terms and conditions of this Agreement You hereby grant to Us and to recipients of Materials distributed by Us a worldwide, royalty-free, non-exclusive, perpetual and irrevocable (except as stated in Section 3.2) patent license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, to make, have made, use, sell, offer for sale, import and otherwise transfer the Contribution and the Contribution in combination with any Material (and portions of such combination). This license applies to all patents owned or controlled by You, whether already acquired or hereafter acquired, that would be infringed by making, having made, using, selling, offering for sale, importing or otherwise transferring of Your Contribution(s) alone or by combination of Your Contribution(s) with any Material.</p>"},{"location":"cla/#32-revocation-of-patent-license","title":"3.2 Revocation of patent license","text":"<p>You reserve the right to revoke the patent license stated in section 3.1 if We make any infringement claim that is targeted at your Contribution and not asserted for a Defensive Purpose. An assertion of claims of the Patents shall be considered for a \"Defensive Purpose\" if the claims are asserted against an entity that has filed, maintained, threatened, or voluntarily participated in a patent infringement lawsuit against Us or any of Our licensees.</p>"},{"location":"cla/#4-license-obligations-by-us","title":"4. License obligations by Us","text":"<p>We agree to license the Contribution only under the terms of the license or licenses that We are using on the Submission Date for the Material (including any rights to adopt any future version of a license).</p> <p>In addition, We may use the following licenses for Documentation in the Contribution: CC-BY-4.0, CC-BY-ND-4.0, CC-BY-NC-4.0, CC-BY-NC-ND-4.0, CC-BY-NC-SA-4.0, CC-BY-SA-4.0, CC0-1.0, MIT License, Apache License, GNU General Public License (GPL) v2.0, GNU General Public License (GPL) v3.0, GNU Affero General Public License v3.0, GNU Lesser General Public License (LGPL) v2.1, GNU Lesser General Public License (LGPL) v3.0, Mozilla Public License 2.0, Eclipse Public License 2.0, Microsoft Public License (Ms-PL), Microsoft Reciprocal License (Ms-RL), BSD 2-Clause \"Simplified\" or \"FreeBSD\" license, BSD 3-Clause \"New\" or \"Revised\" license (including any right to adopt any future version of a license).</p> <p>We agree to license patents owned or controlled by You only to the extent necessary to (sub)license Your Contribution(s) and the combination of Your Contribution(s) with the Material under the terms of the license or licenses that We are using on the Submission Date.</p>"},{"location":"cla/#5-disclaimer","title":"5. Disclaimer","text":"<p>THE CONTRIBUTION IS PROVIDED \"AS IS\". MORE PARTICULARLY, ALL EXPRESS OR IMPLIED WARRANTIES INCLUDING, WITHOUT LIMITATION, ANY IMPLIED WARRANTY OF SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT ARE EXPRESSLY DISCLAIMED BY YOU TO US AND BY US TO YOU. TO THE EXTENT THAT ANY SUCH WARRANTIES CANNOT BE DISCLAIMED, SUCH WARRANTY IS LIMITED IN DURATION AND EXTENT TO THE MINIMUM PERIOD AND EXTENT PERMITTED BY LAW.</p>"},{"location":"cla/#6-consequential-damage-waiver","title":"6. Consequential damage waiver","text":"<p>TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL YOU OR WE BE LIABLE FOR ANY LOSS OF PROFITS, LOSS OF ANTICIPATED SAVINGS, LOSS OF DATA, INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL AND EXEMPLARY DAMAGES ARISING OUT OF THIS AGREEMENT REGARDLESS OF THE LEGAL OR EQUITABLE THEORY (CONTRACT, TORT OR OTHERWISE) UPON WHICH THE CLAIM IS BASED.</p>"},{"location":"cla/#7-approximation-of-disclaimer-and-damage-waiver","title":"7. Approximation of disclaimer and damage waiver","text":"<p>IF THE DISCLAIMER AND DAMAGE WAIVER MENTIONED IN SECTION 5. AND SECTION 6. CANNOT BE GIVEN LEGAL EFFECT UNDER APPLICABLE LOCAL LAW, REVIEWING COURTS SHALL APPLY LOCAL LAW THAT MOST CLOSELY APPROXIMATES AN ABSOLUTE WAIVER OF ALL CIVIL OR CONTRACTUAL LIABILITY IN CONNECTION WITH THE CONTRIBUTION.</p>"},{"location":"cla/#8-term","title":"8. Term","text":"<p>8.1 This Agreement shall come into effect upon Your acceptance of the terms and conditions.</p> <p>8.2 This Agreement shall apply for the term of the copyright and patents licensed here. However, You shall have the right to terminate the Agreement if We do not fulfill the obligations as set forth in Section 4. Such termination must be made in writing.</p> <p>8.3 In the event of a termination of this Agreement Sections 5, 6, 7, 8 and 9 shall survive such termination and shall remain in full force thereafter. For the avoidance of doubt, Free and Open Source Software (sub)licenses that have already been granted for Contributions at the date of the termination shall remain in full force after the termination of this Agreement.</p>"},{"location":"cla/#9-miscellaneous","title":"9. Miscellaneous","text":"<p>9.1 This Agreement and all disputes, claims, actions, suits or other proceedings arising out of this agreement or relating in any way to it shall be governed by the laws of Germany excluding its private international law provisions.</p> <p>9.2 This Agreement sets out the entire agreement between You and Us for Your Contributions to Us and overrides all other agreements or understandings.</p> <p>9.3 In case of Your death, this agreement shall continue with Your heirs. In case of more than one heir, all heirs must exercise their rights through a commonly authorized person.</p> <p>9.4 If any provision of this Agreement is found void and unenforceable, such provision will be replaced to the extent possible with a provision that comes closest to the meaning of the original provision and that is enforceable. The terms and conditions set forth in this Agreement shall apply notwithstanding any failure of essential purpose of this Agreement or any limited remedy to the maximum extent possible under law.</p> <p>9.5 You agree to notify Us of any facts or circumstances of which you become aware that would make this Agreement inaccurate in any respect.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>Put out project that people could contribute to and provide instructions for contributing</p>"},{"location":"docs/","title":"","text":"<p>PriorLabs is building breakthrough foundation models that understand spreadsheets and databases. While foundation models have transformed text and images, tabular data has remained largely untouched. We're tackling this opportunity with technology that could revolutionize how we approach scientific discovery, medical research, financial modeling, and business intelligence.</p>"},{"location":"docs/#why-tabpfn","title":"Why TabPFN","text":"<ul> <li> <p> Rapid Training</p> <p>TabPFN significantly reduces training time, outperforming traditional models tuned for hours in just a few seconds. For instance, it surpasses an ensemble of the strongest baselines in 2.8 seconds compared to 4 hours of tuning.</p> </li> <li> <p> Superior Accuracy</p> <p>TabPFN consistently outperforms state-of-the-art methods like gradient-boosted decision trees (GBDTs) on datasets with up to 10,000 samples. It achieves higher accuracy and better performance metrics across a range of classification and regression tasks.</p> </li> <li> <p> Robustness</p> <p>The model demonstrates robustness to various dataset characteristics, including uninformative features, outliers, and missing values, maintaining high performance where other methods struggle.</p> </li> <li> <p> Generative Capabilities</p> <p>As a generative transformer-based model, TabPFN can be fine-tuned for specific tasks, generate synthetic data, estimate densities, and learn reusable embeddings. This makes it versatile for various applications beyond standard prediction tasks.</p> </li> <li> <p> Sklearn Interface</p> <p>TabPFN follows the interfaces provided by scikit-learn, making it easy to integrate into existing workflows and utilize familiar functions for fitting, predicting, and evaluating models.</p> </li> <li> <p> Minimal Preprocessing</p> <p>The model handles various types of raw data, including missing values and categorical variables, with minimal preprocessing. This reduces the burden on users to perform extensive data preparation.</p> </li> </ul>"},{"location":"docs/#tabpfn-integrations","title":"TabPFN Integrations","text":"<ul> <li> <p> API Client</p> <p>The fastest way to get started with TabPFN. Access our models through the cloud without requiring local GPU resources.</p> </li> <li> <p> User Interface</p> <p>Visual interface for no-code interaction with TabPFN. Perfect for quick experimentation and visualization.</p> <p> Access GUI</p> </li> <li> <p> Python Package</p> <p>Coming soon! Local installation with GPU support and scikit-learn compatible interface.</p> </li> <li> <p> R Integration</p> <p>Currently in development. Bringing TabPFN's capabilities to the R ecosystem for data scientists and researchers.</p> </li> </ul> <p> </p>"},{"location":"enterprise/","title":"TabPFN Business","text":"<p>Unlock the hidden value in your company's databases and spreadsheets using TabPFN. Our state-of-the-art tabular foundation model is faster and more accurate in 96% of the use-cases and requires 50% less data as previous methods.</p> <p>Save your data science team hours &amp; days of work and enable them to focus on mission-critical business problems, even when data availability is limited.</p>"},{"location":"enterprise/#why-tabpfn-business","title":"Why TabPFN Business?","text":""},{"location":"enterprise/#access-to-enterprise-grade-features","title":"Access to Enterprise-Grade Features","text":"<ul> <li>Priority Access: Get early access to new features and model improvements with support for larger datasets</li> <li>Custom Features: Direct access to features designed specifically for business users such as fine-tuning, tech stack integrations and data sharing across your organization</li> <li>Dedicated Support: Premium SLA support from our scientists and engineers on implemenation, training and critical issues</li> <li>Compliance: Enterprise-level GDPR compliance including audit logs and role based access control</li> </ul> Get In Touch Email Address * First Name * Last Name * Company size * 1-49 50-249 250-999 1000+ Company Website * What are you interested in? *<ul> <li>I want to use Prior Labs products for my business</li> <li>I have a press/event request</li> <li>Other</li> </ul> Marketing Permissions <p>Please select all the ways you would like to hear from PriorLabs:</p> Email <p>You can unsubscribe at any time by clicking the link in the footer of our emails.</p> <p>We use Mailchimp as our marketing platform. By clicking below to subscribe, you acknowledge that your information will be transferred to Mailchimp for processing. Learn more about Mailchimp's privacy practices.</p>"},{"location":"newsletter/","title":"Stay Updated with TabPFN","text":"<p>Join our newsletter to get the latest updates on TabPFN's development, best practices, and breakthrough research in tabular machine learning.</p>"},{"location":"newsletter/#what-youll-get","title":"What You'll Get","text":"<ul> <li>Early Access: Be the first to know about new releases and features</li> <li>Technical Insights: Deep dives into TabPFN's architecture and capabilities</li> <li>Best Practices: Tips and tricks for getting the most out of TabPFN</li> <li>Research Updates: Latest developments in tabular foundation models</li> <li>Community Highlights: Featured projects and use cases from our community</li> </ul> Email Address * Marketing Permissions <p>Please select all the ways you would like to hear from PriorLabs:</p> Email <p>You can unsubscribe at any time by clicking the link in the footer of our emails.</p> <p>We use Mailchimp as our marketing platform. By clicking below to subscribe, you acknowledge that your information will be transferred to Mailchimp for processing. Learn more about Mailchimp's privacy practices.</p>"},{"location":"privacy_policy/","title":"PRIVACY POLICY","text":"<p>Last updated: January 8<sup>th</sup>, 2025</p>"},{"location":"privacy_policy/#1-general-information","title":"1. General information","text":"<p>Prior Labs GmbH, Elisabeth-Emter-Weg 18, 79110 Freiburg im Breisgau (hereinafter \"PriorLabs\", \"we\" or \"us\") takes the protection of personal data very seriously.</p> <p>We treat personal data confidentially and always in accordance with the applicable data protection laws, in particular Regulation (EU) 2016/679 (hereinafter \"General Data Protection Regulation\" or \"GDPR\"), the German Federal Data Protection Act (hereinafter \"BDSG\"), and in accordance with the provisions of this privacy policy.</p> <p>The aim of this privacy policy is to inform you (hereinafter \"data subject\" or \"you\") in accordance with Art. 12 et seq. GDPR about how we process your personal data and for what purposes we process your personal data when using our website https://priorlabs.ai/ (hereinafter \"Website\"), our services or contacting us.</p> <p>Unless otherwise stated in this privacy policy, the terms used here have the meaning as defined in the GDPR.</p>"},{"location":"privacy_policy/#2-data-controller","title":"2. Data controller","text":"<p>PriorLabs acts as a controller within the meaning of the GDPR in relation to your personal data processed in connection with the use of our Website, Service or a contact made to or by PriorLabs.</p> <p>If you have any questions about this privacy policy or the processing of your personal data, you can contact us at the following contact details:</p> <p>Prior Labs GmbH Elisabeth-Emter-Weg 18 79110 Freiburg im Breisgau E-mail: dataprotection@priorlabs.ai</p> <p>Categories, purposes and legal bases of the personal data processed</p> <p>We process different categories of your personal data for different purposes. Below you can see which data we process in which contexts, for which purposes and on which legal basis we base the respective processing.</p>"},{"location":"privacy_policy/#1-visiting-our-website","title":"1. Visiting our Website","text":"<p>When visiting our Website for informational purposes, i.e., mere viewing and without you providing us with any other information, certain personal data is automatically collected each time the Website are called up and stored in so-called server log files. These are:</p> <ul> <li> <p>Browser type and version. The specific type and model of Internet browser you are using, such as Google Chrome, Mozilla Firefox, or Microsoft Edge, along with the specific version of the browser.</p> </li> <li> <p>Operating system used. Your operating system for your digital activity, such as Windows, macOS, Linux, iOS, or Android.</p> </li> <li> <p>Host name of the accessing computer. The unique name that your device has on the Internet or on a local network.</p> </li> <li> <p>The date and time of access. The exact time of access to the Website.</p> </li> <li> <p>IP address of the requesting computer. The unique numeric identifier assigned to a device when it connects to the Internet.</p> </li> </ul> <p>Such data is not merged with other data sources, and the data is not evaluated for marketing purposes.</p> <p>Legal basis:</p> <p>The legal basis for the temporary storage and processing of such personal data is Art. 6 para. 1 sent. 1 lit. f GDPR. Our legitimate interest here is to be able to provide you with technically functional, attractive and user-friendly Website as well as to ensure the security of our systems.</p> <p>Duration of the storage:</p> <p>Such personal data will be deleted as soon as it is no longer required to achieve the purpose for which it was collected. For personal data stored in log files, this is the case after [7] days at the latest.</p> <p>However, in some cases, e.g., due to legal retention periods we might be under the legal obligation to continue the storage of your personal data.</p>"},{"location":"privacy_policy/#2-use-of-our-services","title":"2. Use of our Services","text":"<p>We provide you with a software to TabPFN foundation models in the context of the analysis, processing and evaluation of tabular business data (\"Services\"). Please note our Acceptable Use Policy which strictly prohibits the upload of personal data to use our Services.</p> <p>Although, you are not allowed to upload (tabular) personal data to have them analyzed, processed and evaluated, we are processing certain personal data when you are accessing our Services via our API.</p>"},{"location":"privacy_policy/#a-user-account","title":"a) User account","text":"<p>When you register your user account, we process the following personal data:</p> <ul> <li>First and last name</li> <li>E-mail address</li> <li>Organization and role</li> <li>Password</li> </ul> <p>Legal basis:</p> <p>We process the aforementioned information to create your user account and, thus, such data will be processed for the performance of a contract or in order to take steps prior to entering into a contract in accordance with Art. 6 para. 1 sent. 1 lit. b GDPR.</p> <p>Duration of the storage:</p> <p>You can delete your user account at any time by sending an e-mail with your request to dataprotection@priorlabs.ai. We will delete your user account when it has been inactive for 3 years.</p>"},{"location":"privacy_policy/#b-usage-data","title":"b) Usage data","text":"<p>When you use our service, we process certain personal data about how you use it and the device you use to access it. We process the following usage data in the form of log files:</p> <ul> <li> <p>IP address of the requesting computer. The unique numeric identifier assigned to a device when it connects to the Internet.</p> </li> <li> <p>Browser type and version. The specific type and model of Internet browser you are using, such as Google Chrome, Mozilla Firefox, or Microsoft Edge, along with the specific version of the browser.</p> </li> <li> <p>Operating system used. Your operating system for your digital activity, such as Windows, macOS, Linux, iOS, or Android.</p> </li> <li> <p>The date and time of access. The exact time of access to the Website.</p> </li> <li> <p>Host name of the accessing computer. The unique name that your device has on the Internet or on a local network.</p> </li> </ul> <p>The processing of this data is used for the technical provision of our services and their contents, as well as to optimise their usability and ensure the security of our information technology systems.</p> <p>Legal basis:</p> <p>The legal basis for the temporary storage and processing of such personal data is Art. 6 para. 1 sent. 1 lit. f GDPR. Our legitimate interest here is the technical provision of our services and their contents, as well as to optimise their usability and ensure the security of our information technology systems to be able to provide you with technically functional, attractive and user-friendly Website as well as to ensure the security of our systems.</p> <p>Duration of the storage:</p> <p>Such personal data will be deleted as soon as it is no longer required to achieve the purpose for which it was collected. For personal data stored in log files, this is the case after [7] days at the latest.</p> <p>However, in some cases, e.g., due to legal retention periods we might be under the legal obligation to continue the storage of your personal data.</p>"},{"location":"privacy_policy/#3-contact","title":"3. Contact","text":"<p>It is possible to contact us on our Website by e-mail. When you contact us, we collect and process certain information in connection with your specific request, such as, e.g., your name, e-mail address, and other data requested by us or data you voluntarily provide to us (hereinafter \"Contact Data\").</p> <p>Legal basis:</p> <p>If you contact us as part of an existing contractual relationship or contact us in advance for information about our range of services, the Contact Data will be processed for the performance of a contract or in order to take steps prior to entering into a contract and to respond to your contact request in accordance with Art. 6 para. 1 sent. 1 lit. b GDPR.</p> <p>Otherwise, the legal basis for the processing of Contact Data is Art. 6 para. 1 sent. 1 lit. f GDPR. The Contact Data is processed to pursue our legitimate interests in responding appropriately to customer/contact inquiries.</p> <p>Duration of storage:</p> <p>We will delete Contact Data as soon as the purpose for data storage and processing no longer applies (e.g., after your request has been processed).</p> <p>However, in some cases, e.g., due to legal retention periods we might be under the legal obligation to continue the storage of your personal data.</p>"},{"location":"privacy_policy/#4-newsletter","title":"4. Newsletter","text":"<p>With your consent, we may process your personal data to send you a newsletter via e-mail that contains information about our products and services. To send you the newsletter, we require processing your e-mail address, date and time of your registration, your IP address and browser type.</p> <p>[Our newsletters contain so-called tracking links that enable us to analyze the behavior of newsletter recipients. We may collect personal data such as regarding the opening of the newsletter (date and time), selected links, and the following information of the accessing computer system: IP address used, browser type and version, device type and operating system (\"Tracking Data\"). This enables us to statistically analyze the success or failure of online marketing campaigns.]</p> <p>Legal basis:</p> <p>The data processing activities with regard to the newsletter sending and newsletter tracking only take place if and insofar as you have expressly consented to it within the merits of Article 6 para. 1 sent. 1 lit. a GDPR. Your prior consent for such processing activities is obtained during the newsletter subscription process (double opt-in) by way of independent consent declaration referring to this privacy policy.</p> <p>You can revoke your consent at any time with effect for the future by clicking on the unsubscribe link in e-mails. The withdrawal of your consent does not affect the lawfulness of processing based on your consent before its withdrawal.</p> <p>Duration of storage:</p> <p>We will delete your personal data as soon as the purpose for data storage and processing no longer applies. Your e-mail address will be stored for as long as the subscription to our newsletter is active.</p> <p>However, in some cases, e.g., due to legal retention periods, we might be under the legal obligation to continue the storage of your personal data.</p>"},{"location":"privacy_policy/#5-social-media-and-professional-networks-and-platforms","title":"5. Social media and professional networks and platforms","text":"<p>We utilize the possibility of company appearances on social and professional networks and platforms (LinkedIn, Github, X and Discord) in order to be able to communicate with you and to inform you about our services and news about us.</p> <p>You can, inter alia, access the respective network or platform by clicking on the respective network icon displayed on our Website, which includes a hyperlink. A hyperlink activated by clicking on it opens the external destination in a new browser window of your browser. No personal data is transferred to the respective network before this activation.</p>"},{"location":"privacy_policy/#a-visiting-our-page-on-social-media-and-professional-networks-and-platforms","title":"a) Visiting our page on social media and professional networks and platforms","text":"<p>The respective aforementioned network or platform is, in principle, solely responsible for the processing of personal data when you visit our company page on one of those networks or platforms.</p> <p>Please do not contact us via one of the networks or platforms if you wish to avoid this. You use such networks and platforms and their functions on your own responsibility.</p>"},{"location":"privacy_policy/#b-communication-via-social-media-and-professional-networks-and-platforms","title":"b) Communication via social media and professional networks and platforms","text":"<p>We process information that you have made available to us via our company page on the respective network or platform, e.g., your (user) name, e-mail address, contact details, communication content, job title, company name, industry, education, contact options, photo, and other data you voluntarily provide to us. The (user) names of the registered network or platform users who have visited our company page on the networks or platforms may be visible to us.</p> <p>Legal basis:</p> <p>If you contact us as part of an existing contractual relationship or contact us in advance for information about our range of services, the personal data will be processed for the performance of a contract or in order to take steps prior to entering into a contract and to respond to your contact request in accordance with Art. 6 para. 1 sent. 1 lit. b GDPR.</p> <p>Otherwise, the legal basis for the processing of the personal data is Art. 6 para. 1 sent. 1 lit. f GDPR. The personal data is processed to pursue our legitimate interests in responding appropriately to customer/contact inquiries.</p> <p>Duration of storage:</p> <p>We will delete your personal data as soon as the purpose for data storage and processing no longer applies (e.g., after your request has been processed).</p> <p>However, in some cases, e.g., due to legal retention periods we might be under the legal obligation to continue the storage of your personal data.</p>"},{"location":"privacy_policy/#4-data-receiver","title":"4. Data receiver","text":"<p>We might transfer your personal data to certain data receivers if such transfer is necessary to fulfill our contractual and legal obligations.</p> <p>In individual cases, we transfer personal data to our consultants in legal or tax matters, whereby these recipients act independently in their own data protection responsibilities and are also obliged to comply with the requirements of the GDPR and other applicable data protection regulations. In addition, they are bound by special confidentiality and secrecy obligations due to their professional position.</p> <p>In the event of corporate transactions (e.g., sale of our business or a part of it), we may transfer personal data to involved advisors or to potential buyers.</p> <p>Additionally, we also use services provided by various specialized companies, e.g., IT service providers, that process data on our behalf (hereinafter \"Data Processors\"). We have concluded a data processing agreement according to Art. 28 GDPR or EU standard contractual clauses of the EU Commission pursuant to Art. 46 para. 2 lit. c GDPR with each service provider and they only process data in accordance with our instructions and not for their own purposes.</p> <p>Our current Data Processors are:</p> Data Processor Purpose of commissioning the Data Processor / purpose of processing OpenAI Processing text inputs to our model API Mailchimp Newsletter Signup Google Usage analytics"},{"location":"privacy_policy/#5-data-transfers-to-third-countries","title":"5. Data transfers to third countries","text":"<p>Your personal data is generally processed in Germany and other countries within the European Economic Area (EEA).</p> <p>However, it may also be necessary for personal data to be transferred to recipients located outside the EEA, i.e., to third countries, such as the USA. If possible, we conclude the currently applicable EU standard contractual clauses of the EU Commission pursuant to Art. 46 para. 2 lit. c GDPR with all processors located outside the EEA. Otherwise, we ensure that a transfer only takes place if an adequacy decision exists with the respective third country and the recipient is certified under this, if necessary. We will provide you with respective documentation on request.</p>"},{"location":"privacy_policy/#6-your-rights","title":"6. Your rights","text":"<p>The following rights are available to you as a data subject in accordance with the provisions of the GDPR:</p>"},{"location":"privacy_policy/#1-right-of-revocation","title":"1. Right of revocation","text":"<p>You may revoke your consent to the processing of your personal data at any time pursuant to Art. 7 para. 3 GDPR. Please note, that the revocation is only effective for the future. Processing that took place before the revocation remains unaffected.</p>"},{"location":"privacy_policy/#2-right-of-access","title":"2. Right of access","text":"<p>Under the conditions of Art. 15 GDPR you have the right to request confirmation from us at any time as to whether we are processing personal data relating to you. If this is the case, you also have the right within the scope of Art. 15 GDPR to receive access to the personal data as well as certain other information about the personal data and a copy of your personal data. The restrictions of \u00a7 34 BDSG apply.</p>"},{"location":"privacy_policy/#3-right-to-rectification","title":"3. Right to rectification","text":"<p>Under the conditions of Art. 16 GDPR you have the right to request us to correct the personal data stored about you if it is inaccurate or incomplete.</p>"},{"location":"privacy_policy/#4-right-to-erasure","title":"4. Right to erasure","text":"<p>You have the right, under the conditions of Art. 17 GDPR, to demand that we delete the personal data concerning you without delay.</p>"},{"location":"privacy_policy/#5-right-to-restrict-processing","title":"5. Right to restrict processing","text":"<p>You have the right to request that we restrict the processing of your personal data under the conditions of Art. 18 GDPR.</p>"},{"location":"privacy_policy/#6-right-to-data-portability","title":"6. Right to data portability","text":"<p>You have the right, under the conditions of Art. 20 GDPR, to request that we hand over, in a structured, common and machine-readable format, the personal data concerning you that you have provided to us. Please note that this right only applies where the processing is based on your consent, or a contract and the processing is carried out by automated means.</p>"},{"location":"privacy_policy/#7-right-to-object","title":"7. Right to object","text":"<p>You have the right to object to the processing of your personal data under the conditions of Art. 21 GDPR.</p>"},{"location":"privacy_policy/#8-right-to-complain-to-a-supervisory-authority","title":"8. Right to complain to a supervisory authority","text":"<p>Subject to the requirements of Art. 77 GDPR, you have the right to file a complaint with a competent supervisory authority. As a rule, the data subject may contact the supervisory authority of his or her habitual residence or place of work or place of the alleged infringement or the registered office of PriorLabs. The supervisory authority responsible for PriorLabs is the State Commissioner for Data Protection and Freedom of Information for Baden-W\u00fcrttemberg. A list of all German supervisory authorities and their contact details can be found here.</p>"},{"location":"privacy_policy/#7-obligation-to-provide-data","title":"7. Obligation to provide data","text":"<p>When you visit our Website, you may be required to provide us with certain personal data as described in this privacy policy. Beyond that, you are under no obligation to provide us with personal data. However, if you do not provide us with your personal data as required, you may not be able to contact us and/or we may not be able to contact you to respond to your inquiries or questions.</p>"},{"location":"privacy_policy/#8-automated-decisionsprofiling","title":"8. Automated decisions/profiling","text":"<p>The processing of your personal data carried out by us does not contain any automated decisions in individual cases within the meaning of Art. 22 para. 1 GDPR.</p>"},{"location":"privacy_policy/#9-changes-to-this-privacy-policy","title":"9. Changes to this privacy policy","text":"<p>We review this privacy policy regularly and may update it at any time. If we make changes to this privacy policy, we will change the date of the last update above. Please review this privacy policy regularly to be aware of any updates. The current version of this privacy policy can be accessed at any time at https://priorlabs.ai/.</p>"},{"location":"tabpfn-nature/","title":"Highly Accurate Predictions for Small Data With the Tabular Foundation Model TabPFN","text":"<p>Warning</p> <p>This is a beta version of our documentation created for the review phase. Please do not share this review version.</p> <p>Our code is currently stored in a private repository on GitHub We do not share this links on this public website. To access our code and any example notebooks, please use the notebook provided in the links to our submission via the link in our main paper or the code submission checklist. </p> <p>This page contains usage examples and installation instructions of TabPFN. Please find additional instructions on our Classifiers and Regressors on the respective subpages. An in-depth technical documentation of our software interfaces can be found in the API Reference</p>"},{"location":"tabpfn-nature/#installation","title":"Installation","text":"<p>To install our software, we use pip the python package installer in combination with Git for code-management. Please find the code for installation via the private link shared with you, that also contains the private access tokens to the code. An installation typically takes 5 minutes in a setup python environment. </p> <p>Tip</p> <p>The easiest way to install and run our code is via the Colab Notebooks shared in the link in our submission.</p>"},{"location":"tabpfn-nature/#software-dependencies-and-operating-systems","title":"Software Dependencies and Operating Systems","text":"<p>Python: Version &gt;= 3.9</p> <p>Operating Systems: The software has been tested on major operating systems including:</p> <ul> <li> <p>Ubuntu 20.04, 22.04</p> </li> <li> <p>Windows 10, 11</p> </li> <li> <p>macOS 11.0 (Big Sur) and later</p> </li> </ul> <p>Git Version 2 or later (https://git-scm.com/)</p>"},{"location":"tabpfn-nature/#software-dependencies-as-specified-in-requirementstxt","title":"Software Dependencies (as specified in <code>requirements.txt</code>):","text":"TabPFNTabPFN and Baselines <pre><code>torch&gt;=2.1 (Includes CUDA support in version 2.1 and later)\nscikit-learn&gt;=1.4.2\ntqdm&gt;=4.66.\nnumpy&gt;=1.21.2\nhyperopt==0.2.7 (Note: Earlier versions fail with numpy number generator change)\npre-commit&gt;=3.3.3\neinops&gt;=0.6.0\nscipy&gt;=1.8.0\ntorchmetrics==1.2.0\npytest&gt;=7.1.3\npandas[plot,output_formatting]&gt;=2.0.3,&lt;2.2 (Note: Version 2.2 has a bug with multi-index tables (https://github.com/pandas-dev/pandas/issues/57663), recheck when fixed)\npyyaml&gt;=6.0.1\nkditransform&gt;=0.2.0\n</code></pre> <pre><code>torch&gt;=2.1 (Includes CUDA support in version 2.1 and later)\nscikit-learn&gt;=1.4.2\ntqdm&gt;=4.66.\nnumpy&gt;=1.21.2\nhyperopt==0.2.7 (Note: Earlier versions fail with numpy number generator change)\npre-commit&gt;=3.3.3\neinops&gt;=0.6.0\nscipy&gt;=1.8.0\ntorchmetrics==1.2.0\npytest&gt;=7.1.3\npandas[plot,output_formatting]&gt;=2.0.3,&lt;2.2 (Note: Version 2.2 has a bug with multi-index tables (https://github.com/pandas-dev/pandas/issues/57663), recheck when fixed)\npyyaml&gt;=6.0.1\nkditransform&gt;=0.2.0\nseaborn==0.12.2\nopenml==0.14.1\nnumba&gt;=0.58.1\nshap&gt;=0.44.1\n\n# Baselines\nlightgbm==3.3.5\nxgboost&gt;=2.0.0\ncatboost&gt;=1.1.1\n#auto-sklearn==0.14.5\n#autogluon==0.4.0\n\n# -- Quantile Baseline\nquantile-forest==1.2.4\n</code></pre> <p>For GPU usage CUDA 12.1 has been tested.</p>"},{"location":"tabpfn-nature/#non-standard-hardware","title":"Non-Standard Hardware","text":"<p>GPU: A CUDA-enabled GPU is recommended for optimal performance, though the software can also run on a CPU.</p>"},{"location":"tabpfn-nature/#example-usage","title":"Example usage","text":"ClassificationRegression <pre><code>import numpy as np\nimport sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nfrom tabpfn import TabPFNClassifier\n\n# Create a classifier\nclf = TabPFNClassifier(fit_at_predict_time=True)\n\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nclf.fit(X_train, y_train)\npreds = clf.predict_proba(X_test)\ny_eval = np.argmax(preds, axis=1)\n\nprint('ROC AUC: ', sklearn.metrics.roc_auc_score(y_test, preds, multi_class='ovr'), 'Accuracy', sklearn.metrics.accuracy_score(y_test, y_eval))\n</code></pre> <pre><code>from tabpfn import TabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = TabPFNRegressor(device='auto')\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre>"},{"location":"tabpfn-nature/#expected-output","title":"Expected Output","text":"<p>Our models follow the interfaces provided by sklearn, so you can expect the same output as you would from sklearn models. TabPFNClassifier will return a numpy array of shape <code>(n_samples, n_classes)</code> with the probabilities of each class, while TabPFNRegressor will return a numpy array of shape <code>(n_samples,)</code> with the predicted values. For more detailed documentation please check the technical documentation of scripts.estimator.TabPFNClassifier.predict_proba.</p>"},{"location":"tabpfn-nature/#expected-runtime","title":"Expected Runtime","text":"<p>The runtime of the model is dependent on the number of estimators and the size of the dataset. For a dataset of 1000 samples and 4 features, the runtime on GPU is typically less than 1 second. For a dataset of 10000 samples and 4 features, the runtime on GPU is typically less than 10 seconds.</p>"},{"location":"terms-eu-en/","title":"GENERAL TERMS AND CONDITIONS","text":""},{"location":"terms-eu-en/#1-scope-of-application","title":"1. Scope of Application","text":"<ol> <li> <p>These general terms and conditions (\"GTC\") govern the provision of access to the TabPFN foundation models as available at https://www.priorlabs.ai (\"Services\") provided by Prior Labs GmbH, Elisabeth-Emter-Weg 18, 79110 Freiburg im Breisgau (\"PriorLabs\").</p> </li> <li> <p>The Services of PriorLabs are directed exclusively at business customers (Unternehmer) within the meaning of Sec. 14 German Civil Code (B\u00fcrgerliches Gesetzbuch, BGB) (\"Customer\"). PriorLabs may require the Customer to provide sufficient proof of its status as business customer prior to the conclusion of the contract.</p> </li> <li> <p>Conflicting or additional contractual conditions of the Customer shall only apply if PriorLabs expressly confirms them in writing.</p> </li> </ol>"},{"location":"terms-eu-en/#2-conclusion-of-contract","title":"2. Conclusion of Contract","text":"<ol> <li> <p>The contract is concluded with the inclusion of these GTC (\"Contract\") at the earliest of (i) when the Customer registers and sets up an account via the Services (\"PriorLabs Account\") or (ii) when the Customer first uses the Services.</p> </li> <li> <p>Upon conclusion of the Contract, the Customer shall provide PriorLabs with all information that PriorLabs reasonably requires in order to provide the Services correctly and completely. The Customer is obliged to inform PriorLabs immediately of any relevant changes.</p> </li> </ol>"},{"location":"terms-eu-en/#3-registration-and-priorlabs-account","title":"3. Registration and PriorLabs Account","text":"<ol> <li> <p>The Customer agrees by registering or using a PriorLabs Account and represents that they created their PriorLabs Account, and they will use their PriorLabs Account only for themselves. Each Customer shall register only one PriorLabs Account. A PriorLabs Account is not transferable.</p> </li> <li> <p>If and to the extent, PriorLabs stores Customer's data, PriorLabs disclaims any liability for the storage, accessibility, or integrity of such data.</p> </li> <li> <p>The Customer is obliged (i) to provide complete and correct information about its person or entity at the time of registration and (ii) in case of respective changes to correct without undue delay this information insofar such information is mandatory for the performance of the Contract.</p> </li> <li> <p>If PriorLabs receives a notice or otherwise has reason to believe that the information or documents provided by the Customer are wholly or partially incorrect, incomplete or not up to date, PriorLabs is entitled to request the Customer to remedy the situation immediately. If the Customer fails to correct or complete the information or document within the set deadline, PriorLabs is entitled to restrict access to the Services and block the Customer until the Customer has fully complied with the request.</p> </li> <li> <p>The Customer must keep their log-in information secret and carefully secure access to their PriorLabs Account. The Customer shall take reasonable precautions to prevent unauthorized access to the PriorLabs Account, and to protect the Services from unauthorized use. The Customer is obliged to inform PriorLabs immediately if there are indications that a PriorLabs Account has been misused by a third party. The Customer's liability for any activity of or interaction with a corrupted account is subject to statutory rules.</p> </li> </ol>"},{"location":"terms-eu-en/#4-contract-software","title":"4. Contract Software","text":"<ol> <li> <p>PriorLabs has developed the TabPFN foundation models that allows the analysis, processing and evaluation of tabular business data (\"Contract Software\").</p> </li> <li> <p>PriorLabs may, to the extent available, provide the Customer with Customer documentation for the Contract Software in digital form (e.g. as a pdf file).</p> </li> <li> <p>PriorLabs provides the Contract Software \"as is\" with the functionality, scope and performance and in a condition suitable for the contractual use. The Customer is aware that the Contract Software is provided as a beta version for testing and evaluation purposes. PriorLabs disclaims any liability of the availability, accuracy, or correctness of the use of the Contract Software and does not warrant the integration in the Customer's IT systems.</p> </li> <li> <p>The functionality, scope and performance of the Contract Software may change during the Contract Term (as defined below). PriorLabs reserves the right to add, remove, change or substitute elements of the Contract Software as deemed necessary at any time, in particular for the purpose of increasing efficiency, improvements, additional features, and/or safety or due to changes in the legal situation, technical developments or for reasons of IT security, or cease providing the Services altogether.</p> </li> </ol>"},{"location":"terms-eu-en/#5-priorlabs-intellectual-property","title":"5. PriorLabs Intellectual Property","text":"<ol> <li> <p>PriorLabs remains the sole owner of all right, title, and interest in the Contract Software, including but not limited to any models, algorithms, and neural networks. To the extent PriorLabs provides any Services or access to the Contract Software free of charge, PriorLabs does not waive any rights in such Services or the Contract Software.</p> </li> <li> <p>Except as stated in these GTC, PriorLabs does not grant the Customer any rights to patents, copyrights, trade secrets, trademarks, or any other rights in respect to the Contract Software.</p> </li> <li> <p>By using the Contract Software or using any Services, the Customer does not acquire ownership of any rights in the Contract Software, Services, documentation, and/or any related intellectual property other than stated in these GTC.</p> </li> </ol>"},{"location":"terms-eu-en/#6-api-access","title":"6. API Access","text":"<ol> <li> <p>PriorLabs allows registered Customers, as and to the extent available from time to time, access to the Contract Software via an application programming interface (\"API\"), non-exclusively, non-transferable and non-sublicensable to use it exclusively as provided on the PriorLabs website or as described in the Customer documentation for the API (\"API Access\").</p> </li> <li> <p>The Customer's access to and use of the Services must at all times be in accordance with applicable laws and regulations. The Customer is solely responsible for knowing and complying with the applicable laws and regulations. Permitted conditions of use and scope of use of the Services are further set out in the Acceptable Use Policy (\"AUP\") available under https://www.priorlabs.ai/aup. The Customer acknowledges that the provisions set out in the AUP shall be deemed material obligations under this Contract.</p> </li> </ol>"},{"location":"terms-eu-en/#7-customer-content-licenses","title":"7. Customer Content; Licenses","text":"<ol> <li> <p>The Customer must own or hold valid rights of sufficient scope to any material, documents, data or other content uploaded into the Services and to be processed by the Contract Software (\"Customer Content\"). The Customer Content consists exclusively of non-personal data within the meaning of the General Data Protection Regulation (\"GDPR\"), as set out in the AUP.</p> </li> <li> <p>PriorLabs shall take appropriate physical, technical, and organizational security measures with regard to the Contract Software and any Customer Content.</p> </li> <li> <p>The Customer grants PriorLabs the non-exclusive, worldwide, sublicensable right (i) to use Customer Content for the performance of PriorLabs' obligations under this Contract and, in particular, to reproduce such data on the server under PriorLabs' name itself or through a subcontractor for the purpose of providing the Service, and (ii) to use Customer Content as so-called training data in order to develop, test, and improve the Contract Software, in particular the underlying artificial intelligence systems and/or foundation models.</p> </li> <li> <p>The Customer is fully responsible for all Customer Content uploaded to the Services, in particular the Customer ensures that Customer Content is fit for PriorLabs' use in accordance with this Contract (including any necessary licenses pursuant to Section 7.3) and does not violate any applicable law or other rights of third parties, in particular copyright, trade secrets, or rights under the GDPR.</p> </li> </ol>"},{"location":"terms-eu-en/#8-service-results","title":"8. Service Results","text":"<ol> <li> <p>The Contract Software may be used to generate certain analyses, content, documents, reports, or other results (\"Service Results\") based on Customer Content.</p> </li> <li> <p>The Customer may freely use the Service Results. PriorLabs provides the Service Results \"as is\". The Customer is responsible for reviewing any Service Results of its use of the Contract Software. PriorLabs does not warrant the accuracy, correctness, completeness, usability, or fitness for a certain purpose of the Service Results and does not assume any liability for Customer's use of Service Results. In particular, PriorLabs disclaims all warranty if the Customer modifies, adapts or combines Service Results with third-party material or products.</p> </li> <li> <p>PriorLabs may use the Service Results to develop, test and improve the Contract Software, in particular the underlying artificial intelligence systems and/or foundation models.</p> </li> </ol>"},{"location":"terms-eu-en/#9-obligations-of-the-customer","title":"9. Obligations of the Customer","text":"<ol> <li> <p>The Customer shall create their own backup copies of Service Results in case of loss of data. PriorLabs provides a corresponding function for creating backup copies.</p> </li> <li> <p>The Customer shall inform PriorLabs without undue delay as soon as they become aware of the infringement of an intellectual property right or copyright in the Contract Software.</p> </li> <li> <p>The Customer shall ensure that all of its employees authorized to use the Contract Software have (i) received sufficient training on the safe use of the Contract Software, (ii) exercise the necessary care when using it, and (iii) are compliant with these GTC including the Acceptable Use Police available under https://www.priorlabs.ai/aup.</p> </li> </ol>"},{"location":"terms-eu-en/#10-blocking-of-accesses","title":"10. Blocking of Accesses","text":"<ol> <li> <p>PriorLabs is entitled to block access to the Contract Software and the Services temporarily or permanently if there are reliable indications that the Customer or, where applicable, one of its employees is violating or has violated material obligations under this GTC, including the Acceptable Use Policy, and/or applicable intellectual property, data protection of other statutory laws or if PriorLabs has another justified interest in the blocking, such as IT-security concerns.</p> </li> <li> <p>When deciding on a blocking, PriorLabs shall give due consideration to the legitimate interests of the Customer. PriorLabs shall inform the Customer of the blocking no later than five (5) working days before the blocking comes into effect, provided that the information does not conflict with the purpose of the blocking. The blocking shall continue until the contractual or legal violation has been remedied in an appropriate manner.</p> </li> </ol>"},{"location":"terms-eu-en/#11-limitation-of-liability","title":"11. Limitation of Liability","text":"<ol> <li> <p>The Services are provided free of charge. Therefore, PriorLabs' liability is in any cases limited to acts of intent or gross negligence.</p> </li> <li> <p>The strict liability for damages for defects of the Services already existing at the beginning of the Contract Term (as defined below) in terms of Section 536a German Civil Code is excluded. The Services are provided on an \"as is\" basis, which, in accordance with Section 4 of these GTC, refers in particular to the marketability, availability, and security aspects of the Contract Software.</p> </li> </ol>"},{"location":"terms-eu-en/#12-indemnity","title":"12. Indemnity","text":"<p>The Customer shall indemnify PriorLabs from any and all claims of end-users or third parties who assert claims against PriorLabs on account of the use of the Services by the Customer or the Customer's end-users, in particular concerning any Customer Content used in combination with the Contract Software. The provisions of this Section shall apply mutatis mutandis to any liquidated damages (Vertragsstrafen) as well as to any administrative fines (Bu\u00dfgeld) or penalties imposed by the authorities or by the courts, to the extent that the Customer is responsible for such.</p>"},{"location":"terms-eu-en/#13-term-termination-of-the-contract","title":"13. Term; Termination of the Contract","text":"<ol> <li> <p>If not agreed otherwise, the Contract is concluded for an indefinite period of time until terminated by either Party (\"Contract Term\").</p> </li> <li> <p>The Customer may terminate the Contract at any time by deleting its PriorLabs Account.</p> </li> <li> <p>PriorLabs reserves the right to terminate the Contract at any time but will consider the Customer's legitimate interests to the extent possible, e.g., by sending the notice of termination in due time to the email address provided by the Customer upon registration of the PriorLabs Account.</p> </li> <li> <p>The right of PriorLabs and the Customer to extraordinary termination without notice for cause shall remain unaffected.</p> </li> </ol>"},{"location":"terms-eu-en/#14-changes-to-this-contract","title":"14. Changes to this Contract","text":"<ol> <li> <p>PriorLabs may change this Contract during the Contract Term in compliance with the following procedure, provided that the amendment is reasonable for the Customer, i.e. without significant legal or economic disadvantages, taking into account the interests of the Customer and that there is a valid reason for the amendment. Such a reason exists, in particular, in cases of new technical developments or changes in the regulatory environment.</p> </li> <li> <p>PriorLabs shall inform the Customer of any changes to this Contract at least 30 calendar days before the planned entry into force of the changes. The Customer may object to the changes within 30 calendar days from receipt of the notification. If no objection is made and the Customer continues to use the Services after expiry of the objection period, the changes shall be deemed to have been effectively agreed for all Services to be provided from the end of the objection period. In the notification, PriorLabs will inform the Customer of all relevant changes to the Contract, the objection period and the legal consequences of the expiry of the objection period without exercise of the right of objection. If the Customer objects to the changes, PriorLabs may terminate the Contract pursuant to Section 13.</p> </li> </ol>"},{"location":"terms-eu-en/#15-final-provisions","title":"15. Final Provisions","text":"<ol> <li> <p>Should individual provisions of the Contract be or become invalid in whole or in part, this shall not affect the validity of the remaining provisions. Invalid provisions shall be replaced first and foremost by provisions that most closely correspond to the invalid provisions in a legally effective manner. The same applies to any loopholes.</p> </li> <li> <p>The law of the Federal Republic of Germany shall apply with the exception of its provisions on the choice of law which would lead to the application of another legal system. The validity of the CISG (\"UN Sales Convention\") is excluded.</p> </li> <li> <p>For Customers who are merchants (Kaufleute) within the meaning of the German Commercial Code (Handelsgesetzbuch), a special fund (Sonderverm\u00f6gen) under public law or a legal entity under public law, Berlin, Germany, shall be the exclusive place of jurisdiction for all disputes arising from the contractual relationship.</p> </li> </ol>"},{"location":"terms/","title":"GENERAL TERMS AND CONDITIONS","text":""},{"location":"terms/#1-scope-of-application","title":"1. Scope of Application","text":"<ol> <li> <p>These general terms and conditions (\"GTC\") govern the provision of access to the TabPFN foundation models as available at https://www.priorlabs.ai (\"Services\") provided by Prior Labs GmbH, Elisabeth-Emter-Weg 18, 79110 Freiburg im Breisgau (\"PriorLabs\").</p> </li> <li> <p>The Services of PriorLabs are directed exclusively at business customers (Unternehmer) within the meaning of Sec. 14 German Civil Code (B\u00fcrgerliches Gesetzbuch, BGB) (\"Customer\"). PriorLabs may require the Customer to provide sufficient proof of its status as business customer prior to the conclusion of the contract.</p> </li> <li> <p>Conflicting or additional contractual conditions of the Customer shall only apply if PriorLabs expressly confirms them in writing.</p> </li> </ol>"},{"location":"terms/#2-conclusion-of-contract","title":"2. Conclusion of Contract","text":"<ol> <li> <p>The contract is concluded with the inclusion of these GTC (\"Contract\") at the earliest of (i) when the Customer registers and sets up an account via the Services (\"PriorLabs Account\") or (ii) when the Customer first uses the Services.</p> </li> <li> <p>Upon conclusion of the Contract, the Customer shall provide PriorLabs with all information that PriorLabs reasonably requires in order to provide the Services correctly and completely. The Customer is obliged to inform PriorLabs immediately of any relevant changes.</p> </li> </ol>"},{"location":"terms/#3-registration-and-priorlabs-account","title":"3. Registration and PriorLabs Account","text":"<ol> <li> <p>The Customer agrees by registering or using a PriorLabs Account and represents that they created their PriorLabs Account, and they will use their PriorLabs Account only for themselves. Each Customer shall register only one PriorLabs Account. A PriorLabs Account is not transferable.</p> </li> <li> <p>If and to the extent, PriorLabs stores Customer's data, PriorLabs disclaims any liability for the storage, accessibility, or integrity of such data.</p> </li> <li> <p>The Customer is obliged (i) to provide complete and correct information about its person or entity at the time of registration and (ii) in case of respective changes to correct without undue delay this information insofar such information is mandatory for the performance of the Contract.</p> </li> <li> <p>If PriorLabs receives a notice or otherwise has reason to believe that the information or documents provided by the Customer are wholly or partially incorrect, incomplete or not up to date, PriorLabs is entitled to request the Customer to remedy the situation immediately. If the Customer fails to correct or complete the information or document within the set deadline, PriorLabs is entitled to restrict access to the Services and block the Customer until the Customer has fully complied with the request.</p> </li> <li> <p>The Customer must keep their log-in information secret and carefully secure access to their PriorLabs Account. The Customer shall take reasonable precautions to prevent unauthorized access to the PriorLabs Account, and to protect the Services from unauthorized use. The Customer is obliged to inform PriorLabs immediately if there are indications that a PriorLabs Account has been misused by a third party. The Customer's liability for any activity of or interaction with a corrupted account is subject to statutory rules.</p> </li> </ol>"},{"location":"terms/#4-contract-software","title":"4. Contract Software","text":"<ol> <li> <p>PriorLabs has developed the TabPFN foundation models that allows the analysis, processing and evaluation of tabular business data (\"Contract Software\").</p> </li> <li> <p>PriorLabs may, to the extent available, provide the Customer with Customer documentation for the Contract Software in digital form (e.g. as a pdf file).</p> </li> <li> <p>PriorLabs provides the Contract Software \"as is\" with the functionality, scope and performance and in a condition suitable for the contractual use. The Customer is aware that the Contract Software is provided as a beta version for testing and evaluation purposes. PriorLabs disclaims any liability of the availability, accuracy, or correctness of the use of the Contract Software and does not warrant the integration in the Customer's IT systems.</p> </li> <li> <p>The functionality, scope and performance of the Contract Software may change during the Contract Term (as defined below). PriorLabs reserves the right to add, remove, change or substitute elements of the Contract Software as deemed necessary at any time, in particular for the purpose of increasing efficiency, improvements, additional features, and/or safety or due to changes in the legal situation, technical developments or for reasons of IT security, or cease providing the Services altogether.</p> </li> </ol>"},{"location":"terms/#5-priorlabs-intellectual-property","title":"5. PriorLabs Intellectual Property","text":"<ol> <li> <p>PriorLabs remains the sole owner of all right, title, and interest in the Contract Software, including but not limited to any models, algorithms, and neural networks. To the extent PriorLabs provides any Services or access to the Contract Software free of charge, PriorLabs does not waive any rights in such Services or the Contract Software.</p> </li> <li> <p>Except as stated in these GTC, PriorLabs does not grant the Customer any rights to patents, copyrights, trade secrets, trademarks, or any other rights in respect to the Contract Software.</p> </li> <li> <p>By using the Contract Software or using any Services, the Customer does not acquire ownership of any rights in the Contract Software, Services, documentation, and/or any related intellectual property other than stated in these GTC.</p> </li> </ol>"},{"location":"terms/#6-api-access","title":"6. API Access","text":"<ol> <li> <p>PriorLabs allows registered Customers, as and to the extent available from time to time, access to the Contract Software via an application programming interface (\"API\"), non-exclusively, non-transferable and non-sublicensable to use it exclusively as provided on the PriorLabs website or as described in the Customer documentation for the API (\"API Access\").</p> </li> <li> <p>The Customer's access to and use of the Services must at all times be in accordance with applicable laws and regulations. The Customer is solely responsible for knowing and complying with the applicable laws and regulations. Permitted conditions of use and scope of use of the Services are further set out in the Acceptable Use Policy (\"AUP\") available under https://www.priorlabs.ai/aup. The Customer acknowledges that the provisions set out in the AUP shall be deemed material obligations under this Contract.</p> </li> </ol>"},{"location":"terms/#7-customer-content-licenses","title":"7. Customer Content; Licenses","text":"<ol> <li> <p>The Customer must own or hold valid rights of sufficient scope to any material, documents, data or other content uploaded into the Services and to be processed by the Contract Software (\"Customer Content\"). The Customer Content consists exclusively of non-personal data within the meaning of the General Data Protection Regulation (\"GDPR\"), as set out in the AUP.</p> </li> <li> <p>PriorLabs shall take appropriate physical, technical, and organizational security measures with regard to the Contract Software and any Customer Content.</p> </li> <li> <p>The Customer grants PriorLabs the non-exclusive, worldwide, sublicensable right (i) to use Customer Content for the performance of PriorLabs' obligations under this Contract and, in particular, to reproduce such data on the server under PriorLabs' name itself or through a subcontractor for the purpose of providing the Service, and (ii) to use Customer Content as so-called training data in order to develop, test, and improve the Contract Software, in particular the underlying artificial intelligence systems and/or foundation models.</p> </li> <li> <p>The Customer is fully responsible for all Customer Content uploaded to the Services, in particular the Customer ensures that Customer Content is fit for PriorLabs' use in accordance with this Contract (including any necessary licenses pursuant to Section 7.3) and does not violate any applicable law or other rights of third parties, in particular copyright, trade secrets, or rights under the GDPR.</p> </li> </ol>"},{"location":"terms/#8-service-results","title":"8. Service Results","text":"<ol> <li> <p>The Contract Software may be used to generate certain analyses, content, documents, reports, or other results (\"Service Results\") based on Customer Content.</p> </li> <li> <p>The Customer may freely use the Service Results. PriorLabs provides the Service Results \"as is\". The Customer is responsible for reviewing any Service Results of its use of the Contract Software. PriorLabs does not warrant the accuracy, correctness, completeness, usability, or fitness for a certain purpose of the Service Results and does not assume any liability for Customer's use of Service Results. In particular, PriorLabs disclaims all warranty if the Customer modifies, adapts or combines Service Results with third-party material or products.</p> </li> <li> <p>PriorLabs may use the Service Results to develop, test and improve the Contract Software, in particular the underlying artificial intelligence systems and/or foundation models.</p> </li> </ol>"},{"location":"terms/#9-obligations-of-the-customer","title":"9. Obligations of the Customer","text":"<ol> <li> <p>The Customer shall create their own backup copies of Service Results in case of loss of data. PriorLabs provides a corresponding function for creating backup copies.</p> </li> <li> <p>The Customer shall inform PriorLabs without undue delay as soon as they become aware of the infringement of an intellectual property right or copyright in the Contract Software.</p> </li> <li> <p>The Customer shall ensure that all of its employees authorized to use the Contract Software have (i) received sufficient training on the safe use of the Contract Software, (ii) exercise the necessary care when using it, and (iii) are compliant with these GTC including the Acceptable Use Police available under https://www.priorlabs.ai/aup.</p> </li> </ol>"},{"location":"terms/#10-blocking-of-accesses","title":"10. Blocking of Accesses","text":"<ol> <li> <p>PriorLabs is entitled to block access to the Contract Software and the Services temporarily or permanently if there are reliable indications that the Customer or, where applicable, one of its employees is violating or has violated material obligations under this GTC, including the Acceptable Use Policy, and/or applicable intellectual property, data protection of other statutory laws or if PriorLabs has another justified interest in the blocking, such as IT-security concerns.</p> </li> <li> <p>When deciding on a blocking, PriorLabs shall give due consideration to the legitimate interests of the Customer. PriorLabs shall inform the Customer of the blocking no later than five (5) working days before the blocking comes into effect, provided that the information does not conflict with the purpose of the blocking. The blocking shall continue until the contractual or legal violation has been remedied in an appropriate manner.</p> </li> </ol>"},{"location":"terms/#11-limitation-of-liability","title":"11. Limitation of Liability","text":"<ol> <li> <p>The Services are provided free of charge. Therefore, PriorLabs' liability is in any cases limited to acts of intent or gross negligence.</p> </li> <li> <p>The strict liability for damages for defects of the Services already existing at the beginning of the Contract Term (as defined below) in terms of Section 536a German Civil Code is excluded. The Services are provided on an \"as is\" basis, which, in accordance with Section 4 of these GTC, refers in particular to the marketability, availability, and security aspects of the Contract Software.</p> </li> </ol>"},{"location":"terms/#12-indemnity","title":"12. Indemnity","text":"<p>The Customer shall indemnify PriorLabs from any and all claims of end-users or third parties who assert claims against PriorLabs on account of the use of the Services by the Customer or the Customer's end-users, in particular concerning any Customer Content used in combination with the Contract Software. The provisions of this Section shall apply mutatis mutandis to any liquidated damages (Vertragsstrafen) as well as to any administrative fines (Bu\u00dfgeld) or penalties imposed by the authorities or by the courts, to the extent that the Customer is responsible for such.</p>"},{"location":"terms/#13-term-termination-of-the-contract","title":"13. Term; Termination of the Contract","text":"<ol> <li> <p>If not agreed otherwise, the Contract is concluded for an indefinite period of time until terminated by either Party (\"Contract Term\").</p> </li> <li> <p>The Customer may terminate the Contract at any time by deleting its PriorLabs Account.</p> </li> <li> <p>PriorLabs reserves the right to terminate the Contract at any time but will consider the Customer's legitimate interests to the extent possible, e.g., by sending the notice of termination in due time to the email address provided by the Customer upon registration of the PriorLabs Account.</p> </li> <li> <p>The right of PriorLabs and the Customer to extraordinary termination without notice for cause shall remain unaffected.</p> </li> </ol>"},{"location":"terms/#14-changes-to-this-contract","title":"14. Changes to this Contract","text":"<ol> <li> <p>PriorLabs may change this Contract during the Contract Term in compliance with the following procedure, provided that the amendment is reasonable for the Customer, i.e. without significant legal or economic disadvantages, taking into account the interests of the Customer and that there is a valid reason for the amendment. Such a reason exists, in particular, in cases of new technical developments or changes in the regulatory environment.</p> </li> <li> <p>PriorLabs shall inform the Customer of any changes to this Contract at least 30 calendar days before the planned entry into force of the changes. The Customer may object to the changes within 30 calendar days from receipt of the notification. If no objection is made and the Customer continues to use the Services after expiry of the objection period, the changes shall be deemed to have been effectively agreed for all Services to be provided from the end of the objection period. In the notification, PriorLabs will inform the Customer of all relevant changes to the Contract, the objection period and the legal consequences of the expiry of the objection period without exercise of the right of objection. If the Customer objects to the changes, PriorLabs may terminate the Contract pursuant to Section 13.</p> </li> </ol>"},{"location":"terms/#15-final-provisions","title":"15. Final Provisions","text":"<ol> <li> <p>Should individual provisions of the Contract be or become invalid in whole or in part, this shall not affect the validity of the remaining provisions. Invalid provisions shall be replaced first and foremost by provisions that most closely correspond to the invalid provisions in a legally effective manner. The same applies to any loopholes.</p> </li> <li> <p>The law of the Federal Republic of Germany shall apply with the exception of its provisions on the choice of law which would lead to the application of another legal system. The validity of the CISG (\"UN Sales Convention\") is excluded.</p> </li> <li> <p>For Customers who are merchants (Kaufleute) within the meaning of the German Commercial Code (Handelsgesetzbuch), a special fund (Sonderverm\u00f6gen) under public law or a legal entity under public law, Berlin, Germany, shall be the exclusive place of jurisdiction for all disputes arising from the contractual relationship.</p> </li> </ol>"},{"location":"getting_started/api/","title":"TabPFN API Guide","text":""},{"location":"getting_started/api/#authentication","title":"Authentication","text":""},{"location":"getting_started/api/#interactive-login","title":"Interactive Login","text":"<p>The first time you use TabPFN, you'll be guided through an interactive login process:</p> <pre><code>from tabpfn_client import init\ninit()\n</code></pre>"},{"location":"getting_started/api/#managing-access-tokens","title":"Managing Access Tokens","text":"<p>You can save your token for use on other machines:</p> <pre><code>import tabpfn_client\n# Get your token\ntoken = tabpfn_client.get_access_token()\n\n# Use token on another machine\ntabpfn_client.set_access_token(token)\n</code></pre>"},{"location":"getting_started/api/#rate-limits","title":"Rate Limits","text":"<p>Our API implements a fair usage system that resets daily at 00:00:00 UTC.</p>"},{"location":"getting_started/api/#usage-cost-calculation","title":"Usage Cost Calculation","text":"<p>The cost for each API request is calculated as: <pre><code>api_cost = (num_train_rows + num_test_rows) * num_cols * n_estimators\n</code></pre></p> <p>Where <code>n_estimators</code> is by default: - 4 for classification tasks - 8 for regression tasks</p>"},{"location":"getting_started/api/#monitoring-usage","title":"Monitoring Usage","text":"<p>Track your API usage through response headers:</p> Header Description <code>X-RateLimit-Limit</code> Your total allowed usage <code>X-RateLimit-Remaining</code> Remaining usage <code>X-RateLimit-Reset</code> Reset timestamp (UTC)"},{"location":"getting_started/api/#current-limitations","title":"Current Limitations","text":""},{"location":"getting_started/api/#data-privacy-and-security","title":"Data Privacy and Security","text":"<p>Important Data Guidelines</p> <ul> <li>Do NOT upload any Personally Identifiable Information (PII)</li> <li>Do NOT upload any sensitive or confidential data</li> <li>Do NOT upload any data you don't have permission to share</li> <li>Consider anonymizing or pseudonymizing your data</li> <li>Review your organization's data sharing policies</li> </ul>"},{"location":"getting_started/api/#size-limitations","title":"Size Limitations","text":"<ol> <li> <p>Maximum total cells per request must be below 100,000: <pre><code>max_cells = (num_train_rows + num_test_rows) * num_cols\n</code></pre></p> </li> <li> <p>For regression with full output (<code>return_full_output=True</code>), the number of test samples must be below 500: <pre><code>if task == 'regression' and return_full_output and num_test_samples &gt; 500:\n    raise ValueError(\"Cannot return full output for regression with &gt;500 test samples\")\n</code></pre></p> </li> </ol> <p>These limits will be increased in future releases.</p>"},{"location":"getting_started/api/#managing-user-data","title":"Managing User Data","text":"<p>You can access and manage your personal information:</p> <pre><code>from tabpfn_client import UserDataClient\nprint(UserDataClient.get_data_summary())\n</code></pre>"},{"location":"getting_started/api/#error-handling","title":"Error Handling","text":"<p>The API uses standard HTTP status codes:</p> Code Meaning 200 Success 400 Invalid request 429 Rate limit exceeded <p>Example error response: <pre><code>{\n    \"error\": \"API_LIMIT_REACHED\",\n    \"message\": \"Usage limit exceeded\",\n    \"next_available_at\": \"2024-01-07 00:00:00\"\n}\n</code></pre></p>"},{"location":"getting_started/install/","title":"Installation","text":"<p>You can access our models through our API (https://github.com/automl/tabpfn-client) or via our user interface built on top of the API (https://www.ux.priorlabs.ai/).</p> Python API Client (No GPU, Online)Python Local (GPU)Web InterfaceR <pre><code>pip install tabpfn-client\n</code></pre> <pre><code>pip install tabpfn\n</code></pre> <p>You can access our models through our Interface here.</p> <p>Warning</p> <p>R support is currently under development. You can find a work in progress at TabPFN R. Looking for contributors!</p>"},{"location":"getting_started/intended_use/","title":"Usage tips","text":"<p>Note</p> <p>For a simple example getting started with classification see classification tutorial.</p> <p>We provide a comprehensive demo notebook that guides through installation and functionalities at Interactive Colab Tutorial (with GPU usage) and Interactive Colab Tutorial (without GPU usage).</p>"},{"location":"getting_started/intended_use/#when-to-use-tabpfn","title":"When to use TabPFN","text":"<p>TabPFN excels in handling small to medium-sized datasets with up to 10,000 samples and 500 features. For larger datasets, approaches such as CatBoost, XGB, or AutoGluon are likely to outperform TabPFN.</p>"},{"location":"getting_started/intended_use/#intended-use-of-tabpfn","title":"Intended Use of TabPFN","text":"<p>While TabPFN provides a powerful drop-in replacement for traditional tabular data models, achieving top performance on real-world problems often requires domain expertise and the ingenuity of data scientists. Data scientists should continue to apply their skills in feature engineering, data cleaning, and problem framing to get the most out of TabPFN.</p>"},{"location":"getting_started/intended_use/#limitations-of-tabpfn","title":"Limitations of TabPFN","text":"<ol> <li>TabPFN's inference speed may be slower than highly optimized approaches like CatBoost.</li> <li>TabPFN's memory usage scales linearly with dataset size, which can be prohibitive for very large datasets.</li> <li>Our evaluation focused on datasets with up to 10,000 samples and 500 features; scalability to larger datasets requires further study.</li> </ol>"},{"location":"getting_started/intended_use/#computational-and-time-requirements","title":"Computational and Time Requirements","text":"<p>TabPFN is computationally efficient and can run on consumer hardware for most datasets. Training on a new dataset is recommended to run on a GPU as this speeds it up significantly. However, TabPFN is not optimized for real-time inference tasks.</p>"},{"location":"getting_started/intended_use/#data-preparation","title":"Data Preparation","text":"<p>TabPFN can handle raw data with minimal preprocessing. Provide the data in a tabular format, and TabPFN will automatically handle missing values, encode categorical variables, and normalize features. While TabPFN works well out-of-the-box, performance can further be improved using dataset-specific preprocessings.</p>"},{"location":"getting_started/intended_use/#interpreting-results","title":"Interpreting Results","text":"<p>TabPFN's predictions come with uncertainty estimates, allowing you to assess the reliability of the results. You can use SHAP to interpret TabPFN's predictions and identify the most important features driving the model's decisions.</p>"},{"location":"getting_started/intended_use/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>TabPFN provides strong performance out-of-the-box without extensive hyperparameter tuning. If you have additional computational resources, you can further optimize TabPFN's performance using random hyperparameter tuning or the Post-Hoc Ensembling (PHE) technique.</p>"},{"location":"reference/tabpfn/base/","title":"Base","text":""},{"location":"reference/tabpfn/base/#tabpfn.base","title":"base","text":"<p>Common logic for TabPFN models.</p>"},{"location":"reference/tabpfn/base/#tabpfn.base.create_inference_engine","title":"create_inference_engine","text":"<pre><code>create_inference_engine(\n    *,\n    X_train: ndarray,\n    y_train: ndarray,\n    model: PerFeatureTransformer,\n    ensemble_configs: Any,\n    cat_ix: list[int],\n    fit_mode: Literal[\n        \"low_memory\", \"fit_preprocessors\", \"fit_with_cache\"\n    ],\n    device_: device,\n    rng: Generator,\n    n_jobs: int,\n    byte_size: int,\n    forced_inference_dtype_: dtype | None,\n    memory_saving_mode: (\n        bool | Literal[\"auto\"] | float | int\n    ),\n    use_autocast_: bool\n) -&gt; InferenceEngine\n</code></pre> <p>Creates the appropriate TabPFN inference engine based on <code>fit_mode</code>.</p> <p>Each execution mode will perform slightly different operations based on the mode specified by the user. In the case where preprocessors will be fit after <code>prepare</code>, we will use them to further transform the associated borders with each ensemble config member.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Training features</p> required <code>y_train</code> <code>ndarray</code> <p>Training target</p> required <code>model</code> <code>PerFeatureTransformer</code> <p>The loaded TabPFN model.</p> required <code>ensemble_configs</code> <code>Any</code> <p>The ensemble configurations to create multiple \"prompts\".</p> required <code>cat_ix</code> <code>list[int]</code> <p>Indices of inferred categorical features.</p> required <code>fit_mode</code> <code>Literal['low_memory', 'fit_preprocessors', 'fit_with_cache']</code> <p>Determines how we prepare inference (pre-cache or not).</p> required <code>device_</code> <code>device</code> <p>The device for inference.</p> required <code>rng</code> <code>Generator</code> <p>Numpy random generator.</p> required <code>n_jobs</code> <code>int</code> <p>Number of parallel CPU workers.</p> required <code>byte_size</code> <code>int</code> <p>Byte size for the chosen inference precision.</p> required <code>forced_inference_dtype_</code> <code>dtype | None</code> <p>If not None, the forced dtype for inference.</p> required <code>memory_saving_mode</code> <code>bool | Literal['auto'] | float | int</code> <p>GPU/CPU memory saving settings.</p> required <code>use_autocast_</code> <code>bool</code> <p>Whether we use torch.autocast for inference.</p> required"},{"location":"reference/tabpfn/base/#tabpfn.base.determine_precision","title":"determine_precision","text":"<pre><code>determine_precision(\n    inference_precision: (\n        dtype | Literal[\"autocast\", \"auto\"]\n    ),\n    device_: device,\n) -&gt; tuple[bool, dtype | None, int]\n</code></pre> <p>Decide whether to use autocast or a forced precision dtype.</p> <p>Parameters:</p> Name Type Description Default <code>inference_precision</code> <code>dtype | Literal['autocast', 'auto']</code> <ul> <li>If <code>\"auto\"</code>, decide automatically based on the device.</li> <li>If <code>\"autocast\"</code>, explicitly use PyTorch autocast (mixed precision).</li> <li>If a <code>torch.dtype</code>, force that precision.</li> </ul> required <code>device_</code> <code>device</code> <p>The device on which inference is run.</p> required <p>Returns:</p> Name Type Description <code>use_autocast_</code> <code>bool</code> <p>True if mixed-precision autocast will be used.</p> <code>forced_inference_dtype_</code> <code>dtype | None</code> <p>If not None, the forced precision dtype for the model.</p> <code>byte_size</code> <code>int</code> <p>The byte size per element for the chosen precision.</p>"},{"location":"reference/tabpfn/base/#tabpfn.base.initialize_tabpfn_model","title":"initialize_tabpfn_model","text":"<pre><code>initialize_tabpfn_model(\n    model_path: str | Path | Literal[\"auto\"],\n    which: Literal[\"classifier\", \"regressor\"],\n    fit_mode: Literal[\n        \"low_memory\", \"fit_preprocessors\", \"fit_with_cache\"\n    ],\n    static_seed: int,\n) -&gt; tuple[\n    PerFeatureTransformer,\n    InferenceConfig,\n    FullSupportBarDistribution | None,\n]\n</code></pre> <p>Common logic to load the TabPFN model, set up the random state, and optionally download the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str | Path | Literal['auto']</code> <p>Path or directive (\"auto\") to load the pre-trained model from.</p> required <code>which</code> <code>Literal['classifier', 'regressor']</code> <p>Which TabPFN model to load.</p> required <code>fit_mode</code> <code>Literal['low_memory', 'fit_preprocessors', 'fit_with_cache']</code> <p>Determines caching behavior.</p> required <code>static_seed</code> <code>int</code> <p>Random seed for reproducibility logic.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>PerFeatureTransformer</code> <p>The loaded TabPFN model.</p> <code>config</code> <code>InferenceConfig</code> <p>The configuration object associated with the loaded model.</p> <code>bar_distribution</code> <code>FullSupportBarDistribution | None</code> <p>The BarDistribution for regression (<code>None</code> if classifier).</p>"},{"location":"reference/tabpfn/classifier/","title":"Classifier","text":""},{"location":"reference/tabpfn/classifier/#tabpfn.classifier","title":"classifier","text":"<p>TabPFNClassifier class.</p> <p>Example</p> <pre><code>import sklearn.datasets\nfrom tabpfn import TabPFNClassifier\n\nmodel = TabPFNClassifier()\n\nX, y = sklearn.datasets.load_iris(return_X_y=True)\n\nmodel.fit(X, y)\npredictions = model.predict(X)\n</code></pre>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier","title":"TabPFNClassifier","text":"<p>             Bases: <code>ClassifierMixin</code>, <code>BaseEstimator</code></p> <p>TabPFNClassifier class.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.class_counts_","title":"class_counts_  <code>instance-attribute</code>","text":"<pre><code>class_counts_: NDArray[Any]\n</code></pre> <p>The number of classes per class found in the target data during <code>fit()</code>.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.classes_","title":"classes_  <code>instance-attribute</code>","text":"<pre><code>classes_: NDArray[Any]\n</code></pre> <p>The unique classes found in the target data during <code>fit()</code>.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.config_","title":"config_  <code>instance-attribute</code>","text":"<pre><code>config_: InferenceConfig\n</code></pre> <p>The configuration of the loaded model to be used for inference.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.device_","title":"device_  <code>instance-attribute</code>","text":"<pre><code>device_: device\n</code></pre> <p>The device determined to be used.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.executor_","title":"executor_  <code>instance-attribute</code>","text":"<pre><code>executor_: InferenceEngine\n</code></pre> <p>The inference engine used to make predictions.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.feature_names_in_","title":"feature_names_in_  <code>instance-attribute</code>","text":"<pre><code>feature_names_in_: NDArray[Any]\n</code></pre> <p>The feature names of the input data.</p> <p>May not be set if the input data does not have feature names, such as with a numpy array.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.forced_inference_dtype_","title":"forced_inference_dtype_  <code>instance-attribute</code>","text":"<pre><code>forced_inference_dtype_: _dtype | None\n</code></pre> <p>The forced inference dtype for the model based on <code>inference_precision</code>.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.inferred_categorical_indices_","title":"inferred_categorical_indices_  <code>instance-attribute</code>","text":"<pre><code>inferred_categorical_indices_: list[int]\n</code></pre> <p>The indices of the columns that were inferred to be categorical, as a product of any features deemed categorical by the user and what would work best for the model.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.interface_config_","title":"interface_config_  <code>instance-attribute</code>","text":"<pre><code>interface_config_: ModelInterfaceConfig\n</code></pre> <p>Additional configuration of the interface for expert users.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.label_encoder_","title":"label_encoder_  <code>instance-attribute</code>","text":"<pre><code>label_encoder_: LabelEncoder\n</code></pre> <p>The label encoder used to encode the target variable.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.n_classes_","title":"n_classes_  <code>instance-attribute</code>","text":"<pre><code>n_classes_: int\n</code></pre> <p>The number of classes found in the target data during <code>fit()</code>.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.n_features_in_","title":"n_features_in_  <code>instance-attribute</code>","text":"<pre><code>n_features_in_: int\n</code></pre> <p>The number of features in the input data used during <code>fit()</code>.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.n_outputs_","title":"n_outputs_  <code>instance-attribute</code>","text":"<pre><code>n_outputs_: Literal[1]\n</code></pre> <p>The number of outputs the model has. Only 1 for now</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.preprocessor_","title":"preprocessor_  <code>instance-attribute</code>","text":"<pre><code>preprocessor_: ColumnTransformer\n</code></pre> <p>The column transformer used to preprocess the input data to be numeric.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.use_autocast_","title":"use_autocast_  <code>instance-attribute</code>","text":"<pre><code>use_autocast_: bool\n</code></pre> <p>Whether torch's autocast should be used.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.fit","title":"fit","text":"<pre><code>fit(X: XType, y: YType) -&gt; Self\n</code></pre> <p>Fit the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input data.</p> required <code>y</code> <code>YType</code> <p>The target variable.</p> required"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X: XType) -&gt; ndarray\n</code></pre> <p>Predict the class labels for the provided input samples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input samples.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The predicted class labels.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X: XType) -&gt; ndarray\n</code></pre> <p>Predict the probabilities of the classes for the provided input samples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input data.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The predicted probabilities of the classes.</p>"},{"location":"reference/tabpfn/constants/","title":"Constants","text":""},{"location":"reference/tabpfn/constants/#tabpfn.constants","title":"constants","text":"<p>Various constants used throughout the library.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig","title":"ModelInterfaceConfig  <code>dataclass</code>","text":"<p>Constants used as default HPs in the model interfaces.</p> <p>These constants are not exposed to the models' init on purpose to reduce the complexity for users. Furthermore, most of these should not be optimized over by the (standard) user.</p> <p>Several of the preprocessing options are supported by our code for efficiency reasons (to avoid loading TabPFN multiple times). However, these can also be applied outside of the model interface.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.CLASS_SHIFT_METHOD","title":"CLASS_SHIFT_METHOD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLASS_SHIFT_METHOD: Literal[\"rotate\", \"shuffle\"] | None = (\n    \"shuffle\"\n)\n</code></pre> <p>The method used to shift classes during preprocessing for ensembling to emulate the effect of invariance to class order. Without ensembling, TabPFN is not invariant to class order due to using a transformer. Shifting classes can have a positive effect on the model's performance. The options are:     - If \"shuffle\", the classes are shuffled.     - If \"rotate\", the classes are rotated (think of a ring).     - If None, no class shifting is done.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.FEATURE_SHIFT_METHOD","title":"FEATURE_SHIFT_METHOD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FEATURE_SHIFT_METHOD: (\n    Literal[\"shuffle\", \"rotate\"] | None\n) = \"shuffle\"\n</code></pre> <p>The method used to shift features during preprocessing for ensembling to emulate the effect of invariance to feature position. Without ensembling, TabPFN is not invariant to feature position due to using a transformer. Moreover, shifting features can have a positive effect on the model's performance. The options are:    - If \"shuffle\", the features are shuffled.    - If \"rotate\", the features are rotated (think of a ring).    - If None, no feature shifting is done.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.FINGERPRINT_FEATURE","title":"FINGERPRINT_FEATURE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FINGERPRINT_FEATURE: bool = True\n</code></pre> <p>Whether to add a fingerprint feature to the data. The added feature is a hash of the row, counting up for duplicates. This helps TabPFN to distinguish between duplicated data points in the input data. Otherwise, duplicates would be less obvious during attention. This is expected to improve prediction performance and help with stability if the data has many sample duplicates.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.FIX_NAN_BORDERS_AFTER_TARGET_TRANSFORM","title":"FIX_NAN_BORDERS_AFTER_TARGET_TRANSFORM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FIX_NAN_BORDERS_AFTER_TARGET_TRANSFORM: bool = True\n</code></pre> <p>Whether to repair any borders of the bar distribution in regression that are NaN after the transformation. This can happen due to multiple reasons and should in general always be done.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.MAX_NUMBER_OF_CLASSES","title":"MAX_NUMBER_OF_CLASSES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_NUMBER_OF_CLASSES: int = 10\n</code></pre> <p>The number of classes seen during pretraining for classification. If the number of classes is larger than this number, TabPFN requires an additional step to predict for more than classes.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.MAX_NUMBER_OF_FEATURES","title":"MAX_NUMBER_OF_FEATURES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_NUMBER_OF_FEATURES: int = 500\n</code></pre> <p>The number of features that the pretraining was intended for. If the number of features is larger than this number, you may see degraded performance. Note, this is not the number of features seen by the model during pretraining but also accounts for expected generalization (i.e., length extrapolation).</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.MAX_NUMBER_OF_SAMPLES","title":"MAX_NUMBER_OF_SAMPLES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_NUMBER_OF_SAMPLES: int = 10000\n</code></pre> <p>The number of samples that the pretraining was intended for. If the number of samples is larger than this number, you may see degraded performance. Note, this is not the number of samples seen by the model during pretraining but also accounts for expected generalization (i.e., length extrapolation).</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.MAX_UNIQUE_FOR_CATEGORICAL_FEATURES","title":"MAX_UNIQUE_FOR_CATEGORICAL_FEATURES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_UNIQUE_FOR_CATEGORICAL_FEATURES: int = 30\n</code></pre> <p>The maximum number of unique values for a feature to be considered categorical. Otherwise, it is considered numerical.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.MIN_NUMBER_SAMPLES_FOR_CATEGORICAL_INFERENCE","title":"MIN_NUMBER_SAMPLES_FOR_CATEGORICAL_INFERENCE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MIN_NUMBER_SAMPLES_FOR_CATEGORICAL_INFERENCE: int = 100\n</code></pre> <p>The minimum number of samples in the data to run our infer which features might be categorical.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.MIN_UNIQUE_FOR_NUMERICAL_FEATURES","title":"MIN_UNIQUE_FOR_NUMERICAL_FEATURES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MIN_UNIQUE_FOR_NUMERICAL_FEATURES: int = 4\n</code></pre> <p>The minimum number of unique values for a feature to be considered numerical. Otherwise, it is considered categorical.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.OUTLIER_REMOVAL_STD","title":"OUTLIER_REMOVAL_STD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OUTLIER_REMOVAL_STD: float | None | Literal[\"auto\"] = \"auto\"\n</code></pre> <p>The number of standard deviations from the mean to consider a sample an outlier. - If None, no outliers are removed. - If float, the number of standard deviations from the mean to consider a sample     an outlier. - If \"auto\", the OUTLIER_REMOVAL_STD is automatically determined.     -&gt; 12.0 for classification and None for regression.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.POLYNOMIAL_FEATURES","title":"POLYNOMIAL_FEATURES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>POLYNOMIAL_FEATURES: Literal['no', 'all'] | int = 'no'\n</code></pre> <p>The number of 2 factor polynomial features to generate and add to the original data before passing the data to TabPFN. The polynomial features are generated by multiplying the original features together, e.g., this might add a feature <code>x1*x2</code> to the features, if <code>x1</code> and <code>x2</code> are features. In  total, this can add up O(n^2) many features. Adding polynomial features can  improve predictive performance by exploiting simple feature engineering.     - If \"no\", no polynomial features are added.     - If \"all\", all possible polynomial features are added.     - If an int, determines the maximal number of polynomial features to add to the      original data.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.PREPROCESS_TRANSFORMS","title":"PREPROCESS_TRANSFORMS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PREPROCESS_TRANSFORMS: list[PreprocessorConfig] | None = (\n    None\n)\n</code></pre> <p>The preprocessing applied to the data before passing it to TabPFN. See <code>PreprocessorConfig</code> for options and more details. If a list of <code>PreprocessorConfig</code> is provided, the preprocessors are (repeatedly) applied across different estimators.</p> <p>By default, for classification, two preprocessors are applied:     1. Uses the original input data, all features transformed with a quantile         scaler, and the first n-many components of SVD transformer (whereby         n is a fract of on the number of features or samples). Categorical features         are ordinal encoded but all categories with less than 10 features are         ignored.     2. Uses the original input data, with categorical features as ordinal encoded.</p> <p>By default, for regression, two preprocessor are applied:     1. The same as for classification, with a minimal different quantile scaler.     2. The original input data power transformed and categories onehot encoded.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.REGRESSION_Y_PREPROCESS_TRANSFORMS","title":"REGRESSION_Y_PREPROCESS_TRANSFORMS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REGRESSION_Y_PREPROCESS_TRANSFORMS: tuple[\n    Literal[\"safepower\", \"power\", \"quantile_norm\", None],\n    ...,\n] = (None, \"safepower\")\n</code></pre> <p>The preprocessing applied to the target variable before passing it to TabPFN for regression. This can be understood as scaling the target variable to better predict it. The preprocessors should be passed as a tuple/list and are then (repeatedly) used by the estimators in the ensembles.</p> <p>By default, we use no preprocessing and a power transformation (if we have more than one estimator).</p> The options are <ul> <li>If None, no preprocessing is done.</li> <li>If \"power\", a power transformation is applied.</li> <li>If \"safepower\", a power transformation is applied with a safety factor to     avoid numerical issues.</li> <li>If \"quantile_norm\", a quantile normalization is applied.</li> </ul>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.SUBSAMPLE_SAMPLES","title":"SUBSAMPLE_SAMPLES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SUBSAMPLE_SAMPLES: int | float | None = None\n</code></pre> <p>Subsample the input data sample/row-wise before performing any preprocessing and the TabPFN forward pass.     - If None, no subsampling is done.     - If an int, the number of samples to subsample (or oversample if         <code>SUBSAMPLE_SAMPLES</code> is larger than the number of samples).     - If a float, the percentage of samples to subsample.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.USE_SKLEARN_16_DECIMAL_PRECISION","title":"USE_SKLEARN_16_DECIMAL_PRECISION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>USE_SKLEARN_16_DECIMAL_PRECISION: bool = False\n</code></pre> <p>Whether to round the probabilities to float 16 to match the precision of scikit-learn. This can help with reproducibility and compatibility with scikit-learn but is not recommended for general use. This is not exposed to the user or as a hyperparameter. To improve reproducibility,set <code>._sklearn_16_decimal_precision = True</code> before calling <code>.predict()</code> or <code>.predict_proba()</code>.</p>"},{"location":"reference/tabpfn/constants/#tabpfn.constants.ModelInterfaceConfig.from_user_input","title":"from_user_input  <code>staticmethod</code>","text":"<pre><code>from_user_input(\n    *, inference_config: dict | ModelInterfaceConfig | None\n) -&gt; ModelInterfaceConfig\n</code></pre> <p>Converts the user input to a <code>ModelInterfaceConfig</code> object.</p> <p>The input inference_config can be a dictionary, a <code>ModelInterfaceConfig</code> object, or None. If a dictionary is passed, the keys must match the attributes of <code>ModelInterfaceConfig</code>. If a <code>ModelInterfaceConfig</code> object is passed, it is returned as is. If None is passed, a new <code>ModelInterfaceConfig</code> object is created with default values.</p>"},{"location":"reference/tabpfn/inference/","title":"Inference","text":""},{"location":"reference/tabpfn/inference/#tabpfn.inference","title":"inference","text":"<p>Module that defines different ways to run inference with TabPFN.</p>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngine","title":"InferenceEngine  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>These define how tabpfn inference can be run.</p> <p>As there are many things that can be cached, with multiple ways to parallelize, <code>Executor</code> defines three primary things:</p> <p>Most will define a method <code>prepare()</code> which is specific to that inference engine. These do not share a common interface.</p> <ol> <li> <p>What to cache:</p> <p>As we can prepare a lot of the transformers context, there is a tradeoff in terms of how much memory to be spent in caching. This memory is used when <code>prepare()</code> is called, usually in <code>fit()</code>.</p> </li> <li> <p>Using the cached data for inference:</p> <p>Based on what has been prepared for the transformer context, <code>iter_outputs()</code> will use this cached information to make predictions.</p> </li> <li> <p>Controlling parallelism:</p> <p>As we have trivially parallel parts for inference, we can parallelize them. However as the GPU is typically a bottle-neck in most systems, we can define, where and how we would like to parallelize the inference.</p> </li> </ol>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngine.iter_outputs","title":"iter_outputs  <code>abstractmethod</code>","text":"<pre><code>iter_outputs(\n    X: ndarray, *, device: device, autocast: bool\n) -&gt; Iterator[tuple[Tensor, EnsembleConfig]]\n</code></pre> <p>Iterate over the outputs of the model.</p> <p>One for each ensemble configuration that was used to initialize the executor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data to make predictions on.</p> required <code>device</code> <code>device</code> <p>The device to run the model on.</p> required <code>autocast</code> <code>bool</code> <p>Whether to use torch.autocast during inference.</p> required"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineCacheKV","title":"InferenceEngineCacheKV  <code>dataclass</code>","text":"<p>             Bases: <code>InferenceEngine</code></p> <p>Inference engine that caches the actual KV cache calculated from the context of the processed training data.</p> <p>This is by far the most memory intensive inference engine, as for each ensemble member we store the full KV cache of that model. For now this is held in CPU RAM (TODO(eddiebergman): verify)</p>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineCacheKV.prepare","title":"prepare  <code>classmethod</code>","text":"<pre><code>prepare(\n    X_train: ndarray,\n    y_train: ndarray,\n    *,\n    cat_ix: list[int],\n    ensemble_configs: Sequence[EnsembleConfig],\n    n_workers: int,\n    model: PerFeatureTransformer,\n    device: device,\n    rng: Generator,\n    dtype_byte_size: int,\n    force_inference_dtype: dtype | None,\n    save_peak_mem: bool | Literal[\"auto\"] | float | int,\n    autocast: bool\n) -&gt; InferenceEngineCacheKV\n</code></pre> <p>Prepare the inference engine.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>The training data.</p> required <code>y_train</code> <code>ndarray</code> <p>The training target.</p> required <code>cat_ix</code> <code>list[int]</code> <p>The categorical indices.</p> required <code>ensemble_configs</code> <code>Sequence[EnsembleConfig]</code> <p>The ensemble configurations to use.</p> required <code>n_workers</code> <code>int</code> <p>The number of workers to use.</p> required <code>model</code> <code>PerFeatureTransformer</code> <p>The model to use.</p> required <code>device</code> <code>device</code> <p>The device to run the model on.</p> required <code>rng</code> <code>Generator</code> <p>The random number generator.</p> required <code>dtype_byte_size</code> <code>int</code> <p>Size of the dtype in bytes.</p> required <code>force_inference_dtype</code> <code>dtype | None</code> <p>The dtype to force inference to.</p> required <code>save_peak_mem</code> <code>bool | Literal['auto'] | float | int</code> <p>Whether to save peak memory usage.</p> required <code>autocast</code> <code>bool</code> <p>Whether to use torch.autocast during inference.</p> required"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineCachePreprocessing","title":"InferenceEngineCachePreprocessing  <code>dataclass</code>","text":"<p>             Bases: <code>InferenceEngine</code></p> <p>Inference engine that caches the preprocessing for feeding as model context on predict.</p> <p>This will fit the preprocessors on the training data, as well as cache the transformed training data on RAM (not GPU RAM).</p> <p>This saves some time on each predict call, at the cost of increasing the amount of memory in RAM. The main functionality performed at <code>predict()</code> time is to forward pass through the model which is currently done sequentially.</p>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineCachePreprocessing.prepare","title":"prepare  <code>classmethod</code>","text":"<pre><code>prepare(\n    X_train: ndarray,\n    y_train: ndarray,\n    *,\n    cat_ix: list[int],\n    model: PerFeatureTransformer,\n    ensemble_configs: Sequence[EnsembleConfig],\n    n_workers: int,\n    rng: Generator,\n    dtype_byte_size: int,\n    force_inference_dtype: dtype | None,\n    save_peak_mem: bool | Literal[\"auto\"] | float | int\n) -&gt; InferenceEngineCachePreprocessing\n</code></pre> <p>Prepare the inference engine.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>The training data.</p> required <code>y_train</code> <code>ndarray</code> <p>The training target.</p> required <code>cat_ix</code> <code>list[int]</code> <p>The categorical indices.</p> required <code>model</code> <code>PerFeatureTransformer</code> <p>The model to use.</p> required <code>ensemble_configs</code> <code>Sequence[EnsembleConfig]</code> <p>The ensemble configurations to use.</p> required <code>n_workers</code> <code>int</code> <p>The number of workers to use.</p> required <code>rng</code> <code>Generator</code> <p>The random number generator.</p> required <code>dtype_byte_size</code> <code>int</code> <p>The byte size of the dtype.</p> required <code>force_inference_dtype</code> <code>dtype | None</code> <p>The dtype to force inference to.</p> required <code>save_peak_mem</code> <code>bool | Literal['auto'] | float | int</code> <p>Whether to save peak memory usage.</p> required <p>Returns:</p> Type Description <code>InferenceEngineCachePreprocessing</code> <p>The prepared inference engine.</p>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineOnDemand","title":"InferenceEngineOnDemand  <code>dataclass</code>","text":"<p>             Bases: <code>InferenceEngine</code></p> <p>Inference engine that does not cache anything, computes everything as needed.</p> <p>This is one of the slowest ways to run inference, as computation that could be cached is recomputed on every call. However the memory demand is lowest and can be more trivially parallelized across GPUs with some work.</p>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineOnDemand.prepare","title":"prepare  <code>classmethod</code>","text":"<pre><code>prepare(\n    X_train: ndarray,\n    y_train: ndarray,\n    *,\n    cat_ix: list[int],\n    model: PerFeatureTransformer,\n    ensemble_configs: Sequence[EnsembleConfig],\n    rng: Generator,\n    n_workers: int,\n    dtype_byte_size: int,\n    force_inference_dtype: dtype | None,\n    save_peak_mem: bool | Literal[\"auto\"] | float | int\n) -&gt; InferenceEngineOnDemand\n</code></pre> <p>Prepare the inference engine.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>The training data.</p> required <code>y_train</code> <code>ndarray</code> <p>The training target.</p> required <code>cat_ix</code> <code>list[int]</code> <p>The categorical indices.</p> required <code>model</code> <code>PerFeatureTransformer</code> <p>The model to use.</p> required <code>ensemble_configs</code> <code>Sequence[EnsembleConfig]</code> <p>The ensemble configurations to use.</p> required <code>rng</code> <code>Generator</code> <p>The random number generator.</p> required <code>n_workers</code> <code>int</code> <p>The number of workers to use.</p> required <code>dtype_byte_size</code> <code>int</code> <p>The byte size of the dtype.</p> required <code>force_inference_dtype</code> <code>dtype | None</code> <p>The dtype to force inference to.</p> required <code>save_peak_mem</code> <code>bool | Literal['auto'] | float | int</code> <p>Whether to save peak memory usage.</p> required"},{"location":"reference/tabpfn/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing","title":"preprocessing","text":"<p>Defines the preprocessing configurations that define the ensembling of different members.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.ClassifierEnsembleConfig","title":"ClassifierEnsembleConfig  <code>dataclass</code>","text":"<p>             Bases: <code>EnsembleConfig</code></p> <p>Configuration for a classifier ensemble member.</p> <p>See EnsembleConfig for more details.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.ClassifierEnsembleConfig.generate_for_classification","title":"generate_for_classification  <code>classmethod</code>","text":"<pre><code>generate_for_classification(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    class_shift_method: Literal[\"rotate\", \"shuffle\"] | None,\n    n_classes: int,\n    random_state: int | Generator | None\n) -&gt; list[ClassifierEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for classification.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>class_shift_method</code> <code>Literal['rotate', 'shuffle'] | None</code> <p>How to shift classes for classpermutation.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[ClassifierEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.ClassifierEnsembleConfig.generate_for_regression","title":"generate_for_regression  <code>classmethod</code>","text":"<pre><code>generate_for_regression(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    target_transforms: Sequence[\n        TransformerMixin | Pipeline | None\n    ],\n    random_state: int | Generator | None\n) -&gt; list[RegressorEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for regression.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>target_transforms</code> <code>Sequence[TransformerMixin | Pipeline | None]</code> <p>Target transformations to apply.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[RegressorEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.ClassifierEnsembleConfig.to_pipeline","title":"to_pipeline","text":"<pre><code>to_pipeline(\n    *, random_state: int | Generator | None\n) -&gt; SequentialFeatureTransformer\n</code></pre> <p>Convert the ensemble configuration to a preprocessing pipeline.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.EnsembleConfig","title":"EnsembleConfig  <code>dataclass</code>","text":"<p>Configuration for an ensemble member.</p> <p>Attributes:</p> Name Type Description <code>feature_shift_count</code> <code>int</code> <p>How much to shift the features columns.</p> <code>class_permutation</code> <code>int</code> <p>Permutation to apply to classes</p> <code>preprocess_config</code> <code>PreprocessorConfig</code> <p>Preprocessor configuration to use.</p> <code>subsample_ix</code> <code>NDArray[int64] | None</code> <p>Indices of samples to use for this ensemble member. If <code>None</code>, no subsampling is done.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.EnsembleConfig.generate_for_classification","title":"generate_for_classification  <code>classmethod</code>","text":"<pre><code>generate_for_classification(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    class_shift_method: Literal[\"rotate\", \"shuffle\"] | None,\n    n_classes: int,\n    random_state: int | Generator | None\n) -&gt; list[ClassifierEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for classification.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>class_shift_method</code> <code>Literal['rotate', 'shuffle'] | None</code> <p>How to shift classes for classpermutation.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[ClassifierEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.EnsembleConfig.generate_for_regression","title":"generate_for_regression  <code>classmethod</code>","text":"<pre><code>generate_for_regression(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    target_transforms: Sequence[\n        TransformerMixin | Pipeline | None\n    ],\n    random_state: int | Generator | None\n) -&gt; list[RegressorEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for regression.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>target_transforms</code> <code>Sequence[TransformerMixin | Pipeline | None]</code> <p>Target transformations to apply.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[RegressorEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.EnsembleConfig.to_pipeline","title":"to_pipeline","text":"<pre><code>to_pipeline(\n    *, random_state: int | Generator | None\n) -&gt; SequentialFeatureTransformer\n</code></pre> <p>Convert the ensemble configuration to a preprocessing pipeline.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.PreprocessorConfig","title":"PreprocessorConfig  <code>dataclass</code>","text":"<p>Configuration for data preprocessors.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['per_feature', 'power', 'safepower', 'power_box', 'safepower_box', 'quantile_uni_coarse', 'quantile_norm_coarse', 'quantile_uni', 'quantile_norm', 'quantile_uni_fine', 'quantile_norm_fine', 'robust', 'kdi', 'none', 'kdi_random_alpha', 'kdi_uni', 'kdi_random_alpha_uni', 'adaptive', 'norm_and_kdi', 'kdi_alpha_0.3_uni', 'kdi_alpha_0.5_uni', 'kdi_alpha_0.8_uni', 'kdi_alpha_1.0_uni', 'kdi_alpha_1.2_uni', 'kdi_alpha_1.5_uni', 'kdi_alpha_2.0_uni', 'kdi_alpha_3.0_uni', 'kdi_alpha_5.0_uni', 'kdi_alpha_0.3', 'kdi_alpha_0.5', 'kdi_alpha_0.8', 'kdi_alpha_1.0', 'kdi_alpha_1.2', 'kdi_alpha_1.5', 'kdi_alpha_2.0', 'kdi_alpha_3.0', 'kdi_alpha_5.0']</code> <p>Name of the preprocessor.</p> <code>categorical_name</code> <code>Literal['none', 'numeric', 'onehot', 'ordinal', 'ordinal_shuffled', 'ordinal_very_common_categories_shuffled']</code> <p>Name of the categorical encoding method. Options: \"none\", \"numeric\", \"onehot\", \"ordinal\", \"ordinal_shuffled\", \"none\".</p> <code>append_original</code> <code>bool</code> <p>Whether to append original features to the transformed features</p> <code>subsample_features</code> <code>float</code> <p>Fraction of features to subsample. -1 means no subsampling.</p> <code>global_transformer_name</code> <code>str | None</code> <p>Name of the global transformer to use.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.RegressorEnsembleConfig","title":"RegressorEnsembleConfig  <code>dataclass</code>","text":"<p>             Bases: <code>EnsembleConfig</code></p> <p>Configuration for a regression ensemble member.</p> <p>See EnsembleConfig for more details.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.RegressorEnsembleConfig.generate_for_classification","title":"generate_for_classification  <code>classmethod</code>","text":"<pre><code>generate_for_classification(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    class_shift_method: Literal[\"rotate\", \"shuffle\"] | None,\n    n_classes: int,\n    random_state: int | Generator | None\n) -&gt; list[ClassifierEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for classification.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>class_shift_method</code> <code>Literal['rotate', 'shuffle'] | None</code> <p>How to shift classes for classpermutation.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[ClassifierEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.RegressorEnsembleConfig.generate_for_regression","title":"generate_for_regression  <code>classmethod</code>","text":"<pre><code>generate_for_regression(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    target_transforms: Sequence[\n        TransformerMixin | Pipeline | None\n    ],\n    random_state: int | Generator | None\n) -&gt; list[RegressorEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for regression.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>target_transforms</code> <code>Sequence[TransformerMixin | Pipeline | None]</code> <p>Target transformations to apply.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[RegressorEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.RegressorEnsembleConfig.to_pipeline","title":"to_pipeline","text":"<pre><code>to_pipeline(\n    *, random_state: int | Generator | None\n) -&gt; SequentialFeatureTransformer\n</code></pre> <p>Convert the ensemble configuration to a preprocessing pipeline.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.balance","title":"balance","text":"<pre><code>balance(x: Iterable[T], n: int) -&gt; list[T]\n</code></pre> <p>Take a list of elements and make a new list where each appears <code>n</code> times.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.default_classifier_preprocessor_configs","title":"default_classifier_preprocessor_configs","text":"<pre><code>default_classifier_preprocessor_configs() -&gt; (\n    list[PreprocessorConfig]\n)\n</code></pre> <p>Default preprocessor configurations for classification.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.default_regressor_preprocessor_configs","title":"default_regressor_preprocessor_configs","text":"<pre><code>default_regressor_preprocessor_configs() -&gt; (\n    list[PreprocessorConfig]\n)\n</code></pre> <p>Default preprocessor configurations for regression.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.fit_preprocessing","title":"fit_preprocessing","text":"<pre><code>fit_preprocessing(\n    configs: Sequence[EnsembleConfig],\n    X_train: ndarray,\n    y_train: ndarray,\n    *,\n    random_state: int | Generator | None,\n    cat_ix: list[int],\n    n_workers: int,\n    parallel_mode: Literal[\"block\", \"as-ready\", \"in-order\"]\n) -&gt; Iterator[\n    tuple[\n        EnsembleConfig,\n        SequentialFeatureTransformer,\n        ndarray,\n        ndarray,\n        list[int],\n    ]\n]\n</code></pre> <p>Fit preprocessing pipelines in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>configs</code> <code>Sequence[EnsembleConfig]</code> <p>List of ensemble configurations.</p> required <code>X_train</code> <code>ndarray</code> <p>Training data.</p> required <code>y_train</code> <code>ndarray</code> <p>Training target.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <code>cat_ix</code> <code>list[int]</code> <p>Indices of categorical features.</p> required <code>n_workers</code> <code>int</code> <p>Number of workers to use.</p> required <code>parallel_mode</code> <code>Literal['block', 'as-ready', 'in-order']</code> <p>Parallel mode to use.</p> <ul> <li><code>\"block\"</code>: Blocks until all workers are done. Returns in order.</li> <li><code>\"as-ready\"</code>: Returns results as they are ready. Any order.</li> <li><code>\"in-order\"</code>: Returns results in order, blocking only in the order that     needs to be returned in.</li> </ul> required <p>Returns:</p> Type Description <code>EnsembleConfig</code> <p>Iterator of tuples containing the ensemble configuration, the fitted</p> <code>SequentialFeatureTransformer</code> <p>preprocessing pipeline, the transformed training data, the transformed target,</p> <code>ndarray</code> <p>and the indices of categorical features.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.fit_preprocessing_one","title":"fit_preprocessing_one","text":"<pre><code>fit_preprocessing_one(\n    config: EnsembleConfig,\n    X_train: ndarray,\n    y_train: ndarray,\n    random_state: int | Generator | None = None,\n    *,\n    cat_ix: list[int]\n) -&gt; tuple[\n    EnsembleConfig,\n    SequentialFeatureTransformer,\n    ndarray,\n    ndarray,\n    list[int],\n]\n</code></pre> <p>Fit preprocessing pipeline for a single ensemble configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EnsembleConfig</code> <p>Ensemble configuration.</p> required <code>X_train</code> <code>ndarray</code> <p>Training data.</p> required <code>y_train</code> <code>ndarray</code> <p>Training target.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random seed.</p> <code>None</code> <code>cat_ix</code> <code>list[int]</code> <p>Indices of categorical features.</p> required <p>Returns:</p> Type Description <code>EnsembleConfig</code> <p>Tuple containing the ensemble configuration, the fitted preprocessing pipeline,</p> <code>SequentialFeatureTransformer</code> <p>the transformed training data, the transformed target, and the indices of</p> <code>ndarray</code> <p>categorical features.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.generate_index_permutations","title":"generate_index_permutations","text":"<pre><code>generate_index_permutations(\n    n: int,\n    *,\n    max_index: int,\n    subsample: int | float,\n    random_state: int | Generator | None\n) -&gt; list[NDArray[int64]]\n</code></pre> <p>Generate indices for subsampling from the data.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of indices to generate.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate.</p> required <code>subsample</code> <code>int | float</code> <p>Number of indices to subsample. If <code>int</code>, subsample that many indices. If float, subsample that fraction of indices. random_state: Random number generator.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[NDArray[int64]]</code> <p>List of indices to subsample.</p>"},{"location":"reference/tabpfn/regressor/","title":"Regressor","text":""},{"location":"reference/tabpfn/regressor/#tabpfn.regressor","title":"regressor","text":"<p>TabPFNRegressor class.</p> <p>Example</p> <pre><code>import sklearn.datasets\nfrom tabpfn import TabPFNRegressor\n\nmodel = TabPFNRegressor()\nX, y = sklearn.datasets.make_regression(n_samples=50, n_features=10)\n\nmodel.fit(X, y)\npredictions = model.predict(X)\n</code></pre>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor","title":"TabPFNRegressor","text":"<p>             Bases: <code>RegressorMixin</code>, <code>BaseEstimator</code></p> <p>TabPFNRegressor class.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.bardist_","title":"bardist_  <code>instance-attribute</code>","text":"<pre><code>bardist_: FullSupportBarDistribution\n</code></pre> <p>The bar distribution of the target variable, used by the model.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.config_","title":"config_  <code>instance-attribute</code>","text":"<pre><code>config_: InferenceConfig\n</code></pre> <p>The configuration of the loaded model to be used for inference.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.device_","title":"device_  <code>instance-attribute</code>","text":"<pre><code>device_: device\n</code></pre> <p>The device determined to be used.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.executor_","title":"executor_  <code>instance-attribute</code>","text":"<pre><code>executor_: InferenceEngine\n</code></pre> <p>The inference engine used to make predictions.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.feature_names_in_","title":"feature_names_in_  <code>instance-attribute</code>","text":"<pre><code>feature_names_in_: NDArray[Any]\n</code></pre> <p>The feature names of the input data.</p> <p>May not be set if the input data does not have feature names, such as with a numpy array.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.forced_inference_dtype_","title":"forced_inference_dtype_  <code>instance-attribute</code>","text":"<pre><code>forced_inference_dtype_: _dtype | None\n</code></pre> <p>The forced inference dtype for the model based on <code>inference_precision</code>.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.inferred_categorical_indices_","title":"inferred_categorical_indices_  <code>instance-attribute</code>","text":"<pre><code>inferred_categorical_indices_: list[int]\n</code></pre> <p>The indices of the columns that were inferred to be categorical, as a product of any features deemed categorical by the user and what would work best for the model.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.interface_config_","title":"interface_config_  <code>instance-attribute</code>","text":"<pre><code>interface_config_: ModelInterfaceConfig\n</code></pre> <p>Additional configuration of the interface for expert users.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.n_features_in_","title":"n_features_in_  <code>instance-attribute</code>","text":"<pre><code>n_features_in_: int\n</code></pre> <p>The number of features in the input data used during <code>fit()</code>.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.n_outputs_","title":"n_outputs_  <code>instance-attribute</code>","text":"<pre><code>n_outputs_: Literal[1]\n</code></pre> <p>The number of outputs the model supports. Only 1 for now</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.preprocessor_","title":"preprocessor_  <code>instance-attribute</code>","text":"<pre><code>preprocessor_: ColumnTransformer\n</code></pre> <p>The column transformer used to preprocess the input data to be numeric.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.renormalized_criterion_","title":"renormalized_criterion_  <code>instance-attribute</code>","text":"<pre><code>renormalized_criterion_: FullSupportBarDistribution\n</code></pre> <p>The normalized bar distribution used for computing the predictions.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.use_autocast_","title":"use_autocast_  <code>instance-attribute</code>","text":"<pre><code>use_autocast_: bool\n</code></pre> <p>Whether torch's autocast should be used.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.y_train_mean_","title":"y_train_mean_  <code>instance-attribute</code>","text":"<pre><code>y_train_mean_: float\n</code></pre> <p>The mean of the target variable during training.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.y_train_std","title":"y_train_std  <code>instance-attribute</code>","text":"<pre><code>y_train_std: float\n</code></pre> <p>The standard deviation of the target variable during training.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.fit","title":"fit","text":"<pre><code>fit(X: XType, y: YType) -&gt; Self\n</code></pre> <p>Fit the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input data.</p> required <code>y</code> <code>YType</code> <p>The target variable.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>self</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(\n    X: XType,\n    *,\n    output_type: Literal[\n        \"mean\",\n        \"median\",\n        \"mode\",\n        \"quantiles\",\n        \"full\",\n        \"main\",\n    ] = \"mean\",\n    quantiles: list[float] | None = None\n) -&gt; (\n    ndarray\n    | list[ndarray]\n    | dict[str, ndarray]\n    | dict[str, ndarray | FullSupportBarDistribution]\n)\n</code></pre> <p>Predict the target variable.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input data.</p> required <code>output_type</code> <code>Literal['mean', 'median', 'mode', 'quantiles', 'full', 'main']</code> <p>Determines the type of output to return.</p> <ul> <li>If <code>\"mean\"</code>, we return the mean over the predicted distribution.</li> <li>If <code>\"median\"</code>, we return the median over the predicted distribution.</li> <li>If <code>\"mode\"</code>, we return the mode over the predicted distribution.</li> <li>If <code>\"quantiles\"</code>, we return the quantiles of the predicted     distribution. The parameter <code>output_quantiles</code> determines which     quantiles are returned.</li> <li>If <code>\"main\"</code>, we return the all output types above in a dict.</li> <li>If <code>\"full\"</code>, we return the full output of the model, including the   logits and the criterion, and all the output types from \"main\".</li> </ul> <code>'mean'</code> <code>quantiles</code> <code>list[float] | None</code> <p>The quantiles to return if <code>output=\"quantiles\"</code>.</p> <p>By default, the <code>[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]</code> quantiles are returned. The predictions per quantile match the input order.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray | list[ndarray] | dict[str, ndarray] | dict[str, ndarray | FullSupportBarDistribution]</code> <p>The predicted target variable or a list of predictions per quantile.</p>"},{"location":"reference/tabpfn/utils/","title":"Utils","text":""},{"location":"reference/tabpfn/utils/#tabpfn.utils","title":"utils","text":"<p>A collection of random utilities for the TabPFN models.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.infer_categorical_features","title":"infer_categorical_features","text":"<pre><code>infer_categorical_features(\n    X: ndarray,\n    *,\n    provided: Sequence[int] | None,\n    min_samples_for_inference: int,\n    max_unique_for_category: int,\n    min_unique_for_numerical: int\n) -&gt; list[int]\n</code></pre> <p>Infer the categorical features from the given data.</p> <p>Note</p> <p>This function may infer particular columns to not be categorical as defined by what suits the model predictions and it's pre-training.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The data to infer the categorical features from.</p> required <code>provided</code> <code>Sequence[int] | None</code> <p>Any user provided indices of what is considered categorical.</p> required <code>min_samples_for_inference</code> <code>int</code> <p>The minimum number of samples required for automatic inference of features which were not provided as categorical.</p> required <code>max_unique_for_category</code> <code>int</code> <p>The maximum number of unique values for a feature to be considered categorical.</p> required <code>min_unique_for_numerical</code> <code>int</code> <p>The minimum number of unique values for a feature to be considered numerical.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The indices of inferred categorical features.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.infer_device_and_type","title":"infer_device_and_type","text":"<pre><code>infer_device_and_type(\n    device: str | device | None,\n) -&gt; device\n</code></pre> <p>Infer the device and data type from the given device string.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str | device | None</code> <p>The device to infer the type from.</p> required <p>Returns:</p> Type Description <code>device</code> <p>The inferred device</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.infer_fp16_inference_mode","title":"infer_fp16_inference_mode","text":"<pre><code>infer_fp16_inference_mode(\n    device: device, *, enable: bool | None\n) -&gt; bool\n</code></pre> <p>Infer whether fp16 inference should be enabled.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>The device to validate against.</p> required <code>enable</code> <code>bool | None</code> <p>Whether it should be enabled, <code>True</code> or <code>False</code>, otherwise if <code>None</code>, detect if it's possible and use it if so.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether to use fp16 inference or not.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If fp16 inference was enabled and device type does not support it.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.infer_random_state","title":"infer_random_state","text":"<pre><code>infer_random_state(\n    random_state: int | RandomState | Generator | None,\n) -&gt; tuple[int, Generator]\n</code></pre> <p>Infer the random state from the given input.</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>int | RandomState | Generator | None</code> <p>The random state to infer.</p> required <p>Returns:</p> Type Description <code>tuple[int, Generator]</code> <p>A static integer seed and a random number generator.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.is_autocast_available","title":"is_autocast_available","text":"<pre><code>is_autocast_available(device_type: str) -&gt; bool\n</code></pre> <p>Infer whether autocast is available for the given device type.</p> <p>Parameters:</p> Name Type Description Default <code>device_type</code> <code>str</code> <p>The device type to check for autocast availability.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether autocast is available for the given device type.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.load_model_criterion_config","title":"load_model_criterion_config","text":"<pre><code>load_model_criterion_config(\n    model_path: None | str | Path,\n    *,\n    check_bar_distribution_criterion: bool,\n    cache_trainset_representation: bool,\n    which: Literal[\"regressor\", \"classifier\"],\n    version: Literal[\"v2\"] = \"v2\",\n    download: bool,\n    model_seed: int\n) -&gt; tuple[\n    PerFeatureTransformer,\n    BCEWithLogitsLoss\n    | CrossEntropyLoss\n    | FullSupportBarDistribution,\n    InferenceConfig,\n]\n</code></pre> <p>Load the model, criterion, and config from the given path.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>None | str | Path</code> <p>The path to the model.</p> required <code>check_bar_distribution_criterion</code> <code>bool</code> <p>Whether to check if the criterion is a FullSupportBarDistribution, which is the expected criterion for models trained for regression.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether the model should know to cache the trainset representation.</p> required <code>which</code> <code>Literal['regressor', 'classifier']</code> <p>Whether the model is a regressor or classifier.</p> required <code>version</code> <code>Literal['v2']</code> <p>The version of the model.</p> <code>'v2'</code> <code>download</code> <code>bool</code> <p>Whether to download the model if it doesn't exist.</p> required <code>model_seed</code> <code>int</code> <p>The seed of the model.</p> required <p>Returns:</p> Type Description <code>tuple[PerFeatureTransformer, BCEWithLogitsLoss | CrossEntropyLoss | FullSupportBarDistribution, InferenceConfig]</code> <p>The model, criterion, and config.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.translate_probs_across_borders","title":"translate_probs_across_borders","text":"<pre><code>translate_probs_across_borders(\n    logits: Tensor, *, frm: Tensor, to: Tensor\n) -&gt; Tensor\n</code></pre> <p>Translate the probabilities across the borders.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The logits defining the distribution to translate.</p> required <code>frm</code> <code>Tensor</code> <p>The borders to translate from.</p> required <code>to</code> <code>Tensor</code> <p>The borders to translate to.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The translated probabilities.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.update_encoder_outlier_params","title":"update_encoder_outlier_params","text":"<pre><code>update_encoder_outlier_params(\n    model: Module,\n    remove_outliers_std: float | None,\n    seed: int | None,\n    *,\n    inplace: Literal[True]\n) -&gt; None\n</code></pre> <p>Update the encoder to handle outliers in the model.</p> <p>Warning</p> <p>This only happens inplace.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to update.</p> required <code>remove_outliers_std</code> <code>float | None</code> <p>The standard deviation to remove outliers.</p> required <code>seed</code> <code>int | None</code> <p>The seed to use, if any.</p> required <code>inplace</code> <code>Literal[True]</code> <p>Whether to do the operation inplace.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>inplace</code> is not <code>True</code>.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.validate_X_predict","title":"validate_X_predict","text":"<pre><code>validate_X_predict(\n    X: XType, estimator: TabPFNRegressor | TabPFNClassifier\n) -&gt; ndarray\n</code></pre> <p>Validate the input data for prediction.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.validate_Xy_fit","title":"validate_Xy_fit","text":"<pre><code>validate_Xy_fit(\n    X: XType,\n    y: YType,\n    estimator: TabPFNRegressor | TabPFNClassifier,\n    *,\n    max_num_features: int,\n    max_num_samples: int,\n    ensure_y_numeric: bool = False,\n    ignore_pretraining_limits: bool = False\n) -&gt; tuple[ndarray, ndarray, NDArray[Any] | None, int]\n</code></pre> <p>Validate the input data for fitting.</p>"},{"location":"reference/tabpfn/model/bar_distribution/","title":"Bar distribution","text":""},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution","title":"bar_distribution","text":""},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution","title":"BarDistribution","text":"<p>             Bases: <code>Module</code></p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.average_bar_distributions_into_this","title":"average_bar_distributions_into_this","text":"<pre><code>average_bar_distributions_into_this(\n    list_of_bar_distributions: Sequence[BarDistribution],\n    list_of_logits: Sequence[Tensor],\n    *,\n    average_logits: bool = False\n) -&gt; Tensor\n</code></pre> <p>:param list_of_bar_distributions: :param list_of_logits: :param average_logits: :return:</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.cdf","title":"cdf","text":"<pre><code>cdf(logits: Tensor, ys: Tensor) -&gt; Tensor\n</code></pre> <p>Calculates the cdf of the distribution described by the logits. The cdf is scaled by the width of the bars.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>tensor of shape (batch_size, ..., num_bars) with the logits describing the distribution</p> required <code>ys</code> <code>Tensor</code> <p>tensor of shape (batch_size, ..., n_ys to eval) or (n_ys to eval) with the targets.</p> required"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.cdf_temporary","title":"cdf_temporary","text":"<pre><code>cdf_temporary(logits: Tensor) -&gt; Tensor\n</code></pre> <p>Cumulative distribution function.</p> <p>TODO: this already exists here, make sure to merge, at the moment still used.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.get_probs_for_different_borders","title":"get_probs_for_different_borders","text":"<pre><code>get_probs_for_different_borders(\n    logits: Tensor, new_borders: Tensor\n) -&gt; Tensor\n</code></pre> <p>The logits describe the density of the distribution over the current self.borders.</p> <p>This function returns the logits if the self.borders were changed to new_borders. This is useful to average the logits of different models.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.icdf","title":"icdf","text":"<pre><code>icdf(logits: Tensor, left_prob: float) -&gt; Tensor\n</code></pre> <p>Implementation of the quantile function :param logits: Tensor of any shape, with the last dimension being logits :param left_prob: float: The probability mass to the left of the result. :return: Position with <code>left_prob</code> probability weight to the left.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.mean_of_square","title":"mean_of_square","text":"<pre><code>mean_of_square(logits: Tensor) -&gt; Tensor\n</code></pre> <p>Computes E[x^2].</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Output of the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>mean of square</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.pi","title":"pi","text":"<pre><code>pi(\n    logits: Tensor,\n    best_f: float | Tensor,\n    *,\n    maximize: bool = True\n) -&gt; Tensor\n</code></pre> <p>Acquisition Function: Probability of Improvement.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>as returned by Transformer</p> required <code>best_f</code> <code>float | Tensor</code> <p>best evaluation so far (the incumbent)</p> required <code>maximize</code> <code>bool</code> <p>whether to maximize</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>probability of improvement</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.plot","title":"plot","text":"<pre><code>plot(\n    logits: Tensor,\n    ax: Axes | None = None,\n    zoom_to_quantile: float | None = None,\n    **kwargs: Any\n) -&gt; Axes\n</code></pre> <p>Plots the distribution.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.ucb","title":"ucb","text":"<pre><code>ucb(\n    logits: Tensor,\n    best_f: float,\n    rest_prob: float = 1 - 0.682 / 2,\n    *,\n    maximize: bool = True\n) -&gt; Tensor\n</code></pre> <p>UCB utility. Rest Prob is the amount of utility above (below) the confidence interval that is ignored.</p> <p>Higher rest_prob is equivalent to lower beta in the standard GP-UCB formulation.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Logits, as returned by the Transformer.</p> required <code>rest_prob</code> <code>float</code> <p>The amount of utility above (below) the confidence interval that is ignored.</p> <p>The default is equivalent to using GP-UCB with <code>beta=1</code>. To get the corresponding <code>beta</code>, where <code>beta</code> is from the standard GP definition of UCB <code>ucb_utility = mean + beta * std</code>, you can use this computation:</p> <p><code>beta = math.sqrt(2)*torch.erfinv(torch.tensor(2*(1-rest_prob)-1))</code></p> <code>1 - 0.682 / 2</code> <code>best_f</code> <code>float</code> <p>Unused</p> required <code>maximize</code> <code>bool</code> <p>Whether to maximize.</p> <code>True</code>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution","title":"FullSupportBarDistribution","text":"<p>             Bases: <code>BarDistribution</code></p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.average_bar_distributions_into_this","title":"average_bar_distributions_into_this","text":"<pre><code>average_bar_distributions_into_this(\n    list_of_bar_distributions: Sequence[BarDistribution],\n    list_of_logits: Sequence[Tensor],\n    *,\n    average_logits: bool = False\n) -&gt; Tensor\n</code></pre> <p>:param list_of_bar_distributions: :param list_of_logits: :param average_logits: :return:</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.cdf","title":"cdf","text":"<pre><code>cdf(logits: Tensor, ys: Tensor) -&gt; Tensor\n</code></pre> <p>Calculates the cdf of the distribution described by the logits. The cdf is scaled by the width of the bars.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>tensor of shape (batch_size, ..., num_bars) with the logits describing the distribution</p> required <code>ys</code> <code>Tensor</code> <p>tensor of shape (batch_size, ..., n_ys to eval) or (n_ys to eval) with the targets.</p> required"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.cdf_temporary","title":"cdf_temporary","text":"<pre><code>cdf_temporary(logits: Tensor) -&gt; Tensor\n</code></pre> <p>Cumulative distribution function.</p> <p>TODO: this already exists here, make sure to merge, at the moment still used.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.ei_for_halfnormal","title":"ei_for_halfnormal","text":"<pre><code>ei_for_halfnormal(\n    scale: float,\n    best_f: Tensor | float,\n    *,\n    maximize: bool = True\n) -&gt; Tensor\n</code></pre> <p>EI for a standard normal distribution with mean 0 and variance <code>scale</code> times 2.</p> <p>Which is the same as the half normal EI. Tested this with MC approximation:</p> <pre><code>ei_for_halfnormal = lambda scale, best_f: (torch.distributions.HalfNormal(torch.tensor(scale)).sample((10_000_000,))- best_f ).clamp(min=0.).mean()\nprint([(ei_for_halfnormal(scale,best_f), FullSupportBarDistribution().ei_for_halfnormal(scale,best_f)) for scale in [0.1,1.,10.] for best_f in [.1,10.,4.]])\n</code></pre>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.forward","title":"forward","text":"<pre><code>forward(\n    logits: Tensor,\n    y: Tensor,\n    mean_prediction_logits: Tensor | None = None,\n) -&gt; Tensor\n</code></pre> <p>Returns the negative log density (the loss).</p> <p>y: T x B, logits: T x B x self.num_bars.</p> <p>:param logits: Tensor of shape T x B x self.num_bars :param y: Tensor of shape T x B :param mean_prediction_logits: :return:</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.get_probs_for_different_borders","title":"get_probs_for_different_borders","text":"<pre><code>get_probs_for_different_borders(\n    logits: Tensor, new_borders: Tensor\n) -&gt; Tensor\n</code></pre> <p>The logits describe the density of the distribution over the current self.borders.</p> <p>This function returns the logits if the self.borders were changed to new_borders. This is useful to average the logits of different models.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.icdf","title":"icdf","text":"<pre><code>icdf(logits: Tensor, left_prob: float) -&gt; Tensor\n</code></pre> <p>Implementation of the quantile function :param logits: Tensor of any shape, with the last dimension being logits :param left_prob: float: The probability mass to the left of the result. :return: Position with <code>left_prob</code> probability weight to the left.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.mean_of_square","title":"mean_of_square","text":"<pre><code>mean_of_square(logits: Tensor) -&gt; Tensor\n</code></pre> <p>Computes E[x^2].</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Output of the model.</p> required"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.pdf","title":"pdf","text":"<pre><code>pdf(logits: Tensor, y: Tensor) -&gt; Tensor\n</code></pre> <p>Probability density function at y.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.pi","title":"pi","text":"<pre><code>pi(\n    logits: Tensor,\n    best_f: Tensor | float,\n    *,\n    maximize: bool = True\n) -&gt; Tensor\n</code></pre> <p>Acquisition Function: Probability of Improvement.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>as returned by Transformer (evaluation_points x batch x feature_dim)</p> required <code>best_f</code> <code>Tensor | float</code> <p>best evaluation so far (the incumbent)</p> required <code>maximize</code> <code>bool</code> <p>whether to maximize</p> <code>True</code>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.plot","title":"plot","text":"<pre><code>plot(\n    logits: Tensor,\n    ax: Axes | None = None,\n    zoom_to_quantile: float | None = None,\n    **kwargs: Any\n) -&gt; Axes\n</code></pre> <p>Plots the distribution.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.sample","title":"sample","text":"<pre><code>sample(logits: Tensor, t: float = 1.0) -&gt; Tensor\n</code></pre> <p>Samples values from the distribution.</p> <p>Temperature t.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.ucb","title":"ucb","text":"<pre><code>ucb(\n    logits: Tensor,\n    best_f: float,\n    rest_prob: float = 1 - 0.682 / 2,\n    *,\n    maximize: bool = True\n) -&gt; Tensor\n</code></pre> <p>UCB utility. Rest Prob is the amount of utility above (below) the confidence interval that is ignored.</p> <p>Higher rest_prob is equivalent to lower beta in the standard GP-UCB formulation.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Logits, as returned by the Transformer.</p> required <code>rest_prob</code> <code>float</code> <p>The amount of utility above (below) the confidence interval that is ignored.</p> <p>The default is equivalent to using GP-UCB with <code>beta=1</code>. To get the corresponding <code>beta</code>, where <code>beta</code> is from the standard GP definition of UCB <code>ucb_utility = mean + beta * std</code>, you can use this computation:</p> <p><code>beta = math.sqrt(2)*torch.erfinv(torch.tensor(2*(1-rest_prob)-1))</code></p> <code>1 - 0.682 / 2</code> <code>best_f</code> <code>float</code> <p>Unused</p> required <code>maximize</code> <code>bool</code> <p>Whether to maximize.</p> <code>True</code>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.get_bucket_limits","title":"get_bucket_limits","text":"<pre><code>get_bucket_limits(\n    num_outputs: int,\n    full_range: tuple | None = None,\n    ys: Tensor | None = None,\n    *,\n    verbose: bool = False,\n    widen_bucket_limits_factor: float | None = None\n) -&gt; Tensor\n</code></pre> <p>Decide for a set of bucket limits based on a distritbution of ys.</p> <p>Parameters:</p> Name Type Description Default <code>num_outputs</code> <code>int</code> <p>This is only tested for num_outputs=1, but should work for larger num_outputs as well.</p> required <code>full_range</code> <code>tuple | None</code> <p>If ys is not passed, this is the range of the ys that should be used to estimate the bucket limits.</p> <code>None</code> <code>ys</code> <code>Tensor | None</code> <p>If ys is passed, this is the ys that should be used to estimate the bucket limits. Do not pass full_range in this case.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Unused</p> <code>False</code> <code>widen_bucket_limits_factor</code> <code>float | None</code> <p>If set, the bucket limits are widened by this factor. This allows to have a slightly larger range than the actual data.</p> <code>None</code>"},{"location":"reference/tabpfn/model/config/","title":"Config","text":""},{"location":"reference/tabpfn/model/config/#tabpfn.model.config","title":"config","text":""},{"location":"reference/tabpfn/model/config/#tabpfn.model.config.InferenceConfig","title":"InferenceConfig  <code>dataclass</code>","text":"<p>Configuration for the TabPFN model.</p>"},{"location":"reference/tabpfn/model/config/#tabpfn.model.config.InferenceConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config: dict) -&gt; InferenceConfig\n</code></pre> <p>Create a Config object from a dictionary.</p> <p>This method also does some sanity checking initially.</p>"},{"location":"reference/tabpfn/model/encoders/","title":"Encoders","text":""},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders","title":"encoders","text":""},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.CategoricalInputEncoderPerFeatureEncoderStep","title":"CategoricalInputEncoderPerFeatureEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Expects input of size 1.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.CategoricalInputEncoderPerFeatureEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.FrequencyFeatureEncoderStep","title":"FrequencyFeatureEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to add frequency-based features to the input.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.FrequencyFeatureEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.InputEncoder","title":"InputEncoder","text":"<p>             Bases: <code>Module</code></p> <p>Base class for input encoders.</p> <p>All input encoders should subclass this class and implement the <code>forward</code> method.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.InputEncoder.forward","title":"forward","text":"<pre><code>forward(x: Tensor, single_eval_pos: int) -&gt; Tensor\n</code></pre> <p>Encode the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to encode.</p> required <code>single_eval_pos</code> <code>int</code> <p>The position to use for single evaluation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded tensor.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.InputNormalizationEncoderStep","title":"InputNormalizationEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to normalize the input in different ways.</p> <p>Can be used to normalize the input to a ranking, remove outliers, or normalize the input to have unit variance.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.InputNormalizationEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.InputNormalizationEncoderStep.reset_seed","title":"reset_seed","text":"<pre><code>reset_seed() -&gt; None\n</code></pre> <p>Reset the random seed.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.LinearInputEncoder","title":"LinearInputEncoder","text":"<p>             Bases: <code>Module</code></p> <p>A simple linear input encoder.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.LinearInputEncoder.forward","title":"forward","text":"<pre><code>forward(*x: Tensor, **kwargs: Any) -&gt; tuple[Tensor]\n</code></pre> <p>Apply the linear transformation to the input.</p> <p>Parameters:</p> Name Type Description Default <code>*x</code> <code>Tensor</code> <p>The input tensors to concatenate and transform.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>A tuple containing the transformed tensor.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.LinearInputEncoderStep","title":"LinearInputEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>A simple linear input encoder step.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.LinearInputEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.NanHandlingEncoderStep","title":"NanHandlingEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to handle NaN and infinite values in the input.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.NanHandlingEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.RemoveDuplicateFeaturesEncoderStep","title":"RemoveDuplicateFeaturesEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to remove duplicate features.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.RemoveDuplicateFeaturesEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.RemoveEmptyFeaturesEncoderStep","title":"RemoveEmptyFeaturesEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to remove empty (constant) features.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.RemoveEmptyFeaturesEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.SeqEncStep","title":"SeqEncStep","text":"<p>             Bases: <code>Module</code></p> <p>Abstract base class for sequential encoder steps.</p> <p>SeqEncStep is a wrapper around a module that defines the expected input keys and the produced output keys. The outputs are assigned to the output keys in the order specified by <code>out_keys</code>.</p> <p>Subclasses should either implement <code>_forward</code> or <code>_fit</code> and <code>_transform</code>. Subclasses that transform <code>x</code> should always use <code>_fit</code> and <code>_transform</code>, creating any state that depends on the train set in <code>_fit</code> and using it in <code>_transform</code>. This allows fitting on data first and doing inference later without refitting. Subclasses that work with <code>y</code> can alternatively use <code>_forward</code> instead.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.SeqEncStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.SequentialEncoder","title":"SequentialEncoder","text":"<p>             Bases: <code>Sequential</code>, <code>InputEncoder</code></p> <p>An encoder that applies a sequence of encoder steps.</p> <p>SequentialEncoder allows building an encoder from a sequence of EncoderSteps. The input is passed through each step in the provided order.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.SequentialEncoder.forward","title":"forward","text":"<pre><code>forward(input: dict, **kwargs: Any) -&gt; Tensor\n</code></pre> <p>Apply the sequence of encoder steps to the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>dict</code> <p>The input state dictionary. If the input is not a dict and the first layer expects one input key, the input tensor is mapped to the key expected by the first layer.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to each encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output of the final encoder step.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.VariableNumFeaturesEncoderStep","title":"VariableNumFeaturesEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to handle variable number of features.</p> <p>Transforms the input to a fixed number of features by appending zeros. Also normalizes the input by the number of used features to keep the variance of the input constant, even when zeros are appended.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.VariableNumFeaturesEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.normalize_data","title":"normalize_data","text":"<pre><code>normalize_data(\n    data: Tensor,\n    *,\n    normalize_positions: int = -1,\n    return_scaling: bool = False,\n    clip: bool = True,\n    std_only: bool = False,\n    mean: Tensor | None = None,\n    std: Tensor | None = None\n) -&gt; Tensor | tuple[Tensor, tuple[Tensor, Tensor]]\n</code></pre> <p>Normalize data to mean 0 and std 1.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The data to normalize. (T, B, H)</p> required <code>normalize_positions</code> <code>int</code> <p>If &gt; 0, only use the first <code>normalize_positions</code> positions for normalization.</p> <code>-1</code> <code>return_scaling</code> <code>bool</code> <p>If True, return the scaling parameters as well (mean, std).</p> <code>False</code> <code>std_only</code> <code>bool</code> <p>If True, only divide by std.</p> <code>False</code> <code>clip</code> <code>bool</code> <p>If True, clip the data to [-100, 100].</p> <code>True</code> <code>mean</code> <code>Tensor | None</code> <p>If given, use this value instead of computing it.</p> <code>None</code> <code>std</code> <code>Tensor | None</code> <p>If given, use this value instead of computing it.</p> <code>None</code>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.select_features","title":"select_features","text":"<pre><code>select_features(x: Tensor, sel: Tensor) -&gt; Tensor\n</code></pre> <p>Select features from the input tensor based on the selection mask.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>sel</code> <code>Tensor</code> <p>The boolean selection mask indicating which features to keep.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor with selected features.</p>"},{"location":"reference/tabpfn/model/layer/","title":"Layer","text":""},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer","title":"layer","text":""},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer.LayerNorm","title":"LayerNorm","text":"<p>             Bases: <code>LayerNorm</code></p> <p>Custom LayerNorm module that supports saving peak memory factor.</p> <p>This module extends the PyTorch LayerNorm implementation to handle FP16 inputs efficiently and support saving peak memory factor.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments passed to the base LayerNorm class.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to the base LayerNorm class.</p> <code>{}</code>"},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer.LayerNorm.forward","title":"forward","text":"<pre><code>forward(\n    input: Tensor,\n    *,\n    allow_inplace: bool = False,\n    save_peak_mem_factor: int | None = None\n) -&gt; Tensor\n</code></pre> <p>Perform layer normalization on the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>allow_inplace</code> <code>bool</code> <p>Whether to allow in-place operations. Default is False.</p> <code>False</code> <code>save_peak_mem_factor</code> <code>int | None</code> <p>The factor to save peak memory. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The layer normalized tensor.</p>"},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer.PerFeatureEncoderLayer","title":"PerFeatureEncoderLayer","text":"<p>             Bases: <code>Module</code></p> <p>Transformer encoder layer that processes each feature block separately.</p> <p>This layer consists of multi-head attention between features, multi-head attention between items, and feedforward neural networks (MLPs).</p> <p>It supports various configurations and optimization options.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The dimensionality of the input and output embeddings.</p> required <code>nhead</code> <code>int</code> <p>The number of attention heads.</p> required <code>dim_feedforward</code> <code>int | None</code> <p>The dimensionality of the feedforward network. Default is None (2 * d_model).</p> <code>None</code> <code>activation</code> <code>str</code> <p>The activation function to use in the MLPs.</p> <code>'relu'</code> <code>layer_norm_eps</code> <code>float</code> <p>The epsilon value for layer normalization.</p> <code>1e-05</code> <code>pre_norm</code> <code>bool</code> <p>Whether to apply layer normalization before or after the attention and MLPs.</p> <code>False</code> <code>device</code> <code>device | None</code> <p>The device to use for the layer parameters.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>The data type to use for the layer parameters.</p> <code>None</code> <code>recompute_attn</code> <code>bool</code> <p>Whether to recompute attention during backpropagation.</p> <code>False</code> <code>second_mlp</code> <code>bool</code> <p>Whether to include a second MLP in the layer.</p> <code>False</code> <code>layer_norm_with_elementwise_affine</code> <code>bool</code> <p>Whether to use elementwise affine parameters in layer normalization.</p> <code>False</code> <code>zero_init</code> <code>bool</code> <p>Whether to initialize the output of the MLPs to zero.</p> <code>False</code> <code>save_peak_mem_factor</code> <code>int | None</code> <p>The factor to save peak memory, only effective with post-norm.</p> <code>None</code> <code>attention_between_features</code> <code>bool</code> <p>Whether to apply attention between feature blocks.</p> <code>True</code> <code>multiquery_item_attention</code> <code>bool</code> <p>Whether to use multiquery attention for items.</p> <code>False</code> <code>multiquery_item_attention_for_test_set</code> <code>bool</code> <p>Whether to use multiquery attention for the test set.</p> <code>False</code> <code>attention_init_gain</code> <code>float</code> <p>The gain value for initializing attention parameters.</p> <code>1.0</code> <code>d_k</code> <code>int | None</code> <p>The dimensionality of the query and key vectors. Default is (d_model // nhead).</p> <code>None</code> <code>d_v</code> <code>int | None</code> <p>The dimensionality of the value vectors. Default is (d_model // nhead).</p> <code>None</code> <code>precomputed_kv</code> <code>None | Tensor | tuple[Tensor, Tensor]</code> <p>Precomputed key-value pairs for attention.</p> <code>None</code>"},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer.PerFeatureEncoderLayer.empty_trainset_representation_cache","title":"empty_trainset_representation_cache","text":"<pre><code>empty_trainset_representation_cache() -&gt; None\n</code></pre> <p>Empty the trainset representation cache.</p>"},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer.PerFeatureEncoderLayer.forward","title":"forward","text":"<pre><code>forward(\n    state: Tensor,\n    single_eval_pos: int | None = None,\n    *,\n    cache_trainset_representation: bool = False,\n    att_src: Tensor | None = None\n) -&gt; Tensor\n</code></pre> <p>Pass the input through the encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Tensor</code> <p>The transformer state passed as input to the layer of shape (batch_size, num_items, num_feature_blocks, d_model).</p> required <code>single_eval_pos</code> <code>int | None</code> <p>The position from which on everything is treated as test set.</p> <code>None</code> <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the trainset representation. If single_eval_pos is set (&gt; 0 and not None), create a cache of the trainset KV. This may require a lot of memory. Otherwise, use cached KV representations for inference.</p> <code>False</code> <code>att_src</code> <code>Tensor | None</code> <p>The tensor to attend to from the final layer of the encoder. It has a shape of (batch_size, num_train_items, num_feature_blocks, d_model). This does not work with multiquery_item_attention_for_test_set and cache_trainset_representation at this point.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The transformer state passed through the encoder layer.</p>"},{"location":"reference/tabpfn/model/loading/","title":"Loading","text":""},{"location":"reference/tabpfn/model/loading/#tabpfn.model.loading","title":"loading","text":""},{"location":"reference/tabpfn/model/loading/#tabpfn.model.loading.download_model","title":"download_model","text":"<pre><code>download_model(\n    to: Path,\n    *,\n    version: Literal[\"v2\"],\n    which: Literal[\"classifier\", \"regressor\"],\n    model_name: str | None = None\n) -&gt; Literal[\"ok\"] | list[Exception]\n</code></pre> <p>Download a TabPFN model, trying all available sources.</p> <p>Parameters:</p> Name Type Description Default <code>to</code> <code>Path</code> <p>The directory to download the model to.</p> required <code>version</code> <code>Literal['v2']</code> <p>The version of the model to download.</p> required <code>which</code> <code>Literal['classifier', 'regressor']</code> <p>The type of model to download.</p> required <code>model_name</code> <code>str | None</code> <p>Optional specific model name to download.</p> <code>None</code> <p>Returns:</p> Type Description <code>Literal['ok'] | list[Exception]</code> <p>\"ok\" if the model was downloaded successfully, otherwise a list of</p> <code>Literal['ok'] | list[Exception]</code> <p>exceptions that occurred that can be handled as desired.</p>"},{"location":"reference/tabpfn/model/loading/#tabpfn.model.loading.load_model","title":"load_model","text":"<pre><code>load_model(*, path: Path, model_seed: int) -&gt; tuple[\n    PerFeatureTransformer,\n    BCEWithLogitsLoss\n    | CrossEntropyLoss\n    | FullSupportBarDistribution,\n    InferenceConfig,\n]\n</code></pre> <p>Loads a model from a given path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the checkpoint</p> required <code>model_seed</code> <code>int</code> <p>The seed to use for the model</p> required"},{"location":"reference/tabpfn/model/memory/","title":"Memory","text":""},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory","title":"memory","text":""},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator","title":"MemoryUsageEstimator","text":""},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.convert_bytes_to_unit","title":"convert_bytes_to_unit  <code>classmethod</code>","text":"<pre><code>convert_bytes_to_unit(\n    value: float, unit: Literal[\"b\", \"mb\", \"gb\"]\n) -&gt; float\n</code></pre> <p>Convenience method to convert bytes to a different unit.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The number of bytes.</p> required <code>unit</code> <code>Literal['b', 'mb', 'gb']</code> <p>The unit to convert to.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The number of bytes in the new unit.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.convert_units","title":"convert_units  <code>classmethod</code>","text":"<pre><code>convert_units(\n    value: float,\n    from_unit: Literal[\"b\", \"mb\", \"gb\"],\n    to_unit: Literal[\"b\", \"mb\", \"gb\"],\n) -&gt; float\n</code></pre> <p>Convert a value from one unit to another.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.estimate_memory_of_one_batch","title":"estimate_memory_of_one_batch  <code>classmethod</code>","text":"<pre><code>estimate_memory_of_one_batch(\n    X: Tensor,\n    model: Module,\n    *,\n    cache_kv: bool,\n    dtype_byte_size: int,\n    unit: Literal[\"b\", \"mb\", \"gb\"] = \"gb\",\n    n_train_samples: int | None = None\n) -&gt; float\n</code></pre> <p>Estimate the memory usage of a single batch.</p> <p>The calculation is done based on the assumption that save_peak_mem_factor is not used (since this estimation is used to determine whether to use it).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>The input tensor.</p> required <code>model</code> <code>Module</code> <p>The model to estimate the memory usage of.</p> required <code>cache_kv</code> <code>bool</code> <p>Whether key and value tensors are cached.</p> required <code>dtype_byte_size</code> <code>int</code> <p>The size of the data type in bytes.</p> required <code>unit</code> <code>Literal['b', 'mb', 'gb']</code> <p>The unit to convert the memory usage to.</p> <code>'gb'</code> <code>n_train_samples</code> <code>int | None</code> <p>The number of training samples (only for cache_kv mode)</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The estimated memory usage of a single batch.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.estimate_memory_remainder_after_batch","title":"estimate_memory_remainder_after_batch  <code>classmethod</code>","text":"<pre><code>estimate_memory_remainder_after_batch(\n    X: Tensor,\n    model: Module,\n    *,\n    cache_kv: bool,\n    device: device,\n    dtype_byte_size: int,\n    safety_factor: float,\n    n_train_samples: int | None = None,\n    max_free_mem: float | int | None = None\n) -&gt; float\n</code></pre> <p>Whether to save peak memory or not.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>The input tensor.</p> required <code>model</code> <code>Module</code> <p>The model to estimate the memory usage of.</p> required <code>cache_kv</code> <code>bool</code> <p>Whether key and value tensors are cached.</p> required <code>device</code> <code>device</code> <p>The device to use.</p> required <code>dtype_byte_size</code> <code>int</code> <p>The size of the data type in bytes.</p> required <code>safety_factor</code> <code>float</code> <p>The safety factor to apply.</p> required <code>n_train_samples</code> <code>int | None</code> <p>The number of training samples (only for cache_kv mode)</p> <code>None</code> <code>max_free_mem</code> <code>float | int | None</code> <p>The amount of free memory available.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The amount of free memory available after a batch is computed.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.get_max_free_memory","title":"get_max_free_memory  <code>classmethod</code>","text":"<pre><code>get_max_free_memory(\n    device: device,\n    *,\n    unit: Literal[\"b\", \"mb\", \"gb\"] = \"gb\",\n    default_gb_cpu_if_failed_to_calculate: float\n) -&gt; float\n</code></pre> <p>How much memory to use at most in GB, the memory usage will be calculated based on an estimation of the systems free memory.</p> <p>For CUDA will use the free memory of the GPU. For CPU will default to 32 GB.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.get_max_free_memory--returns","title":"Returns:","text":"<p>The maximum memory usage in GB.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.reset_peak_memory_if_required","title":"reset_peak_memory_if_required  <code>classmethod</code>","text":"<pre><code>reset_peak_memory_if_required(\n    save_peak_mem: bool | Literal[\"auto\"] | float | int,\n    model: Module,\n    X: Tensor,\n    *,\n    cache_kv: bool,\n    device: device,\n    dtype_byte_size: int,\n    safety_factor: float = 5.0,\n    n_train_samples: int | None = None\n) -&gt; None\n</code></pre> <p>Reset the peak memory if required.</p> <p>Parameters:</p> Name Type Description Default <code>save_peak_mem</code> <code>bool | 'auto' | float | int</code> <p>If bool, specifies whether to save peak memory or not. If \"auto\", the amount of free memory is estimated and the option is enabled or disabled based on the estimated usage. If float or int, it is considered as the amount of memory available (in GB) explicitly specified by the user. In this case, this value is used to estimate whether or not to save peak memory.</p> required <code>model</code> <code>Module</code> <p>The model to reset the peak memory of.</p> required <code>X</code> <code>Tensor</code> <p>The input tensor.</p> required <code>cache_kv</code> <code>bool</code> <p>Whether key and value tensors are cached.</p> required <code>device</code> <code>device</code> <p>The device to use.</p> required <code>dtype_byte_size</code> <code>int</code> <p>The size of the data type in bytes.</p> required <code>safety_factor</code> <code>float</code> <p>The safety factor to apply.</p> <code>5.0</code> <code>n_train_samples</code> <code>int</code> <p>The number of training samples (to be used only for cache_kv mode)</p> <code>None</code>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.support_save_peak_mem_factor","title":"support_save_peak_mem_factor","text":"<pre><code>support_save_peak_mem_factor(\n    method: MethodType,\n) -&gt; Callable\n</code></pre> <p>Can be applied to a method acting on a tensor 'x' whose first dimension is a flat batch dimension (i.e. the operation is trivially parallel over the first dimension).</p> <p>For additional tensor arguments, it is assumed that the first dimension is again the batch dimension, and that non-tensor arguments can be passed as-is to splits when parallelizing over the batch dimension.</p> <p>The decorator adds options 'add_input' to add the principal input 'x' to the result of the method and 'allow_inplace'. By setting 'allow_inplace', the caller indicates that 'x' is not used after the call and its buffer can be reused for the output.</p> <p>Setting 'allow_inplace' does not ensure that the operation will be inplace, and the return value should be used for clarity and simplicity.</p> <p>Moreover, it adds an optional int parameter 'save_peak_mem_factor' that is only supported in combination with 'allow_inplace' during inference and subdivides the operation into the specified number of chunks to reduce peak memory consumption.</p>"},{"location":"reference/tabpfn/model/mlp/","title":"Mlp","text":""},{"location":"reference/tabpfn/model/mlp/#tabpfn.model.mlp","title":"mlp","text":""},{"location":"reference/tabpfn/model/mlp/#tabpfn.model.mlp.Activation","title":"Activation","text":"<p>             Bases: <code>Enum</code></p> <p>Enum for activation functions.</p>"},{"location":"reference/tabpfn/model/mlp/#tabpfn.model.mlp.MLP","title":"MLP","text":"<p>             Bases: <code>Module</code></p> <p>Multi-Layer Perceptron (MLP) module.</p> <p>This module consists of two linear layers with an activation function in between. It supports various configurations such as the hidden size, activation function, initializing the output to zero, and recomputing the forward pass during backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The input and output size of the MLP.</p> required <code>hidden_size</code> <code>int</code> <p>The size of the hidden layer.</p> required <code>activation</code> <code>Activation | str</code> <p>The activation function to use. Can be either an Activation enum or a string representing the activation name.</p> required <code>device</code> <code>device | None</code> <p>The device to use for the linear layers.</p> required <code>dtype</code> <code>dtype | None</code> <p>The data type to use for the linear layers.</p> required <code>initialize_output_to_zero</code> <code>bool</code> <p>Whether to initialize the output layer weights to zero. Default is False.</p> <code>False</code> <code>recompute</code> <code>bool</code> <p>Whether to recompute the forward pass during backpropagation. This can save memory but increase computation time. Default is False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>linear1</code> <code>Linear</code> <p>The first linear layer.</p> <code>linear2</code> <code>Linear</code> <p>The second linear layer.</p> <code>activation</code> <code>Activation</code> <p>The activation function to use.</p> Example <p>mlp = MLP(size=128, hidden_size=256, activation='gelu', device='cuda') x = torch.randn(32, 128, device='cuda', dtype=torch.float32) output = mlp(x)</p>"},{"location":"reference/tabpfn/model/mlp/#tabpfn.model.mlp.MLP.forward","title":"forward","text":"<pre><code>forward(\n    x: Tensor,\n    *,\n    add_input: bool = False,\n    allow_inplace: bool = False,\n    save_peak_mem_factor: int | None = None\n) -&gt; Tensor\n</code></pre> <p>Performs the forward pass of the MLP.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>add_input</code> <code>bool</code> <p>Whether to add input to the output. Default is False.</p> <code>False</code> <code>allow_inplace</code> <code>bool</code> <p>Indicates that 'x' is not used after the call and its buffer can be reused for the output. The operation is not guaranteed to be inplace. Default is False.</p> <code>False</code> <code>save_peak_mem_factor</code> <code>int | None</code> <p>If provided, enables a memory-saving technique that reduces peak memory usage during the forward pass. This requires 'add_input' and 'allow_inplace' to be True. See the documentation of the decorator 'support_save_peak_mem_factor' for details. Default is None.</p> <code>None</code>"},{"location":"reference/tabpfn/model/multi_head_attention/","title":"Multi head attention","text":""},{"location":"reference/tabpfn/model/multi_head_attention/#tabpfn.model.multi_head_attention","title":"multi_head_attention","text":""},{"location":"reference/tabpfn/model/multi_head_attention/#tabpfn.model.multi_head_attention.MultiHeadAttention","title":"MultiHeadAttention","text":"<p>             Bases: <code>Module</code></p>"},{"location":"reference/tabpfn/model/multi_head_attention/#tabpfn.model.multi_head_attention.MultiHeadAttention.forward","title":"forward","text":"<pre><code>forward(\n    x: Tensor,\n    x_kv: Tensor | None = None,\n    *,\n    cache_kv: bool = False,\n    add_input: bool = False,\n    allow_inplace: bool = False,\n    save_peak_mem_factor: int | None = None,\n    reuse_first_head_kv: bool = False,\n    only_cache_first_head_kv: bool = False,\n    use_cached_kv: bool = False,\n    use_second_set_of_queries: bool = False\n)\n</code></pre> <p>X is the current hidden and has a shape of [batch, ..., seq_len, input_size]. If keys and values are present in the cache and 'freeze_kv' is not set, they are obtained from there and 'x_kv' has to be None. Else, if 'x_kv' is not None, keys and values are obtained by applying the respective linear transformations to 'x_kv'. Else, keys and values are attained by applying the respective linear transformations to 'x' (self attention).</p>"},{"location":"reference/tabpfn/model/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing","title":"preprocessing","text":""},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.AddFingerprintFeaturesStep","title":"AddFingerprintFeaturesStep","text":"<p>             Bases: <code>FeaturePreprocessingTransformerStep</code></p> <p>Adds a fingerprint feature to the features based on hash of each row.</p> <p>If <code>is_test = True</code>, it keeps the first hash even if there are collisions. If <code>is_test = False</code>, it handles hash collisions by counting up and rehashing until a unique hash is found.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.AddFingerprintFeaturesStep.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fits the preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.AddFingerprintFeaturesStep.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transforms the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.FeaturePreprocessingTransformerStep","title":"FeaturePreprocessingTransformerStep","text":"<p>Base class for feature preprocessing steps.</p> <p>It's main abstraction is really just to provide categorical indices along the pipeline.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.FeaturePreprocessingTransformerStep.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fits the preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.FeaturePreprocessingTransformerStep.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transforms the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.KDITransformerWithNaN","title":"KDITransformerWithNaN","text":"<p>             Bases: <code>KDITransformer</code></p> <p>KDI transformer that can handle NaN values. It performs KDI with NaNs replaced by mean values and then fills the NaN values with NaNs after the transformation.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.RemoveConstantFeaturesStep","title":"RemoveConstantFeaturesStep","text":"<p>             Bases: <code>FeaturePreprocessingTransformerStep</code></p> <p>Remove features that are constant in the training data.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.RemoveConstantFeaturesStep.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fits the preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.RemoveConstantFeaturesStep.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transforms the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ReshapeFeatureDistributionsStep","title":"ReshapeFeatureDistributionsStep","text":"<p>             Bases: <code>FeaturePreprocessingTransformerStep</code></p> <p>Reshape the feature distributions using different transformations.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ReshapeFeatureDistributionsStep.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fits the preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ReshapeFeatureDistributionsStep.get_adaptive_preprocessors","title":"get_adaptive_preprocessors  <code>staticmethod</code>","text":"<pre><code>get_adaptive_preprocessors(\n    num_examples: int = 100, random_state: int | None = None\n) -&gt; dict[str, ColumnTransformer]\n</code></pre> <p>Returns a dictionary of adaptive column transformers that can be used to preprocess the data. Adaptive column transformers are used to preprocess the data based on the column type, they receive a pandas dataframe with column names, that indicate the column type. Column types are not datatypes, but rather a string that indicates how the data should be preprocessed.</p> <p>Parameters:</p> Name Type Description Default <code>num_examples</code> <code>int</code> <p>The number of examples in the dataset.</p> <code>100</code> <code>random_state</code> <code>int | None</code> <p>The random state to use for the transformers.</p> <code>None</code>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ReshapeFeatureDistributionsStep.get_column_types","title":"get_column_types  <code>staticmethod</code>","text":"<pre><code>get_column_types(X: ndarray) -&gt; list[str]\n</code></pre> <p>Returns a list of column types for the given data, that indicate how the data should be preprocessed.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ReshapeFeatureDistributionsStep.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transforms the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.SafePowerTransformer","title":"SafePowerTransformer","text":"<p>             Bases: <code>PowerTransformer</code></p> <p>Power Transformer which reverts features back to their original values if they are transformed to large values or the output column does not have unit variance. This happens e.g. when the input data has a large number of outliers.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.SequentialFeatureTransformer","title":"SequentialFeatureTransformer","text":"<p>             Bases: <code>UserList</code></p> <p>A transformer that applies a sequence of feature preprocessing steps. This is very related to sklearn's Pipeline, but it is designed to work with categorical_features lists that are always passed on.</p> <p>Currently this class is only used once, thus this could also be made less general if needed.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.SequentialFeatureTransformer.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fit all the steps in the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.SequentialFeatureTransformer.fit_transform","title":"fit_transform","text":"<pre><code>fit_transform(\n    X: ndarray, categorical_features: list[int]\n) -&gt; _TransformResult\n</code></pre> <p>Fit and transform the data using the fitted pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical features.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.SequentialFeatureTransformer.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transform the data using the fitted pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ShuffleFeaturesStep","title":"ShuffleFeaturesStep","text":"<p>             Bases: <code>FeaturePreprocessingTransformerStep</code></p> <p>Shuffle the features in the data.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ShuffleFeaturesStep.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fits the preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ShuffleFeaturesStep.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transforms the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.add_safe_standard_to_safe_power_without_standard","title":"add_safe_standard_to_safe_power_without_standard","text":"<pre><code>add_safe_standard_to_safe_power_without_standard(\n    input_transformer: TransformerMixin,\n) -&gt; Pipeline\n</code></pre> <p>In edge cases PowerTransformer can create inf values and similar. Then, the post standard scale crashes. This fixes this issue.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.make_box_cox_safe","title":"make_box_cox_safe","text":"<pre><code>make_box_cox_safe(\n    input_transformer: TransformerMixin | Pipeline,\n) -&gt; Pipeline\n</code></pre> <p>Make box cox save.</p> <p>The Box-Cox transformation can only be applied to strictly positive data. With first MinMax scaling, we achieve this without loss of function. Additionally, for test data, we also need clipping.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.skew","title":"skew","text":"<pre><code>skew(x: ndarray) -&gt; float\n</code></pre>"},{"location":"reference/tabpfn/model/transformer/","title":"Transformer","text":""},{"location":"reference/tabpfn/model/transformer/#tabpfn.model.transformer","title":"transformer","text":""},{"location":"reference/tabpfn/model/transformer/#tabpfn.model.transformer.LayerStack","title":"LayerStack","text":"<p>             Bases: <code>Module</code></p> <p>Similar to nn.Sequential, but with support for passing keyword arguments to layers and stacks the same layer multiple times.</p>"},{"location":"reference/tabpfn/model/transformer/#tabpfn.model.transformer.PerFeatureTransformer","title":"PerFeatureTransformer","text":"<p>             Bases: <code>Module</code></p> <p>A Transformer model processes a token per feature and sample.</p> <p>This model extends the standard Transformer architecture to operate on a per-feature basis. It allows for processing each feature separately while still leveraging the power of self-attention.</p> <p>The model consists of an encoder, decoder, and optional components such as a feature positional embedding and a separate decoder for each feature.</p>"},{"location":"reference/tabpfn/model/transformer/#tabpfn.model.transformer.PerFeatureTransformer.forward","title":"forward","text":"<pre><code>forward(*args: Any, **kwargs: Any) -&gt; dict[str, Tensor]\n</code></pre> <p>Performs a forward pass through the model.</p> <p>This method supports multiple calling conventions:</p> <ul> <li><code>model((x,y), **kwargs)</code></li> <li><code>model(train_x, train_y, test_x, **kwargs)</code></li> <li><code>model((style,x,y), **kwargs)</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>train_x</code> <p>torch.Tensor | None The input data for the training set.</p> required <code>train_y</code> <p>torch.Tensor | None The target data for the training set.</p> required <code>test_x</code> <p>torch.Tensor | None The input data for the test set.</p> required <code>x</code> <p>torch.Tensor The input data.</p> required <code>y</code> <p>torch.Tensor | None The target data.</p> required <code>style</code> <p>torch.Tensor | None The style vector.</p> required <code>single_eval_pos</code> <p>int The position to evaluate at.</p> required <code>only_return_standard_out</code> <p>bool Whether to only return the standard output.</p> required <code>data_dags</code> <p>Any The data DAGs for each example.</p> required <code>categorical_inds</code> <p>list[int] The indices of categorical features.</p> required <code>freeze_kv</code> <p>bool Whether to freeze the key and value weights.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>The output of the model, which can be a tensor or a dictionary of tensors.</p>"},{"location":"reference/tabpfn/model/transformer/#tabpfn.model.transformer.PerFeatureTransformer.reset_save_peak_mem_factor","title":"reset_save_peak_mem_factor","text":"<pre><code>reset_save_peak_mem_factor(\n    factor: int | None = None,\n) -&gt; None\n</code></pre> <p>Sets the save_peak_mem_factor for all layers.</p> <p>This factor controls how much memory is saved during the forward pass in inference mode.</p> <p>Setting this factor &gt; 1 will cause the model to save more memory during the forward pass in inference mode.</p> <p>A value of 8 is good for a 4x larger width in the fully-connected layers. and yields a situation were we need around <code>2*num_features*num_items*emsize*2</code> bytes of memory</p> <p>for a forward pass (using mixed precision).</p> <p>WARNING: It should only be used with post-norm.</p> <p>Parameters:</p> Name Type Description Default <code>factor</code> <code>int | None</code> <p>The save_peak_mem_factor to set. Recommended value is 8.</p> <code>None</code>"},{"location":"reference/tabpfn/model/transformer/#tabpfn.model.transformer.SerializableGenerator","title":"SerializableGenerator","text":"<p>             Bases: <code>Generator</code></p> <p>A serializable version of the torch.Generator, that cna be saved and pickled.</p>"},{"location":"reference/tabpfn_client/browser_auth/","title":"Browser auth","text":""},{"location":"reference/tabpfn_client/browser_auth/#tabpfn_client.browser_auth","title":"browser_auth","text":""},{"location":"reference/tabpfn_client/browser_auth/#tabpfn_client.browser_auth.BrowserAuthHandler","title":"BrowserAuthHandler","text":""},{"location":"reference/tabpfn_client/browser_auth/#tabpfn_client.browser_auth.BrowserAuthHandler.try_browser_login","title":"try_browser_login","text":"<pre><code>try_browser_login() -&gt; Tuple[bool, Optional[str]]\n</code></pre> <p>Attempts to perform browser-based login Returns (success: bool, token: Optional[str])</p>"},{"location":"reference/tabpfn_client/client/","title":"Client","text":""},{"location":"reference/tabpfn_client/client/#tabpfn_client.client","title":"client","text":""},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager","title":"DatasetUIDCacheManager","text":"<p>Manages a cache of the last 50 uploaded datasets, tracking dataset hashes and their UIDs.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager.add_dataset_uid","title":"add_dataset_uid","text":"<pre><code>add_dataset_uid(hash: str, dataset_uid: str)\n</code></pre> <p>Adds a new dataset to the cache, removing the oldest item if the cache exceeds 50 entries. Assumes the dataset is not already in the cache.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager.delete_uid","title":"delete_uid","text":"<pre><code>delete_uid(dataset_uid: str) -&gt; Optional[str]\n</code></pre> <p>Deletes an entry from the cache based on the dataset UID.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager.get_dataset_uid","title":"get_dataset_uid","text":"<pre><code>get_dataset_uid(*args)\n</code></pre> <p>Generates hash by all received arguments and returns cached dataset uid if in cache, otherwise None.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager.load_cache","title":"load_cache","text":"<pre><code>load_cache()\n</code></pre> <p>Loads the cache from disk if it exists, otherwise initializes an empty cache.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager.save_cache","title":"save_cache","text":"<pre><code>save_cache()\n</code></pre> <p>Saves the current cache to disk.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.GCPOverloaded","title":"GCPOverloaded","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when the Google Cloud Platform service is overloaded or unavailable.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient","title":"ServiceClient","text":"<p>             Bases: <code>Singleton</code></p> <p>Singleton class for handling communication with the server. It encapsulates all the API calls to the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.delete_all_datasets","title":"delete_all_datasets  <code>classmethod</code>","text":"<pre><code>delete_all_datasets() -&gt; [str]\n</code></pre> <p>Delete all datasets uploaded by the user from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.delete_all_datasets--returns","title":"Returns","text":"<p>deleted_dataset_uids : [str]     The list of deleted dataset UIDs.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.delete_dataset","title":"delete_dataset  <code>classmethod</code>","text":"<pre><code>delete_dataset(dataset_uid: str) -&gt; list[str]\n</code></pre> <p>Delete the dataset with the provided UID from the server. Note that deleting a train set with lead to deleting all associated test sets.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.delete_dataset--parameters","title":"Parameters","text":"<p>dataset_uid : str     The UID of the dataset to be deleted.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.delete_dataset--returns","title":"Returns","text":"<p>deleted_dataset_uids : [str]     The list of deleted dataset UIDs.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.download_all_data","title":"download_all_data  <code>classmethod</code>","text":"<pre><code>download_all_data(save_dir: Path) -&gt; Union[Path, None]\n</code></pre> <p>Download all data uploaded by the user from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.download_all_data--returns","title":"Returns","text":"<p>save_path : Path | None     The path to the downloaded file. Return None if download fails.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.fit","title":"fit  <code>classmethod</code>","text":"<pre><code>fit(X, y, config=None) -&gt; str\n</code></pre> <p>Upload a train set to server and return the train set UID if successful.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.fit--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     The training input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs)     The target values. config : dict, optional     Configuration for the fit method. Includes tabpfn_systems and paper_version.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.fit--returns","title":"Returns","text":"<p>train_set_uid : str     The unique ID of the train set in the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.get_data_summary","title":"get_data_summary  <code>classmethod</code>","text":"<pre><code>get_data_summary() -&gt; dict\n</code></pre> <p>Get the data summary of the user from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.get_data_summary--returns","title":"Returns","text":"<p>data_summary : dict     The data summary returned from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.get_password_policy","title":"get_password_policy  <code>classmethod</code>","text":"<pre><code>get_password_policy() -&gt; dict\n</code></pre> <p>Get the password policy from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.get_password_policy--returns","title":"Returns","text":"<p>password_policy : {}     The password policy returned from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.is_auth_token_outdated","title":"is_auth_token_outdated  <code>classmethod</code>","text":"<pre><code>is_auth_token_outdated(access_token) -&gt; Union[bool, None]\n</code></pre> <p>Check if the provided access token is valid and return True if successful.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.login","title":"login  <code>classmethod</code>","text":"<pre><code>login(email: str, password: str) -&gt; tuple[str, str]\n</code></pre> <p>Login with the provided credentials and return the access token if successful.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.login--parameters","title":"Parameters","text":"<p>email : str password : str</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.login--returns","title":"Returns","text":"<p>access_token : str | None     The access token returned from the server. Return None if login fails. message : str     The message returned from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.predict","title":"predict  <code>classmethod</code>","text":"<pre><code>predict(\n    train_set_uid: str,\n    x_test,\n    task: Literal[\"classification\", \"regression\"],\n    predict_params: Union[dict, None] = None,\n    tabpfn_config: Union[dict, None] = None,\n    X_train=None,\n    y_train=None,\n) -&gt; dict[str, ndarray]\n</code></pre> <p>Predict the class labels for the provided data (test set).</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.predict--parameters","title":"Parameters","text":"<p>train_set_uid : str     The unique ID of the train set in the server. x_test : array-like of shape (n_samples, n_features)     The test input.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.predict--returns","title":"Returns","text":"<p>y_pred : array-like of shape (n_samples,)     The predicted class labels.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(\n    email: str,\n    password: str,\n    password_confirm: str,\n    validation_link: str,\n    additional_info: dict,\n)\n</code></pre> <p>Register a new user with the provided credentials.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.register--parameters","title":"Parameters","text":"<p>email : str password : str password_confirm : str validation_link: str additional_info : dict</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.register--returns","title":"Returns","text":"<p>is_created : bool     True if the user is created successfully. message : str     The message returned from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.retrieve_greeting_messages","title":"retrieve_greeting_messages  <code>classmethod</code>","text":"<pre><code>retrieve_greeting_messages() -&gt; list[str]\n</code></pre> <p>Retrieve greeting messages that are new for the user.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.send_reset_password_email","title":"send_reset_password_email  <code>classmethod</code>","text":"<pre><code>send_reset_password_email(email: str) -&gt; tuple[bool, str]\n</code></pre> <p>Let the server send an email for resetting the password.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.send_verification_email","title":"send_verification_email  <code>classmethod</code>","text":"<pre><code>send_verification_email(\n    access_token: str,\n) -&gt; tuple[bool, str]\n</code></pre> <p>Let the server send an email for verifying the email.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.try_browser_login","title":"try_browser_login  <code>classmethod</code>","text":"<pre><code>try_browser_login() -&gt; tuple[bool, str]\n</code></pre> <p>Attempts browser-based login flow Returns (success: bool, message: str)</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.try_connection","title":"try_connection  <code>classmethod</code>","text":"<pre><code>try_connection() -&gt; bool\n</code></pre> <p>Check if server is reachable and accepts the connection.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.validate_email","title":"validate_email  <code>classmethod</code>","text":"<pre><code>validate_email(email: str) -&gt; tuple[bool, str]\n</code></pre> <p>Send entered email to server that checks if it is valid and not already in use.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.validate_email--parameters","title":"Parameters","text":"<p>email : str</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.validate_email--returns","title":"Returns","text":"<p>is_valid : bool     True if the email is valid. message : str     The message returned from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.verify_email","title":"verify_email  <code>classmethod</code>","text":"<pre><code>verify_email(\n    token: str, access_token: str\n) -&gt; tuple[bool, str]\n</code></pre> <p>Verify the email with the provided token.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.verify_email--parameters","title":"Parameters","text":"<p>token : str access_token : str</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.verify_email--returns","title":"Returns","text":"<p>is_verified : bool     True if the email is verified successfully. message : str     The message returned from the server.</p>"},{"location":"reference/tabpfn_client/config/","title":"Config","text":""},{"location":"reference/tabpfn_client/config/#tabpfn_client.config","title":"config","text":""},{"location":"reference/tabpfn_client/config/#tabpfn_client.config.Config","title":"Config","text":""},{"location":"reference/tabpfn_client/constants/","title":"Constants","text":""},{"location":"reference/tabpfn_client/constants/#tabpfn_client.constants","title":"constants","text":""},{"location":"reference/tabpfn_client/estimator/","title":"Estimator","text":""},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator","title":"estimator","text":""},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNClassifier","title":"TabPFNClassifier","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code>, <code>TabPFNModelSelection</code></p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict class labels for samples in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The input samples.</p> required <p>Returns:</p> Type Description <p>The predicted class labels.</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict class probabilities for X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The input samples.</p> required <p>Returns:</p> Type Description <p>The class probabilities of the input samples.</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNModelSelection","title":"TabPFNModelSelection","text":"<p>Base class for TabPFN model selection and path handling.</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNRegressor","title":"TabPFNRegressor","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code>, <code>TabPFNModelSelection</code></p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(\n    X: ndarray,\n    output_type: Literal[\n        \"mean\",\n        \"median\",\n        \"mode\",\n        \"quantiles\",\n        \"full\",\n        \"main\",\n    ] = \"mean\",\n    quantiles: Optional[list[float]] = None,\n) -&gt; Union[ndarray, list[ndarray], dict[str, ndarray]]\n</code></pre> <p>Predict regression target for X.</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNRegressor.predict--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     The input samples. output_type : str, default=\"mean\"     The type of prediction to return:     - \"mean\": Return mean prediction     - \"median\": Return median prediction     - \"mode\": Return mode prediction     - \"quantiles\": Return predictions for specified quantiles     - \"full\": Return full prediction details     - \"main\": Return main prediction metrics quantiles : list[float] or None, default=None     Quantiles to compute when output_type=\"quantiles\".     Default is [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNRegressor.predict--returns","title":"Returns","text":"<p>array-like or dict     The predicted values.</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.validate_data_size","title":"validate_data_size","text":"<pre><code>validate_data_size(\n    X: ndarray, y: Union[ndarray, None] = None\n)\n</code></pre> <p>Check the integrity of the training data. - check if the number of rows between X and y is consistent     if y is not None (ValueError) - check if the number of rows is less than MAX_ROWS (ValueError) - check if the number of columns is less than MAX_COLS (ValueError)</p>"},{"location":"reference/tabpfn_client/prompt_agent/","title":"Prompt agent","text":""},{"location":"reference/tabpfn_client/prompt_agent/#tabpfn_client.prompt_agent","title":"prompt_agent","text":""},{"location":"reference/tabpfn_client/prompt_agent/#tabpfn_client.prompt_agent.PromptAgent","title":"PromptAgent","text":""},{"location":"reference/tabpfn_client/prompt_agent/#tabpfn_client.prompt_agent.PromptAgent.password_req_to_policy","title":"password_req_to_policy  <code>staticmethod</code>","text":"<pre><code>password_req_to_policy(password_req: list[str])\n</code></pre> <p>Small function that receives password requirements as a list of strings like \"Length(8)\" and returns a corresponding PasswordPolicy object.</p>"},{"location":"reference/tabpfn_client/service_wrapper/","title":"Service wrapper","text":""},{"location":"reference/tabpfn_client/service_wrapper/#tabpfn_client.service_wrapper","title":"service_wrapper","text":""},{"location":"reference/tabpfn_client/service_wrapper/#tabpfn_client.service_wrapper.InferenceClient","title":"InferenceClient","text":"<p>             Bases: <code>ServiceClientWrapper</code>, <code>Singleton</code></p> <p>Wrapper of ServiceClient to handle inference, including: - fitting - prediction</p>"},{"location":"reference/tabpfn_client/service_wrapper/#tabpfn_client.service_wrapper.UserAuthenticationClient","title":"UserAuthenticationClient","text":"<p>             Bases: <code>ServiceClientWrapper</code>, <code>Singleton</code></p> <p>Wrapper of ServiceClient to handle user authentication, including: - user registration and login - access token caching</p> <p>This is implemented as a singleton class with classmethods.</p>"},{"location":"reference/tabpfn_client/service_wrapper/#tabpfn_client.service_wrapper.UserAuthenticationClient.try_browser_login","title":"try_browser_login  <code>classmethod</code>","text":"<pre><code>try_browser_login() -&gt; tuple[bool, str]\n</code></pre> <p>Try to authenticate using browser-based login</p>"},{"location":"reference/tabpfn_client/service_wrapper/#tabpfn_client.service_wrapper.UserDataClient","title":"UserDataClient","text":"<p>             Bases: <code>ServiceClientWrapper</code>, <code>Singleton</code></p> <p>Wrapper of ServiceClient to handle user data, including: - query, or delete user account data - query, download, or delete uploaded data</p>"},{"location":"reference/tabpfn_client/tabpfn_common_utils/regression_pred_result/","title":"Regression pred result","text":""},{"location":"reference/tabpfn_client/tabpfn_common_utils/regression_pred_result/#tabpfn_client.tabpfn_common_utils.regression_pred_result","title":"regression_pred_result","text":""},{"location":"reference/tabpfn_client/tabpfn_common_utils/utils/","title":"Utils","text":""},{"location":"reference/tabpfn_client/tabpfn_common_utils/utils/#tabpfn_client.tabpfn_common_utils.utils","title":"utils","text":""},{"location":"reference/tabpfn_extensions/utils/","title":"Utils","text":""},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils","title":"utils","text":""},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.get_tabpfn_models","title":"get_tabpfn_models","text":"<pre><code>get_tabpfn_models() -&gt; Tuple[Type, Type, Type]\n</code></pre> <p>Get TabPFN models with fallback between local and client versions.</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.is_tabpfn","title":"is_tabpfn","text":"<pre><code>is_tabpfn(estimator: Any) -&gt; bool\n</code></pre> <p>Check if an estimator is a TabPFN model.</p>"},{"location":"reference/tabpfn_extensions/utils_todo/","title":"Utils todo","text":""},{"location":"reference/tabpfn_extensions/utils_todo/#tabpfn_extensions.utils_todo","title":"utils_todo","text":""},{"location":"reference/tabpfn_extensions/utils_todo/#tabpfn_extensions.utils_todo.infer_categorical_features","title":"infer_categorical_features","text":"<pre><code>infer_categorical_features(\n    X: ndarray, categorical_features\n) -&gt; List[int]\n</code></pre> <p>Infer the categorical features from the input data. We take <code>self.categorical_features</code> as the initial list of categorical features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>Tuple[int, ...]: The indices of the categorical features.</p>"},{"location":"reference/tabpfn_extensions/benchmarking/experiment/","title":"Experiment","text":""},{"location":"reference/tabpfn_extensions/benchmarking/experiment/#tabpfn_extensions.benchmarking.experiment","title":"experiment","text":""},{"location":"reference/tabpfn_extensions/benchmarking/experiment/#tabpfn_extensions.benchmarking.experiment.Experiment","title":"Experiment","text":"<p>Base class for experiments. Experiments should be reproducible, i.e. the settings should give all the information     needed to run the experiment. Experiments should be deterministic, i.e. the same settings should always give the same results.</p>"},{"location":"reference/tabpfn_extensions/benchmarking/experiment/#tabpfn_extensions.benchmarking.experiment.Experiment.run","title":"run","text":"<pre><code>run(tabpfn, **kwargs)\n</code></pre> <p>Runs the experiment.</p> <p>Should set self.results</p>"},{"location":"reference/tabpfn_extensions/classifier_as_regressor/classifier_as_regressor/","title":"Classifier as regressor","text":""},{"location":"reference/tabpfn_extensions/classifier_as_regressor/classifier_as_regressor/#tabpfn_extensions.classifier_as_regressor.classifier_as_regressor","title":"classifier_as_regressor","text":""},{"location":"reference/tabpfn_extensions/classifier_as_regressor/classifier_as_regressor/#tabpfn_extensions.classifier_as_regressor.classifier_as_regressor.ClassifierAsRegressor","title":"ClassifierAsRegressor","text":"<p>             Bases: <code>RegressorMixin</code></p> <p>Wrapper class to use a classifier as a regressor.</p> <p>This class takes a classifier estimator and converts it into a regressor by encoding the target labels and treating the regression problem as a classification task.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>object Classifier estimator to be used as a regressor.</p> required <p>Attributes:</p> Name Type Description <code>label_encoder_</code> <p>LabelEncoder Label encoder used to transform target regression labels to classes.</p> <code>y_train_</code> <p>array-like of shape (n_samples,) Transformed target labels used for training.</p> <code>categorical_features</code> <p>list List of categorical feature indices.</p> Example<pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from tabpfn_extensions import ManyClassClassifier, TabPFNClassifier, ClassifierAsRegressor\n&gt;&gt;&gt; x, y = load_diabetes(return_X_y=True)\n&gt;&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n&gt;&gt;&gt; clf = TabPFNClassifier()\n&gt;&gt;&gt; clf = ManyClassClassifier(clf, n_estimators=10, alphabet_size=clf.max_num_classes_)\n&gt;&gt;&gt; reg = ClassifierAsRegressor(clf)\n&gt;&gt;&gt; reg.fit(x_train, y_train)\n&gt;&gt;&gt; y_pred = reg.predict(x_test)\n</code></pre>"},{"location":"reference/tabpfn_extensions/classifier_as_regressor/classifier_as_regressor/#tabpfn_extensions.classifier_as_regressor.classifier_as_regressor.ClassifierAsRegressor.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre> <p>Fit the classifier as a regressor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Training data.</p> required <code>y</code> <p>array-like of shape (n_samples,) Target labels.</p> required <p>Returns:</p> Name Type Description <code>self</code> <p>object Fitted estimator.</p>"},{"location":"reference/tabpfn_extensions/classifier_as_regressor/classifier_as_regressor/#tabpfn_extensions.classifier_as_regressor.classifier_as_regressor.ClassifierAsRegressor.get_optimization_mode","title":"get_optimization_mode","text":"<pre><code>get_optimization_mode()\n</code></pre> <p>Get the optimization mode for the regressor.</p> <p>Returns:</p> Type Description <p>str Optimization mode (\"mean\").</p>"},{"location":"reference/tabpfn_extensions/classifier_as_regressor/classifier_as_regressor/#tabpfn_extensions.classifier_as_regressor.classifier_as_regressor.ClassifierAsRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict the target values for the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Input data.</p> required <p>Returns:</p> Name Type Description <code>y_pred</code> <p>array-like of shape (n_samples,) Predicted target values.</p>"},{"location":"reference/tabpfn_extensions/classifier_as_regressor/classifier_as_regressor/#tabpfn_extensions.classifier_as_regressor.classifier_as_regressor.ClassifierAsRegressor.predict_full","title":"predict_full","text":"<pre><code>predict_full(X)\n</code></pre> <p>Predict the full set of output values for the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Input data.</p> required <p>Returns:</p> Type Description <p>dict Dictionary containing the predicted output values, including: - \"mean\": Predicted mean values. - \"median\": Predicted median values. - \"mode\": Predicted mode values. - \"logits\": Predicted logits. - \"buckets\": Predicted bucket probabilities. - \"quantile_{q:.2f}\": Predicted quantile values for each quantile q.</p>"},{"location":"reference/tabpfn_extensions/classifier_as_regressor/classifier_as_regressor/#tabpfn_extensions.classifier_as_regressor.classifier_as_regressor.ClassifierAsRegressor.probabilities_to_logits_multiclass","title":"probabilities_to_logits_multiclass  <code>staticmethod</code>","text":"<pre><code>probabilities_to_logits_multiclass(\n    probabilities, eps=1e-06\n)\n</code></pre> <p>Convert probabilities to logits for a multi-class problem.</p> <p>Parameters:</p> Name Type Description Default <code>probabilities</code> <p>array-like of shape (n_samples, n_classes) Input probabilities for each class.</p> required <code>eps</code> <p>float, default=1e-6 Small value to avoid division by zero or taking logarithm of zero.</p> <code>1e-06</code> <p>Returns:</p> Name Type Description <code>logits</code> <p>array-like of shape (n_samples, n_classes) Output logits for each class.</p>"},{"location":"reference/tabpfn_extensions/classifier_as_regressor/classifier_as_regressor/#tabpfn_extensions.classifier_as_regressor.classifier_as_regressor.ClassifierAsRegressor.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features)\n</code></pre> <p>Set the categorical feature indices.</p> <p>Parameters:</p> Name Type Description Default <code>categorical_features</code> <p>list List of categorical feature indices.</p> required"},{"location":"reference/tabpfn_extensions/hpo/search_space/","title":"Search space","text":""},{"location":"reference/tabpfn_extensions/hpo/search_space/#tabpfn_extensions.hpo.search_space","title":"search_space","text":""},{"location":"reference/tabpfn_extensions/hpo/tuned_tabpfn/","title":"Tuned tabpfn","text":""},{"location":"reference/tabpfn_extensions/hpo/tuned_tabpfn/#tabpfn_extensions.hpo.tuned_tabpfn","title":"tuned_tabpfn","text":""},{"location":"reference/tabpfn_extensions/hpo/tuned_tabpfn/#tabpfn_extensions.hpo.tuned_tabpfn.TunedTabPFNBase","title":"TunedTabPFNBase","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>Base class for tuned TabPFN models with proper categorical handling.</p>"},{"location":"reference/tabpfn_extensions/hpo/tuned_tabpfn/#tabpfn_extensions.hpo.tuned_tabpfn.TunedTabPFNClassifier","title":"TunedTabPFNClassifier","text":"<p>             Bases: <code>TunedTabPFNBase</code>, <code>ClassifierMixin</code></p> <p>TabPFN Classifier with hyperparameter tuning and proper categorical handling.</p>"},{"location":"reference/tabpfn_extensions/hpo/tuned_tabpfn/#tabpfn_extensions.hpo.tuned_tabpfn.TunedTabPFNRegressor","title":"TunedTabPFNRegressor","text":"<p>             Bases: <code>TunedTabPFNBase</code>, <code>RegressorMixin</code></p> <p>TabPFN Regressor with hyperparameter tuning and proper categorical handling.</p>"},{"location":"reference/tabpfn_extensions/interpretability/experiments/","title":"Experiments","text":""},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments","title":"experiments","text":""},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments.FeatureSelectionExperiment","title":"FeatureSelectionExperiment","text":"<p>             Bases: <code>Experiment</code></p> <p>This class is used to run experiments on generating synthetic data.</p>"},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments.FeatureSelectionExperiment.run","title":"run","text":"<pre><code>run(tabpfn, **kwargs)\n</code></pre> <p>:param tabpfn: :param kwargs:     indices: list of indices from X features to use :return:</p>"},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments.FeatureSelectionInPredictExperiment","title":"FeatureSelectionInPredictExperiment","text":"<p>             Bases: <code>Experiment</code></p> <p>This class is used to run experiments on generating synthetic data.</p>"},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments.FeatureSelectionInPredictExperiment.run","title":"run","text":"<pre><code>run(tabpfn, **kwargs)\n</code></pre> <p>:param tabpfn: :param kwargs:     indices: list of indices from X features to use :return:</p>"},{"location":"reference/tabpfn_extensions/interpretability/feature_selection/","title":"Feature selection","text":""},{"location":"reference/tabpfn_extensions/interpretability/feature_selection/#tabpfn_extensions.interpretability.feature_selection","title":"feature_selection","text":""},{"location":"reference/tabpfn_extensions/interpretability/shap/","title":"Shap","text":""},{"location":"reference/tabpfn_extensions/interpretability/shap/#tabpfn_extensions.interpretability.shap","title":"shap","text":""},{"location":"reference/tabpfn_extensions/interpretability/shap/#tabpfn_extensions.interpretability.shap.get_shap_values","title":"get_shap_values","text":"<pre><code>get_shap_values(\n    estimator, test_x, attribute_names=None, **kwargs\n) -&gt; ndarray\n</code></pre> <p>Computes SHAP (SHapley Additive exPlanations) values for the model's predictions on the given input features.</p> <p>Parameters:</p> Name Type Description Default <code>test_x</code> <code>Union[DataFrame, ndarray]</code> <p>The input features to compute SHAP values for.</p> required <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the SHAP explainer.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The computed SHAP values.</p>"},{"location":"reference/tabpfn_extensions/interpretability/shap/#tabpfn_extensions.interpretability.shap.plot_shap","title":"plot_shap","text":"<pre><code>plot_shap(shap_values: ndarray)\n</code></pre> <p>Plots the shap values for the given test data. It will plot aggregated shap values for each feature, as well as per sample shap values. Additionally, if multiple samples are provided, it will plot the 3 most important interactions with the most important feature.</p> <p>Parameters:</p> Name Type Description Default <code>shap_values</code> <code>ndarray</code> required"},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/","title":"Many class classifier","text":""},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/#tabpfn_extensions.many_class.many_class_classifier","title":"many_class_classifier","text":""},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/#tabpfn_extensions.many_class.many_class_classifier.ManyClassClassifier","title":"ManyClassClassifier","text":"<p>             Bases: <code>OutputCodeClassifier</code></p> <p>Output-Code multiclass strategy with deciary codebook.</p> <p>This class extends the original OutputCodeClassifier to support n-ary codebooks (with n=alphabet_size), allowing for handling more classes.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>estimator object An estimator object implementing :term:<code>fit</code> and one of :term:<code>decision_function</code> or :term:<code>predict_proba</code>. The base classifier should be able to handle up to <code>alphabet_size</code> classes.</p> required <code>random_state</code> <p>int, RandomState instance, default=None The generator used to initialize the codebook. Pass an int for reproducible output across multiple function calls. See :term:<code>Glossary &lt;random_state&gt;</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimators_</code> <p>list of <code>int(n_classes * code_size)</code> estimators Estimators used for predictions.</p> <code>classes_</code> <p>ndarray of shape (n_classes,) Array containing labels.</p> <code>code_book_</code> <p>ndarray of shape (n_classes, <code>len(estimators_)</code>) Deciary array containing the code of each class.</p> Example<pre><code>&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from tabpfn.scripts.estimator import ManyClassClassifier, TabPFNClassifier\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; x, y = load_iris(return_X_y=True)\n&gt;&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n&gt;&gt;&gt; clf = TabPFNClassifier()\n&gt;&gt;&gt; clf = ManyClassClassifier(clf, alphabet_size=clf.max_num_classes_)\n&gt;&gt;&gt; clf.fit(x_train, y_train)\n&gt;&gt;&gt; clf.predict(x_test)\n</code></pre>"},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/#tabpfn_extensions.many_class.many_class_classifier.ManyClassClassifier.fit","title":"fit","text":"<pre><code>fit(X, y, **fit_params)\n</code></pre> <p>Fit underlying estimators.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>{array-like, sparse matrix} of shape (n_samples, n_features) Data.</p> required <code>y</code> <p>array-like of shape (n_samples,) Multi-class targets.</p> required <code>**fit_params</code> <p>dict Parameters passed to the <code>estimator.fit</code> method of each sub-estimator.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <p>object Returns a fitted instance of self.</p>"},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/#tabpfn_extensions.many_class.many_class_classifier.ManyClassClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict probabilities using the underlying estimators.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>{array-like, sparse matrix} of shape (n_samples, n_features) Data.</p> required <p>Returns:</p> Name Type Description <code>p</code> <p>ndarray of shape (n_samples, n_classes) Returns the probability of the samples for each class in the model, where classes are ordered as they are in <code>self.classes_</code>.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/","title":"Abstract validation utils","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/#tabpfn_extensions.post_hoc_ensembles.abstract_validation_utils","title":"abstract_validation_utils","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/#tabpfn_extensions.post_hoc_ensembles.abstract_validation_utils.AbstractValidationUtils","title":"AbstractValidationUtils","text":"<p>             Bases: <code>ABC</code>, <code>BaseEstimator</code></p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/#tabpfn_extensions.post_hoc_ensembles.abstract_validation_utils.AbstractValidationUtils.get_oof_per_estimator","title":"get_oof_per_estimator","text":"<pre><code>get_oof_per_estimator(\n    X: ndarray,\n    y: ndarray,\n    *,\n    return_loss_per_estimator: bool = False,\n    impute_dropped_instances: bool = True,\n    _extra_processing: bool = False\n) -&gt; list[ndarray] | tuple[list[ndarray], list[float]]\n</code></pre> <p>Get OOF predictions for each base model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>training data (features)</p> required <code>y</code> <code>ndarray</code> <p>training labels</p> required <code>return_loss_per_estimator</code> <code>bool</code> <p>if True, also return the loss per estimator.</p> <code>False</code> <code>impute_dropped_instances</code> <code>bool</code> <p>if True, impute instances that were dropped during the splits (e.g., due to not enough instances per class).</p> <code>True</code> <code>_extra_processing</code> <code>bool</code> <code>False</code> <p>either only OOF predictions or OOF predictions and loss per estimator.</p> Type Description <code>list[ndarray] | tuple[list[ndarray], list[float]]</code> <p>If self.is_holdout is True, the OOF predictions can return NaN values for instances not covered during repeated holdout.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/#tabpfn_extensions.post_hoc_ensembles.abstract_validation_utils.AbstractValidationUtils.not_enough_time","title":"not_enough_time","text":"<pre><code>not_enough_time(current_repeat: int) -&gt; bool\n</code></pre> <p>Simple heuristic to stop cross-validation early if not enough time is left for another repeat.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/","title":"Greedy weighted ensemble","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble","title":"greedy_weighted_ensemble","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsemble","title":"GreedyWeightedEnsemble","text":"<p>             Bases: <code>AbstractValidationUtils</code></p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsemble.get_oof_per_estimator","title":"get_oof_per_estimator","text":"<pre><code>get_oof_per_estimator(\n    X: ndarray,\n    y: ndarray,\n    *,\n    return_loss_per_estimator: bool = False,\n    impute_dropped_instances: bool = True,\n    _extra_processing: bool = False\n) -&gt; list[ndarray] | tuple[list[ndarray], list[float]]\n</code></pre> <p>Get OOF predictions for each base model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>training data (features)</p> required <code>y</code> <code>ndarray</code> <p>training labels</p> required <code>return_loss_per_estimator</code> <code>bool</code> <p>if True, also return the loss per estimator.</p> <code>False</code> <code>impute_dropped_instances</code> <code>bool</code> <p>if True, impute instances that were dropped during the splits (e.g., due to not enough instances per class).</p> <code>True</code> <code>_extra_processing</code> <code>bool</code> <code>False</code> <p>either only OOF predictions or OOF predictions and loss per estimator.</p> Type Description <code>list[ndarray] | tuple[list[ndarray], list[float]]</code> <p>If self.is_holdout is True, the OOF predictions can return NaN values for instances not covered during repeated holdout.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsemble.not_enough_time","title":"not_enough_time","text":"<pre><code>not_enough_time(current_repeat: int) -&gt; bool\n</code></pre> <p>Simple heuristic to stop cross-validation early if not enough time is left for another repeat.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleClassifier","title":"GreedyWeightedEnsembleClassifier","text":"<p>             Bases: <code>GreedyWeightedEnsemble</code>, <code>AbstractValidationUtilsClassification</code></p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleClassifier.get_oof_per_estimator","title":"get_oof_per_estimator","text":"<pre><code>get_oof_per_estimator(\n    X: ndarray,\n    y: ndarray,\n    *,\n    return_loss_per_estimator: bool = False,\n    impute_dropped_instances: bool = True,\n    _extra_processing: bool = False\n) -&gt; list[ndarray] | tuple[list[ndarray], list[float]]\n</code></pre> <p>Get OOF predictions for each base model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>training data (features)</p> required <code>y</code> <code>ndarray</code> <p>training labels</p> required <code>return_loss_per_estimator</code> <code>bool</code> <p>if True, also return the loss per estimator.</p> <code>False</code> <code>impute_dropped_instances</code> <code>bool</code> <p>if True, impute instances that were dropped during the splits (e.g., due to not enough instances per class).</p> <code>True</code> <code>_extra_processing</code> <code>bool</code> <code>False</code> <p>either only OOF predictions or OOF predictions and loss per estimator.</p> Type Description <code>list[ndarray] | tuple[list[ndarray], list[float]]</code> <p>If self.is_holdout is True, the OOF predictions can return NaN values for instances not covered during repeated holdout.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleClassifier.not_enough_time","title":"not_enough_time","text":"<pre><code>not_enough_time(current_repeat: int) -&gt; bool\n</code></pre> <p>Simple heuristic to stop cross-validation early if not enough time is left for another repeat.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleRegressor","title":"GreedyWeightedEnsembleRegressor","text":"<p>             Bases: <code>GreedyWeightedEnsemble</code>, <code>AbstractValidationUtilsRegression</code></p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleRegressor.get_oof_per_estimator","title":"get_oof_per_estimator","text":"<pre><code>get_oof_per_estimator(\n    X: ndarray,\n    y: ndarray,\n    *,\n    return_loss_per_estimator: bool = False,\n    impute_dropped_instances: bool = True,\n    _extra_processing: bool = False\n) -&gt; list[ndarray] | tuple[list[ndarray], list[float]]\n</code></pre> <p>Get OOF predictions for each base model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>training data (features)</p> required <code>y</code> <code>ndarray</code> <p>training labels</p> required <code>return_loss_per_estimator</code> <code>bool</code> <p>if True, also return the loss per estimator.</p> <code>False</code> <code>impute_dropped_instances</code> <code>bool</code> <p>if True, impute instances that were dropped during the splits (e.g., due to not enough instances per class).</p> <code>True</code> <code>_extra_processing</code> <code>bool</code> <code>False</code> <p>either only OOF predictions or OOF predictions and loss per estimator.</p> Type Description <code>list[ndarray] | tuple[list[ndarray], list[float]]</code> <p>If self.is_holdout is True, the OOF predictions can return NaN values for instances not covered during repeated holdout.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleRegressor.not_enough_time","title":"not_enough_time","text":"<pre><code>not_enough_time(current_repeat: int) -&gt; bool\n</code></pre> <p>Simple heuristic to stop cross-validation early if not enough time is left for another repeat.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.caruana_weighted","title":"caruana_weighted","text":"<pre><code>caruana_weighted(\n    predictions: list[ndarray],\n    labels: ndarray,\n    seed,\n    n_iterations,\n    loss_function,\n)\n</code></pre> <p>Caruana's ensemble selection with replacement.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/","title":"Pfn phe","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/#tabpfn_extensions.post_hoc_ensembles.pfn_phe","title":"pfn_phe","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/#tabpfn_extensions.post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor","title":"AutoPostHocEnsemblePredictor","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>A wrapper to effectively performing post hoc ensemble with TabPFN models.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/#tabpfn_extensions.post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray,\n    y: ndarray,\n    categorical_feature_indices: list[int] | None = None,\n) -&gt; AutoPostHocEnsemblePredictor\n</code></pre> <p>Fits the post hoc ensemble on the given data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data to fit the ensemble on.</p> required <code>y</code> <code>ndarray</code> <p>The target values to fit the ensemble on.</p> required <code>categorical_feature_indices</code> <code>list[int] | None</code> <p>The indices of the categorical features in the data. If None, no categorical features are assumed to be present.</p> <code>None</code>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/#tabpfn_extensions.post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; ndarray\n</code></pre> <p>Predicts the target values for the given data.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/#tabpfn_extensions.post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X: ndarray) -&gt; ndarray\n</code></pre> <p>Predicts the target values for the given data.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/save_splitting/","title":"Save splitting","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/save_splitting/#tabpfn_extensions.post_hoc_ensembles.save_splitting","title":"save_splitting","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/save_splitting/#tabpfn_extensions.post_hoc_ensembles.save_splitting.assert_valid_splits","title":"assert_valid_splits","text":"<pre><code>assert_valid_splits(\n    splits: list[list[list[int], list[int]]],\n    y: ndarray,\n    *,\n    non_empty: bool = True,\n    each_selected_class_in_each_split_subset: bool = True,\n    same_length_training_splits: bool = True\n)\n</code></pre> <p>Verify that the splits are valid.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/save_splitting/#tabpfn_extensions.post_hoc_ensembles.save_splitting.fix_split_by_dropping_classes","title":"fix_split_by_dropping_classes","text":"<pre><code>fix_split_by_dropping_classes(\n    x: ndarray,\n    y: ndarray,\n    n_splits: int,\n    spliter_kwargs: dict,\n) -&gt; list[list[list[int], list[int]]]\n</code></pre> <p>Fixes stratifed splits for edge case.</p> <p>For each class that has fewer instances than number of splits, we oversample before split to n_splits and then remove all oversamples and original samples from the splits; effectively removing the class from the data without touching the indices.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/save_splitting/#tabpfn_extensions.post_hoc_ensembles.save_splitting.get_cv_split_for_data","title":"get_cv_split_for_data","text":"<pre><code>get_cv_split_for_data(\n    x: ndarray,\n    y: ndarray,\n    splits_seed: int,\n    n_splits: int,\n    *,\n    stratified_split: bool,\n    safety_shuffle: bool = True,\n    auto_fix_stratified_splits: bool = False,\n    force_same_length_training_splits: bool = False\n) -&gt; list[list[list[int], list[int]]] | str\n</code></pre> <p>Safety shuffle and generate (safe) splits.</p> <p>If it returns str at the first entry, no valid split could be generated and the str is the reason why. Due to the safety shuffle, the original x and y are also returned and must be used.</p> <p>Note: the function does not support repeated splits at this point. Simply call this function multiple times with different seeds to get repeated splits.</p> <p>Test with:</p> <pre><code>    if __name__ == \"__main__\":\n    print(\n        get_cv_split_for_data(\n            x=np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]).T,\n            y=np.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4]),\n            splits_seed=42,\n            n_splits=3,\n            stratified_split=True,\n            auto_fix_stratified_splits=True,\n        )\n    )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The data to split.</p> required <code>y</code> <code>ndarray</code> <p>The labels to split.</p> required <code>splits_seed</code> <code>int</code> <p>The seed to use for the splits. Or a RandomState object.</p> required <code>n_splits</code> <code>int</code> <p>The number of splits to generate.</p> required <code>stratified_split</code> <code>bool</code> <p>Whether to use stratified splits.</p> required <code>safety_shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting.</p> <code>True</code> <code>auto_fix_stratified_splits</code> <code>bool</code> <p>Whether to try to fix stratified splits automatically. Fix by dropping classes with less than n_splits samples.</p> <code>False</code> <code>force_same_length_training_splits</code> <code>bool</code> <p>Whether to force the training splits to have the same amount of samples. Force by duplicating random instance in the training subset of a too small split until all training splits have the same amount of samples.</p> <code>False</code>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/","title":"Sklearn interface","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/#tabpfn_extensions.post_hoc_ensembles.sklearn_interface","title":"sklearn_interface","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/#tabpfn_extensions.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier","title":"AutoTabPFNClassifier","text":"<p>             Bases: <code>ClassifierMixin</code>, <code>BaseEstimator</code></p> <p>Automatic Post Hoc Ensemble Classifier for TabPFN models.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/#tabpfn_extensions.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier--parameters","title":"Parameters","text":"<pre><code>max_time : int | None, default=None\n    The maximum time to spend on fitting the post hoc ensemble.\npreset: {\"default\", \"custom_hps\", \"avoid_overfitting\"}, default=\"default\"\n    The preset to use for the post hoc ensemble.\nges_scoring_string : str, default=\"roc\"\n    The scoring string to use for the greedy ensemble search.\n    Allowed values are: {\"accuracy\", \"roc\" / \"auroc\", \"f1\", \"log_loss\"}.\ndevice : {\"cpu\", \"cuda\"}, default=\"cuda\"\n    The device to use for training and prediction.\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness base models and the post hoc ensembling method.\ncategorical_feature_indices: list[int] or None, default=None\n    The indices of the categorical features in the input data. Can also be passed to `fit()`.\nphe_init_args : dict | None, default=None\n    The initialization arguments for the post hoc ensemble predictor.\n    See post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more options and all details.\n</code></pre> <pre><code>predictor_ : AutoPostHocEnsemblePredictor\n    The predictor interface used to make predictions, see post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more.\nphe_init_args_ : dict\n    The optional initialization arguments used for the post hoc ensemble predictor.\n</code></pre>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/#tabpfn_extensions.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor","title":"AutoTabPFNRegressor","text":"<p>             Bases: <code>RegressorMixin</code>, <code>BaseEstimator</code></p> <p>Automatic Post Hoc Ensemble Regressor for TabPFN models.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/#tabpfn_extensions.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor--parameters","title":"Parameters","text":"<pre><code>max_time : int | None, default=None\n    The maximum time to spend on fitting the post hoc ensemble.\npreset: {\"default\", \"custom_hps\", \"avoid_overfitting\"}, default=\"default\"\n    The preset to use for the post hoc ensemble.\nges_scoring_string : str, default=\"mse\"\n    The scoring string to use for the greedy ensemble search.\n    Allowed values are: {\"rmse\", \"mse\", \"mae\"}.\ndevice : {\"cpu\", \"cuda\"}, default=\"cuda\"\n    The device to use for training and prediction.\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness base models and the post hoc ensembling method.\ncategorical_feature_indices: list[int] or None, default=None\n    The indices of the categorical features in the input data. Can also be passed to `fit()`.\nphe_init_args : dict | None, default=None\n    The initialization arguments for the post hoc ensemble predictor.\n    See post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more options and all details.\n</code></pre> <pre><code>predictor_ : AutoPostHocEnsemblePredictor\n    The predictor interface used to make predictions, see post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more.\nphe_init_args_ : dict\n    The optional initialization arguments used for the post hoc ensemble predictor.\n</code></pre>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/","title":"SklearnBasedDecisionTreeTabPFN","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN","title":"SklearnBasedDecisionTreeTabPFN","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNBase","title":"DecisionTreeTabPFNBase","text":"<p>             Bases: <code>BaseDecisionTree</code></p> <p>Class that implements a DT-TabPFN model based on sklearn package</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNBase.apply_tree","title":"apply_tree","text":"<pre><code>apply_tree(X)\n</code></pre> <p>Apply tree for different kinds of tree types. TODO: This function could also be overwritten in each type of tree</p> <p>(N_samples, N_nodes, N_estimators) :param bootstrap_X: :return:</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNBase.predict_","title":"predict_","text":"<pre><code>predict_(X, y=None, check_input=True)\n</code></pre> <p>Predicts X :param X: Data that should be evaluated :param y: True labels of holdout data used for adaptive tree.     - If not None: Prunes nodes based on the performance of the holdout data y     - If None: Predicts the data based on the previous hold out performances :param check_input: :return: Probabilities of each class</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNBase.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features)\n</code></pre> <p>Sets categorical features :param categorical_features: Categorical features :return: None</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNClassifier","title":"DecisionTreeTabPFNClassifier","text":"<p>             Bases: <code>ClassifierMixin</code>, <code>DecisionTreeTabPFNBase</code></p> <p>Class that implements a DT-TabPFN model based on sklearn package</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNClassifier.apply_tree","title":"apply_tree","text":"<pre><code>apply_tree(X)\n</code></pre> <p>Apply tree for different kinds of tree types. TODO: This function could also be overwritten in each type of tree</p> <p>(N_samples, N_nodes, N_estimators) :param bootstrap_X: :return:</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X, check_input=True)\n</code></pre> <p>Predicts X_test :param X: Data that should be evaluated :param check_input: :return: Labels of the predictions</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNClassifier.predict_","title":"predict_","text":"<pre><code>predict_(X, y=None, check_input=True)\n</code></pre> <p>Predicts X :param X: Data that should be evaluated :param y: True labels of holdout data used for adaptive tree.     - If not None: Prunes nodes based on the performance of the holdout data y     - If None: Predicts the data based on the previous hold out performances :param check_input: :return: Probabilities of each class</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X, check_input=True)\n</code></pre> <p>Predicts X_test :param X: Data that should be evaluated :param check_input: :return: Probabilities of each class</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNClassifier.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features)\n</code></pre> <p>Sets categorical features :param categorical_features: Categorical features :return: None</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNRegressor","title":"DecisionTreeTabPFNRegressor","text":"<p>             Bases: <code>RegressorMixin</code>, <code>DecisionTreeTabPFNBase</code></p> <p>Class that implements a DT-TabPFN model based on sklearn package</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNRegressor.apply_tree","title":"apply_tree","text":"<pre><code>apply_tree(X)\n</code></pre> <p>Apply tree for different kinds of tree types. TODO: This function could also be overwritten in each type of tree</p> <p>(N_samples, N_nodes, N_estimators) :param bootstrap_X: :return:</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(X, check_input=True)\n</code></pre> <p>Predicts X_test :param X: Data that should be evaluated :param check_input: :return: Labels of the predictions</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNRegressor.predict_","title":"predict_","text":"<pre><code>predict_(X, y=None, check_input=True)\n</code></pre> <p>Predicts X :param X: Data that should be evaluated :param y: True labels of holdout data used for adaptive tree.     - If not None: Prunes nodes based on the performance of the holdout data y     - If None: Predicts the data based on the previous hold out performances :param check_input: :return: Probabilities of each class</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNRegressor.predict_full","title":"predict_full","text":"<pre><code>predict_full(X)\n</code></pre> <p>Predicts X :param X: Data that should be evaluated :param check_input: :return: Labels of the predictions</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedDecisionTreeTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedDecisionTreeTabPFN.DecisionTreeTabPFNRegressor.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features)\n</code></pre> <p>Sets categorical features :param categorical_features: Categorical features :return: None</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/","title":"SklearnBasedRandomForestTabPFN","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN","title":"SklearnBasedRandomForestTabPFN","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNBase","title":"RandomForestTabPFNBase","text":"<p>Base Class for common functionalities.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNBase.fit","title":"fit","text":"<pre><code>fit(X, y, sample_weight=None)\n</code></pre> <p>Fits RandomForestTabPFN :param X: Feature training data :param y: Label training data :param sample_weight: Weights of each sample :return: None.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNBase.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features)\n</code></pre> <p>Sets categorical features :param categorical_features: Categorical features :return: None.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNClassifier","title":"RandomForestTabPFNClassifier","text":"<p>             Bases: <code>RandomForestTabPFNBase</code>, <code>RandomForestClassifier</code></p> <p>RandomForestTabPFNClassifier.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNClassifier.fit","title":"fit","text":"<pre><code>fit(X, y, sample_weight=None)\n</code></pre> <p>Fits RandomForestTabPFN :param X: Feature training data :param y: Label training data :param sample_weight: Weights of each sample :return: None.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict class for X.</p> <p>The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNClassifier.predict--parameters","title":"Parameters","text":"<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)     The input samples. Internally, its dtype will be converted to     <code>dtype=np.float32</code>. If a sparse matrix is provided, it will be     converted into a sparse <code>csr_matrix</code>.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNClassifier.predict--returns","title":"Returns:","text":"<p>y : ndarray of shape (n_samples,) or (n_samples, n_outputs)     The predicted classes.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict class probabilities for X.</p> <p>The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNClassifier.predict_proba--parameters","title":"Parameters","text":"<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)     The input samples. Internally, its dtype will be converted to     <code>dtype=np.float32</code>. If a sparse matrix is provided, it will be     converted into a sparse <code>csr_matrix</code>.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNClassifier.predict_proba--returns","title":"Returns:","text":"<p>p : ndarray of shape (n_samples, n_classes), or a list of such arrays     The class probabilities of the input samples. The order of the     classes corresponds to that in the attribute :term:<code>classes_</code>.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNClassifier.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features)\n</code></pre> <p>Sets categorical features :param categorical_features: Categorical features :return: None.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNRegressor","title":"RandomForestTabPFNRegressor","text":"<p>             Bases: <code>RandomForestTabPFNBase</code>, <code>RandomForestRegressor</code></p> <p>RandomForestTabPFNClassifier.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNRegressor.fit","title":"fit","text":"<pre><code>fit(X, y, sample_weight=None)\n</code></pre> <p>Fits RandomForestTabPFN :param X: Feature training data :param y: Label training data :param sample_weight: Weights of each sample :return: None.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict regression target for X.</p> <p>The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNRegressor.predict--parameters","title":"Parameters","text":"<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)     The input samples. Internally, its dtype will be converted to     <code>dtype=np.float32</code>. If a sparse matrix is provided, it will be     converted into a sparse <code>csr_matrix</code>.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNRegressor.predict--returns","title":"Returns:","text":"<p>y : ndarray of shape (n_samples,) or (n_samples, n_outputs)     The predicted values.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/SklearnBasedRandomForestTabPFN/#tabpfn_extensions.rf_pfn.SklearnBasedRandomForestTabPFN.RandomForestTabPFNRegressor.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features)\n</code></pre> <p>Sets categorical features :param categorical_features: Categorical features :return: None.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/configs/","title":"Configs","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/configs/#tabpfn_extensions.rf_pfn.configs","title":"configs","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/utils/","title":"Utils","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/utils/#tabpfn_extensions.rf_pfn.utils","title":"utils","text":"<p>Copyright 2023</p> <p>Author: Lukas Schweizer schweizer.lukas@web.de</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/utils/#tabpfn_extensions.rf_pfn.utils.preprocess_data","title":"preprocess_data","text":"<pre><code>preprocess_data(\n    data,\n    nan_values=True,\n    one_hot_encoding=False,\n    normalization=True,\n    categorical_indices=None,\n)\n</code></pre> <p>This method preprocesses data regarding missing values, categorical features and data normalization (for the kNN Model) :param data: Data to preprocess :param nan_values: Preprocesses nan values if True :param one_hot_encoding: Whether use OHE for categoricals :param normalization: Normalizes data if True :param categorical_indices: Categorical columns of data :return: Preprocessed version of the data</p>"},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/","title":"Scoring utils","text":""},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/#tabpfn_extensions.scoring.scoring_utils","title":"scoring_utils","text":""},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/#tabpfn_extensions.scoring.scoring_utils.safe_roc_auc_score","title":"safe_roc_auc_score","text":"<pre><code>safe_roc_auc_score(y_true, y_score, **kwargs)\n</code></pre> <p>Compute the Area Under the Receiver Operating Characteristic Curve (ROC AUC) score.</p> <p>This function is a safe wrapper around <code>sklearn.metrics.roc_auc_score</code> that handles cases where the input data may have missing classes or binary classification problems.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>array-like of shape (n_samples,) True binary labels or binary label indicators.</p> required <code>y_score</code> <p>array-like of shape (n_samples,) or (n_samples, n_classes) Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.</p> required <code>**kwargs</code> <p>dict Additional keyword arguments to pass to <code>sklearn.metrics.roc_auc_score</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The ROC AUC score.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are missing classes in <code>y_true</code> that cannot be handled.</p>"},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/#tabpfn_extensions.scoring.scoring_utils.score_classification","title":"score_classification","text":"<pre><code>score_classification(\n    optimize_metric: Literal[\n        \"roc\", \"auroc\", \"accuracy\", \"f1\", \"log_loss\"\n    ],\n    y_true,\n    y_pred,\n    sample_weight=None,\n    *,\n    y_pred_is_labels: bool = False\n)\n</code></pre> <p>General function to score classification predictions.</p> <p>Parameters:</p> Name Type Description Default <code>optimize_metric</code> <p>{\"roc\", \"auroc\", \"accuracy\", \"f1\", \"log_loss\"} The metric to use for scoring the predictions.</p> required <code>y_true</code> <p>array-like of shape (n_samples,) True labels or binary label indicators.</p> required <code>y_pred</code> <p>array-like of shape (n_samples,) or (n_samples, n_classes) Predicted labels, probabilities, or confidence values.</p> required <code>sample_weight</code> <p>array-like of shape (n_samples,), default=None Sample weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The score for the specified metric.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown metric is specified.</p>"},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/#tabpfn_extensions.scoring.scoring_utils.score_regression","title":"score_regression","text":"<pre><code>score_regression(\n    optimize_metric: Literal[\"rmse\", \"mse\", \"mae\"],\n    y_true,\n    y_pred,\n    sample_weight=None,\n)\n</code></pre> <p>General function to score regression predictions.</p> <p>Parameters:</p> Name Type Description Default <code>optimize_metric</code> <p>{\"rmse\", \"mse\", \"mae\"} The metric to use for scoring the predictions.</p> required <code>y_true</code> <p>array-like of shape (n_samples,) True target values.</p> required <code>y_pred</code> <p>array-like of shape (n_samples,) Predicted target values.</p> required <code>sample_weight</code> <p>array-like of shape (n_samples,), default=None Sample weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The score for the specified metric.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown metric is specified.</p>"},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/#tabpfn_extensions.scoring.scoring_utils.score_survival","title":"score_survival","text":"<pre><code>score_survival(\n    optimize_metric: Literal[\"cindex\"],\n    y_true,\n    y_pred,\n    event_observed,\n    sample_weight=None,\n)\n</code></pre> <p>General function to score regression predictions.</p> <p>Parameters:</p> Name Type Description Default <code>optimize_metric</code> <p>{\"rmse\", \"mse\", \"mae\"} The metric to use for scoring the predictions.</p> required <code>y_true</code> <p>array-like of shape (n_samples,) True target values.</p> required <code>y_pred</code> <p>array-like of shape (n_samples,) Predicted target values.</p> required <code>sample_weight</code> <p>array-like of shape (n_samples,), default=None Sample weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The score for the specified metric.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown metric is specified.</p>"},{"location":"reference/tabpfn_extensions/sklearn_ensembles/configs/","title":"Configs","text":""},{"location":"reference/tabpfn_extensions/sklearn_ensembles/configs/#tabpfn_extensions.sklearn_ensembles.configs","title":"configs","text":""},{"location":"reference/tabpfn_extensions/sklearn_ensembles/meta_models/","title":"Meta models","text":""},{"location":"reference/tabpfn_extensions/sklearn_ensembles/meta_models/#tabpfn_extensions.sklearn_ensembles.meta_models","title":"meta_models","text":""},{"location":"reference/tabpfn_extensions/sklearn_ensembles/meta_models/#tabpfn_extensions.sklearn_ensembles.meta_models.get_tabpfn_outer_ensemble","title":"get_tabpfn_outer_ensemble","text":"<pre><code>get_tabpfn_outer_ensemble(config: TabPFNConfig, **kwargs)\n</code></pre> <p>This will create a model very similar to our standard TabPFN estimators, but it uses multiple model weights to generate predictions. Thus the <code>configs.TabPFNModelPathsConfig</code> can contain multiple paths which are all used.</p> <p>A product of the preprocessor_trasnforms and paths is created to yield interesting ensemble members.</p> <p>This only supports multiclass for now. If you want to add regression, you probably want to add the y_transforms to the relevant_config_product. :param config: TabPFNConfig :param kwargs: kwargs are passed to get_single_tabpfn, e.g. device :return: A TabPFNEnsemble, which is a soft voting classifier that mixes multiple standard TabPFN estimators.</p>"},{"location":"reference/tabpfn_extensions/sklearn_ensembles/weighted_ensemble/","title":"Weighted ensemble","text":""},{"location":"reference/tabpfn_extensions/sklearn_ensembles/weighted_ensemble/#tabpfn_extensions.sklearn_ensembles.weighted_ensemble","title":"weighted_ensemble","text":""},{"location":"reference/tabpfn_extensions/unsupervised/experiments/","title":"Experiments","text":""},{"location":"reference/tabpfn_extensions/unsupervised/experiments/#tabpfn_extensions.unsupervised.experiments","title":"experiments","text":""},{"location":"reference/tabpfn_extensions/unsupervised/experiments/#tabpfn_extensions.unsupervised.experiments.EmbeddingUnsupervisedExperiment","title":"EmbeddingUnsupervisedExperiment","text":"<p>             Bases: <code>Experiment</code></p> <p>This class is used to run experiments on synthetic toy functions.</p>"},{"location":"reference/tabpfn_extensions/unsupervised/experiments/#tabpfn_extensions.unsupervised.experiments.GenerateSyntheticDataExperiment","title":"GenerateSyntheticDataExperiment","text":"<p>             Bases: <code>Experiment</code></p> <p>This class is used to run experiments on generating synthetic data.</p>"},{"location":"reference/tabpfn_extensions/unsupervised/experiments/#tabpfn_extensions.unsupervised.experiments.GenerateSyntheticDataExperiment.run","title":"run","text":"<pre><code>run(tabpfn, **kwargs)\n</code></pre> <p>:param tabpfn: :param kwargs:     indices: list of indices from X features to use :return:</p>"},{"location":"reference/tabpfn_extensions/unsupervised/experiments/#tabpfn_extensions.unsupervised.experiments.OutlierDetectionUnsupervisedExperiment","title":"OutlierDetectionUnsupervisedExperiment","text":"<p>             Bases: <code>Experiment</code></p> <p>This class is used to run experiments for outlier detection.</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/","title":"Unsupervised","text":""},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised","title":"unsupervised","text":""},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel","title":"TabPFNUnsupervisedModel","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>TabPFN experiments model for imputation, outlier detection, and synthetic data generation.</p> <p>This model combines a TabPFNClassifier for categorical features and a TabPFNRegressor for numerical features to perform various experiments learning tasks on tabular data.</p> <p>Parameters:</p> Name Type Description Default <code>tabpfn_clf</code> <p>TabPFNClassifier, optional TabPFNClassifier instance for handling categorical features. If not provided, the model assumes that there are no categorical features in the data.</p> <code>None</code> <code>tabpfn_reg</code> <p>TabPFNRegressor, optional TabPFNRegressor instance for handling numerical features. If not provided, the model assumes that there are no numerical features in the data.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>categorical_features</code> <p>list List of indices of categorical features in the input data.</p> Example<pre><code>&gt;&gt;&gt; tabpfn_clf = TabPFNClassifier()\n&gt;&gt;&gt; tabpfn_reg = TabPFNRegressor()\n&gt;&gt;&gt; model = TabPFNUnsupervisedModel(tabpfn_clf, tabpfn_reg)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\n&gt;&gt;&gt; model.fit(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_imputed = model.impute(X)\n&gt;&gt;&gt; X_outliers = model.outliers(X)\n&gt;&gt;&gt; X_synthetic = model.generate_synthetic_data(n_samples=100)\n</code></pre>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.fit","title":"fit","text":"<pre><code>fit(X: ndarray, y: Optional[ndarray] = None) -&gt; None\n</code></pre> <p>Fit the model to the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Input data to fit the model.</p> required <code>y</code> <p>array-like of shape (n_samples,), optional Target values.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>None</code> <p>TabPFNUnsupervisedModel Fitted model.</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.generate_synthetic_data","title":"generate_synthetic_data","text":"<pre><code>generate_synthetic_data(\n    n_samples=100, t=1.0, n_permutations=3\n)\n</code></pre> <p>Generate synthetic data using the trained models. Uses imputation method to generate synthetic data, passed with a matrix of nans. Samples are generated feature by feature in one pass, so samples are not dependent on each other per feature.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <p>int, default=100 Number of synthetic samples to generate.</p> <code>100</code> <code>t</code> <p>float, default=1.0 Temperature for sampling from the imputation distribution. Lower values result in more deterministic samples.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>torch.Tensor of shape (n_samples, n_features) Generated synthetic data.</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    X: tensor, per_column: bool = False\n) -&gt; tensor\n</code></pre> <p>Get the transformer embeddings for the test data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>tensor</code> required <p>Returns:</p> Type Description <code>tensor</code> <p>torch.Tensor of shape (n_samples, embedding_dim)</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.get_embeddings_per_column","title":"get_embeddings_per_column","text":"<pre><code>get_embeddings_per_column(X: tensor) -&gt; tensor\n</code></pre> <p>Alternative implementation for get_embeddings, where we get the embeddings for each column as a label  separately and concatenate the results. This alternative way needs more passes but might be more accurate</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.impute","title":"impute","text":"<pre><code>impute(\n    X: tensor, t: float = 1e-09, n_permutations: int = 10\n) -&gt; tensor\n</code></pre> <p>Impute missing values in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>torch.Tensor of shape (n_samples, n_features) Input data with missing values encoded as np.nan.</p> required <code>t</code> <p>float, default=0.000000001 Temperature for sampling from the imputation distribution. Lower values result in more deterministic imputations.</p> <code>1e-09</code> <p>Returns:</p> Type Description <code>tensor</code> <p>torch.Tensor of shape (n_samples, n_features) Imputed data with missing values replaced.</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.impute_","title":"impute_","text":"<pre><code>impute_(\n    X: tensor,\n    t: float = 1e-09,\n    n_permutations: int = 10,\n    condition_on_all_features: bool = True,\n) -&gt; tensor\n</code></pre> <p>Impute missing values (np.nan) in X by sampling all cells independently from the trained models</p> <p>:param X: Input data of the shape (num_examples, num_features) with missing values encoded as np.nan :param t: Temperature for sampling from the imputation distribution, lower values are more deterministic :return: Imputed data, with missing values replaced</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.impute_single_permutation_","title":"impute_single_permutation_","text":"<pre><code>impute_single_permutation_(\n    X: tensor,\n    feature_permutation: list[int] | tuple[int],\n    t: float = 1e-09,\n    condition_on_all_features: bool = True,\n) -&gt; tensor\n</code></pre> <p>Impute missing values (np.nan) in X by sampling all cells independently from the trained models</p> <p>:param X: Input data of the shape (num_examples, num_features) with missing values encoded as np.nan :param t: Temperature for sampling from the imputation distribution, lower values are more deterministic :return: Imputed data, with missing values replaced</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.outliers","title":"outliers","text":"<pre><code>outliers(X: tensor, n_permutations: int = 10) -&gt; tensor\n</code></pre> <p>Preferred implementation for outliers, where we calculate the sample probability for each sample in X by multiplying the probabilities of each feature according to chain rule of probability. The first feature is estimated by using a zero feature as input.</p> <p>Args     X: Samples to calculate the sample probability for, shape (n_samples, n_features)</p> <p>Returns:</p> Type Description <code>tensor</code> <p>Sample unnormalized probability for each sample in X, shape (n_samples,)</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.efficient_random_permutation_","title":"efficient_random_permutation_","text":"<pre><code>efficient_random_permutation_(indices)\n</code></pre> <p>Generate a single random permutation from a very large space.</p> <p>:param n: The size of the permutation (number of elements) :return: A list representing a random permutation of numbers from 0 to n-1</p>"},{"location":"research/papers/","title":"Papers","text":""},{"location":"research/papers/#tabpfn-followups","title":"TabPFN Followups","text":"<p>Forecastpfn: Synthetically-trained zero-shot forecasting    Dooley, Khurana, Mohapatra, Naidu, White Advances in Neural Information Processing Systems, 2024, Volume 36.</p> <p>Interpretable machine learning for TabPFN    Rundel, Kobialka, von Crailsheim, Feurer, Nagler, R{\"u}gamer World Conference on Explainable Artificial Intelligence, 2024, Pages 465--476.</p> <p>Scaling tabpfn: Sketching and feature selection for tabular prior-data fitted networks    Feuer, Hegde, Cohen arXiv preprint arXiv:2311.10609, 2023.</p> <p>In-Context Data Distillation with TabPFN    Ma, Thomas, Yu, Caterini arXiv preprint arXiv:2402.06971, 2024.</p> <p>Tokenize features, enhancing tables: the FT-TABPFN model for tabular classification    Liu, Yang, Liang, Pang, Zou arXiv preprint arXiv:2406.06891, 2024.</p> <p>Towards Localization via Data Embedding for TabPFN    Koshil, Nagler, Feurer, Eggensperger NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Enhancing Classification Performance Through the Synergistic Use of XGBoost, TABPFN, and LGBM Models    Prabowo, others 2023 15<sup>th</sup> International Congress on Advanced Applied Informatics Winter (IIAI-AAI-Winter), 2023, Pages 255--259.</p> <p>The Tabular Foundation Model TabPFN Outperforms Specialized Time Series Forecasting Models Based on Simple Features    Hoo, M{\"u}ller, Salinas, Hutter NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabPFGen--Tabular Data Generation with TabPFN    Ma, Dankar, Stein, Yu, Caterini arXiv preprint arXiv:2406.05216, 2024.</p> <p>Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data    Helli, Schnurr, Hollmann, M{\"u}ller, Hutter arXiv preprint arXiv:2411.10634, 2024.</p> <p>TabFlex: Scaling Tabular Learning to Millions with Linear Attention    Zeng, Kang, Mueller NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Retrieval \\&amp; Fine-Tuning for In-Context Tabular Models    Thomas, Ma, Hosseinzadeh, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2406.05207, 2024.</p> <p>TabDPT: Scaling Tabular Foundation Models    Ma, Thomas, Hosseinzadeh, Kamkari, Labach, Cresswell, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2410.18164, 2024.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    Breejen, Bae, Cha, Yun arXiv preprint arXiv:2405.13396, 2024.</p> <p>MotherNet: Fast Training and Inference via Hyper-Network Transformers    Mueller, Curino, Ramakrishnan NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Mixture of In-Context Prompters for Tabular PFNs    Xu, Cirit, Asadi, Sun, Wang arXiv preprint arXiv:2405.16156, 2024.</p> <p>Fast and Accurate Zero-Training Classification for Tabular Engineering Data    Picard, Ahmed arXiv preprint arXiv:2401.06948, 2024.</p> <p>Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning    den Breejen, Bae, Cha, Kim, Koh, Yun NeurIPS 2023 Second Table Representation Learning Workshop, 2023.</p> <p>TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks    Feuer, Schirrmeister, Cherepanova, Hegde, Hutter, Goldblum, Cohen, White arXiv preprint arXiv:2402.11137, 2024.</p> <p>Exploration of autoregressive models for in-context learning on tabular data    Baur, Kim NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabMDA: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting    Margeloiu, Bazaga, Simidjievski, Li{`o}, Jamnik arXiv preprint arXiv:2406.01805, 2024.</p> <p>Large Scale Transfer Learning for Tabular Data via Language Modeling    Gardner, Perdomo, Schmidt arXiv preprint arXiv:2406.12031, 2024.</p> <p>AnnotatedTables: A Large Tabular Dataset with Language Model Annotations    Hu, Fountalis, Tian, Vasiloglou arXiv preprint arXiv:2406.16349, 2024.</p> <p>TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling    Gorishniy, Kotelnikov, Babenko arXiv preprint arXiv:2410.24210, 2024.</p> <p>Pre-Trained Tabular Transformer for Real-Time, Efficient, Stable Radiomics Data Processing: A Comprehensive Study    Jiang, Jia, Zhang, Li 2023 IEEE International Conference on E-health Networking, Application \\&amp; Services (Healthcom), 2023, Pages 276--281.</p> <p>TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik arXiv preprint arXiv:2409.16118, 2024.</p> <p>Augmenting Small-size Tabular Data with Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>FORECASTPFN: ZERO-SHOT LOW-RESOURCE FORECASTING    Khurana, Dooley, Naidu, White, AI No Source, No Year.</p> <p>What exactly has TabPFN learned to do?    McCarter The Third Blogpost Track at ICLR 2024, No Year.</p> <p>Statistical foundations of prior-data fitted networks    Nagler International Conference on Machine Learning, 2023, Pages 25660--25676.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    den Breejen, Bae, Cha, Yun arXiv e-prints, 2024, Pages arXiv--2405.</p>"},{"location":"research/papers/#tabpfn-application","title":"TabPFN Application","text":"<p>Large-scale chemoproteomics expedites ligand discovery and predicts ligand behavior in cells    Offensperger, Tin, Duran-Frigola, Hahn, Dobner, Ende, Strohbach, Rukavina, Brennsteiner, Ogilvie, others Science, 2024, Volume 384, Issue 6694, Pages eadk5864.</p> <p>Deep learning for cross-selling health insurance classification    Chu, Than, Jo 2024 International Conference on Green Energy, Computing and Sustainable Technology (GECOST), 2024, Pages 453--457.</p> <p>Early fault classification in rotating machinery with limited data using TabPFN    Magad{'a}n, Rold{'a}n-G{'o}mez, Granda, Su{'a}rez IEEE Sensors Journal, 2023.</p> <p>Artificial intelligence-driven predictive framework for early detection of still birth    Alzakari, Aldrees, Umer, Cascone, Innab, Ashraf SLAS technology, 2024, Volume 29, Issue 6, Pages 100203.</p> <p>Prostate Cancer Diagnosis via Visual Representation of Tabular Data and Deep Transfer Learning    El-Melegy, Mamdouh, Ali, Badawy, El-Ghar, Alghamdi, El-Baz Bioengineering, 2024, Volume 11, Issue 7, Pages 635.</p> <p>A machine learning-based approach for individualized prediction of short-term outcomes after anterior cervical corpectomy    Karabacak, Schupper, Carr, Margetis Asian Spine Journal, 2024, Volume 18, Issue 4, Pages 541.</p> <p>Comparing the Performance of a Deep Learning Model (TabPFN) for Predicting River Algal Blooms with Varying Data Composition    Yang, Park Journal of Wetlands Research, 2024, Volume 26, Issue 3, Pages 197--203.</p> <p>Adapting TabPFN for Zero-Inflated Metagenomic Data    Perciballi, Granese, Fall, Zehraoui, Prifti, Zucker NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Comprehensive peripheral blood immunoprofiling reveals five immunotypes with immunotherapy response characteristics in patients with cancer    Dyikanov, Zaitsev, Vasileva, Wang, Sokolov, Bolshakov, Frank, Turova, Golubeva, Gantseva, others Cancer Cell, 2024, Volume 42, Issue 5, Pages 759--779.</p> <p>Predicting dementia in Parkinson's disease on a small tabular dataset using hybrid LightGBM--TabPFN and SHAP    Tran, Byeon Digital Health, 2024, Volume 10, Pages 20552076241272585.</p> <p>Enhancing actuarial non-life pricing models via transformers    Brauer European Actuarial Journal, 2024, Pages 1--22.</p> <p>Machine learning-based diagnostic prediction of minimal change disease: model development study    Noda, Ichikawa, Shibagaki Scientific Reports, 2024, Volume 14, Issue 1, Pages 23460.</p> <p>Using AutoML and generative AI to predict the type of wildfire propagation in Canadian conifer forests    Khanmohammadi, Cruz, Perrakis, Alexander, Arashpour Ecological Informatics, 2024, Volume 82, Pages 102711.</p> <p>Machine learning applications on lunar meteorite minerals: From classification to mechanical properties prediction    Pe{~n}a-Asensio, Trigo-Rodr{'\\i}guez, Sort, Ib{'a}{~n}ez-Insa, Rimola International Journal of Mining Science and Technology, 2024.</p> <p>Data-Driven Prognostication in Distal Medium Vessel Occlusions Using Explainable Machine Learning    Karabacak, Ozkara, Faizy, Hardigan, Heit, Lakhani, Margetis, Mocco, Nael, Wintermark, others American Journal of Neuroradiology, 2024.</p>"},{"location":"tutorials/cheat_sheet/","title":"Cheat Sheet / Best practices","text":"<p>Look at Autogluon cheat sheet [https://auto.gluon.ai/stable/cheatsheet.html]</p>"},{"location":"tutorials/classification/","title":"Classification","text":"<p>TabPFN provides a powerful interface for handling classification tasks on tabular data. The <code>TabPFNClassifier</code> class can be used for binary and multi-class classification problems.</p>"},{"location":"tutorials/classification/#example","title":"Example","text":"<p>Below is an example of how to use <code>TabPFNClassifier</code> for a multi-class classification task:</p> Python API Client (No GPU, Online)Python Local (GPU) <pre><code>from tabpfn_client import TabPFNClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train classifier\nclassifier = TabPFNClassifier(device='cuda', N_ensemble_configurations=10)\nclassifier.fit(X_train, y_train)\n\n# Evaluate\ny_pred = classifier.predict(X_test)\nprint('Test Accuracy:', accuracy_score(y_test, y_pred))\n</code></pre> <pre><code>from tabpfn import TabPFNClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train classifier\nclassifier = TabPFNClassifier(device='cuda', N_ensemble_configurations=10)\nclassifier.fit(X_train, y_train)\n\n# Evaluate\ny_pred = classifier.predict(X_test)\nprint('Test Accuracy:', accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"tutorials/classification/#example-with-autotabpfnclassifier","title":"Example with AutoTabPFNClassifier","text":"<p>Abstract</p> <p>AutoTabPFNClassifier yields the most accurate predictions for TabPFN and is recommended for most use cases. The AutoTabPFNClassifier and AutoTabPFNRegressor automatically run a hyperparameter search and build an ensemble of strong hyperparameters. You can control the runtime using \u00b4max_time\u00b4 and need to make no further adjustments to get best results.</p> <pre><code>from tabpfn_extensions.post_hoc_ensembles.sklearn_interface import AutoTabPFNClassifier\n\n# we refer to the PHE variant of TabPFN as AutoTabPFN in the code\nclf = AutoTabPFNClassifier(device='auto', max_time=30)\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict_proba(X_test)\ny_eval = np.argmax(preds, axis=1)\n\nprint('ROC AUC: ',  sklearn.metrics.roc_auc_score(y_test, preds[:,1], multi_class='ovr'), 'Accuracy', sklearn.metrics.accuracy_score(y_test, y_eval))\n</code></pre>"},{"location":"tutorials/distshift/","title":"TabPFN's Out-of-Distribution Excellence","text":"<p>Recent research demonstrates TabPFN's exceptional out-of-distribution (OOD) performance on tabular data, with further improvements through Drift-Resilient modifications.</p>"},{"location":"tutorials/distshift/#key-performance-metrics","title":"Key Performance Metrics","text":"Model OOD Accuracy OOD ROC AUC TabPFN Base 0.688 0.786 TabPFN + Drift-Resilient 0.744 0.832 XGBoost 0.664 0.754 CatBoost 0.677 0.766"},{"location":"tutorials/distshift/#technical-improvements","title":"Technical Improvements","text":"<p>The Drift-Resilient modifications introduce:</p> <ul> <li>2<sup>nd</sup>-order structural causal model for temporal adaptation</li> <li>Enhanced pattern recognition across distribution shifts </li> <li>Zero hyperparameter tuning requirement</li> <li>Inference in seconds on small/medium datasets</li> </ul>"},{"location":"tutorials/distshift/#benchmark","title":"Benchmark","text":"<p>The enhanced model shows robust generalization across:</p> <ul> <li>18 diverse datasets (synthetic + real-world)</li> <li>Various temporal shift patterns</li> <li>Multiple industry applications</li> </ul> <p>For comprehensive documentation and implementation details, visit the GitHub repository.</p>"},{"location":"tutorials/distshift/#citation","title":"Citation","text":"<pre><code>@inproceedings{\n  helli2024driftresilient,\n  title={Drift-Resilient Tab{PFN}: In-Context Learning Temporal Distribution Shifts on Tabular Data},\n  author={Kai Helli and David Schnurr and Noah Hollmann and Samuel M{\\\"u}ller and Frank Hutter},\n  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\n  year={2024},\n  url={https://openreview.net/forum?id=p3tSEFMwpG}\n}\n</code></pre>"},{"location":"tutorials/example_competitions/","title":"Cheat Sheet / Best practices","text":"<p>Look at Autogluon</p>"},{"location":"tutorials/regression/","title":"Regression","text":"<p>TabPFN can also be applied to regression tasks using the <code>TabPFNRegressor</code> class. This allows for predictive modeling of continuous outcomes.</p>"},{"location":"tutorials/regression/#example","title":"Example","text":"<p>An example usage of <code>TabPFNRegressor</code> is shown below:</p> Python API Client (No GPU, Online)Python Local (GPU) <pre><code>from tabpfn_client import TabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = TabPFNRegressor(device='auto')\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre> <pre><code>from tabpfn import TabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = TabPFNRegressor(device='auto')\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre> <p>This example demonstrates how to train and evaluate a regression model. For more details on TabPFNRegressor and its parameters, refer to the API Reference section.</p>"},{"location":"tutorials/regression/#example-with-autotabpfnregressor","title":"Example with AutoTabPFNRegressor","text":"<p>Abstract</p> <p>AutoTabPFNRegressor yields the most accurate predictions for TabPFN and is recommended for most use cases. The AutoTabPFNClassifier and AutoTabPFNRegressor automatically run a hyperparameter search and build an ensemble of strong hyperparameters. You can control the runtime using \u00b4max_time\u00b4 and need to make no further adjustments to get best results.</p> <pre><code>from tabpfn_extensions.post_hoc_ensembles.sklearn_interface import AutoTabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = AutoTabPFNRegressor(max_time=30) # runs for 30 seconds\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre>"},{"location":"tutorials/reproduction/","title":"Experimental Reproduction","text":"<p>Our code is currently stored in a private repository on GitHub, to avoid indexing at this point. We do not share this links on this public website. To access our code and any example notebooks, please use the notebook provided in the links to our submission via the \u201clinktree\u201d in our main paper or the code submission checklist. </p>"},{"location":"tutorials/timeseries/","title":"Time Series Tutorial","text":"<p>TabPFN can be used for time series forecasting by framing it as a tabular regression problem. This tutorial demonstrates how to use the TabPFN Time Series package for accurate zero-shot forecasting. It was developed by Shi Bin Hoo, Samuel M\u00fcller, David Salinas and Frank Hutter.</p>"},{"location":"tutorials/timeseries/#quick-start","title":"Quick Start","text":"<p>First, install the package:</p> <pre><code>!git clone https://github.com/liam-sbhoo/tabpfn-time-series.git\n!pip install -r tabpfn-time-series/requirements.txt\n</code></pre> <p>See the demo notebook for a complete example.</p>"},{"location":"tutorials/timeseries/#how-it-works","title":"How It Works","text":"<p>TabPFN performs time series forecasting by:</p> <ol> <li>Converting time series data into a tabular format</li> <li>Extracting temporal features (trends, seasonality, etc.)</li> <li>Using TabPFN's regression capabilities for prediction</li> <li>Converting predictions back to time series format</li> </ol> <p>This approach provides several benefits:</p> <ul> <li>Zero-shot forecasting: No training required - just fit and predict</li> <li>Both point and probabilistic forecasts: Get confidence intervals with your predictions</li> <li>Support for exogenous variables: Easily incorporate external factors</li> <li>Fast inference: Uses tabpfn-client for GPU-accelerated predictions</li> </ul>"},{"location":"tutorials/timeseries/#additional-resources","title":"Additional Resources","text":"<ul> <li>GitHub Repository</li> <li>Research Paper</li> </ul>"},{"location":"tutorials/timeseries/#getting-help","title":"Getting Help","text":"<p>Join our Discord community for support and discussions about TabPFN time series forecasting.</p>"},{"location":"tutorials/unsupervised/","title":"Unsupervised functionalities","text":"<pre><code>from tabpfn.scripts.estimator import TabPFNUnsupervisedModel, TabPFNClassifier, TabPFNRegressor\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nmodel_unsupervised = TabPFNUnsupervisedModel(TabPFNClassifier(), TabPFNRegressor())\n\n# Load the Iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n\nmodel_unsupervised.fit(X_train, y_train)\nembeddings = model_unsupervised.get_embeddings(X_test)\nX_outliers = model_unsupervised.outliers(X_test)\nX_synthetic = model_unsupervised.generate_synthetic_data(n_samples=100)\n</code></pre>"}]}