{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TabPFN Documentation","text":"<p>TabPFN is a cutting-edge neural network designed specifically for tabular data prediction, leveraging the power of transformers to provide state-of-the-art performance on a wide range of datasets. This documentation will guide you through the installation, basic usage, and advanced features of TabPFN, helping you to efficiently integrate this powerful tool into your data science workflow.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>High Performance: TabPFN brings the power of transformers to tabular data, achieving top-tier results across various datasets.</li> <li>Easy Integration: Designed with an sklearn-like interface, TabPFN can be seamlessly integrated into existing workflows.</li> <li>Versatility: Whether you're working on classification, regression, or even survival analysis tasks, TabPFN has you covered.</li> <li>Efficiency: Optimized for both CPU and GPU, TabPFN ensures fast training and inference times, making it suitable for projects of any scale.</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started: Learn how to quickly train and evaluate your first TabPFN model.</li> <li>Classification: Dive into using TabPFN for classification tasks, with examples and tips.</li> <li>Regression: Explore how to apply TabPFN to regression problems, including setup and evaluation.</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<p>If you encounter any issues or have questions about using TabPFN, please refer to the FAQs or reach out to the community through GitHub Issues.</p> <p>Thank you for choosing TabPFN. We hope you find this tool valuable in your data science endeavors.</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>PreprocessorConfig </p> <p>ClassificationOptimizationMetricType </p> <p>RegressionOptimizationMetricType </p> <p>ManyClassClassifier </p> <p>ClassifierAsRegressor </p>"},{"location":"api_reference/#scripts.estimator.PreprocessorConfig","title":"PreprocessorConfig  <code>dataclass</code>","text":"<p>Configuration for data preprocessors.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal</code> <p>Name of the preprocessor.</p> <code>categorical_name</code> <code>Literal</code> <p>Name of the categorical encoding method. Valid options are \"none\", \"numeric\",                     \"onehot\", \"ordinal\", \"ordinal_shuffled\". Default is \"none\".</p> <code>append_original</code> <code>bool</code> <p>Whether to append the original features to the transformed features. Default is False.</p> <code>subsample_features</code> <code>float</code> <p>Fraction of features to subsample. -1 means no subsampling. Default is -1.</p> <code>global_transformer_name</code> <code>str</code> <p>Name of the global transformer to use. Default is None.</p> Source code in <code>scripts/estimator/configs.py</code> <pre><code>@dataclass(eq=True, frozen=True)\nclass PreprocessorConfig:\n    \"\"\"\n    Configuration for data preprocessors.\n\n    Attributes:\n        name (Literal): Name of the preprocessor.\n        categorical_name (Literal): Name of the categorical encoding method. Valid options are \"none\", \"numeric\",\n                                \"onehot\", \"ordinal\", \"ordinal_shuffled\". Default is \"none\".\n        append_original (bool): Whether to append the original features to the transformed features. Default is False.\n        subsample_features (float): Fraction of features to subsample. -1 means no subsampling. Default is -1.\n        global_transformer_name (str): Name of the global transformer to use. Default is None.\n    \"\"\"\n\n    name: Literal[\n        \"per_feature\",  # a different transformation for each feature\n        \"power\",  # a standard sklearn power transformer\n        \"safepower\",  # a power transformer that prevents some numerical issues\n        \"power_box\",\n        \"safepower_box\",\n        \"quantile_uni_coarse\",  # different quantile transformations with few quantiles up to a lot\n        \"quantile_norm_coarse\",\n        \"quantile_uni\",\n        \"quantile_norm\",\n        \"quantile_uni_fine\",\n        \"quantile_norm_fine\",\n        \"robust\",  # a standard sklearn robust scaler\n        \"kdi\",\n        \"none\",  # no transformation (inside the transformer we anyways do a standardization)\n    ]\n    categorical_name: Literal[\n        \"none\",\n        \"numeric\",\n        \"onehot\",\n        \"ordinal\",\n        \"ordinal_shuffled\",\n        \"ordinal_very_common_categories_shuffled\",\n    ] = \"none\"\n    # categorical_name meanings:\n    # \"none\": categorical features are pretty much treated as ordinal, just not resorted\n    # \"numeric\": categorical features are treated as numeric, that means they are also power transformed for example\n    # \"onehot\": categorical features are onehot encoded\n    # \"ordinal\": categorical features are sorted and encoded as integers from 0 to n_categories - 1\n    # \"ordinal_shuffled\": categorical features are encoded as integers from 0 to n_categories - 1 in a random order\n    append_original: bool = False\n    subsample_features: Optional[float] = -1\n    global_transformer_name: Optional[str] = None\n    # if True, the transformed features (e.g. power transformed) are appended to the original features\n\n    def __str__(self):\n        return (\n            f\"{self.name}_cat:{self.categorical_name}\"\n            + (\"_and_none\" if self.append_original else \"\")\n            + (\n                \"_subsample_feats_\" + str(self.subsample_features)\n                if self.subsample_features &gt; 0\n                else \"\"\n            )\n            + (\n                f\"_global_transformer_{self.global_transformer_name}\"\n                if self.global_transformer_name is not None\n                else \"\"\n            )\n        )\n\n    def can_be_cached(self):\n        return not self.subsample_features &gt; 0\n\n    def to_dict(self):\n        return {k: str(v) for k, v in asdict(self).items()}\n</code></pre>"},{"location":"api_reference/#scripts.estimator.ClassificationOptimizationMetricType","title":"ClassificationOptimizationMetricType  <code>module-attribute</code>","text":"<pre><code>ClassificationOptimizationMetricType = Literal['auroc', 'roc', 'auroc_ovo', 'balanced_acc', 'acc', 'log_loss', None]\n</code></pre>"},{"location":"api_reference/#scripts.estimator.RegressionOptimizationMetricType","title":"RegressionOptimizationMetricType  <code>module-attribute</code>","text":"<pre><code>RegressionOptimizationMetricType = Literal['mse', 'rmse', 'mae', 'r2', 'mean', 'median', 'mode', 'exact_match', None]\n</code></pre>"},{"location":"api_reference/#scripts.estimator.ManyClassClassifier","title":"ManyClassClassifier","text":"<p>             Bases: <code>OutputCodeClassifier</code></p> <p>Output-Code multiclass strategy with deciary codebook.</p> <p>This class extends the original OutputCodeClassifier to support n-ary codebooks (with n=alphabet_size), allowing for handling more classes.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>estimator object An estimator object implementing :term:<code>fit</code> and one of :term:<code>decision_function</code> or :term:<code>predict_proba</code>. The base classifier should be able to handle up to <code>alphabet_size</code> classes.</p> required <code>random_state</code> <p>int, RandomState instance, default=None The generator used to initialize the codebook. Pass an int for reproducible output across multiple function calls. See :term:<code>Glossary &lt;random_state&gt;</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimators_</code> <p>list of <code>int(n_classes * code_size)</code> estimators Estimators used for predictions.</p> <code>classes_</code> <p>ndarray of shape (n_classes,) Array containing labels.</p> <code>code_book_</code> <p>ndarray of shape (n_classes, <code>len(estimators_)</code>) Deciary array containing the code of each class.</p> <p>from sklearn.datasets import load_iris from tabpfn.scripts.estimator import ManyClassClassifier, TabPFNClassifier from sklearn.model_selection import train_test_split x, y = load_iris(return_X_y=True) x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42) clf = TabPFNClassifier() clf = ManyClassClassifier(clf, alphabet_size=clf.max_num_classes_) clf.fit(x_train, y_train) clf.predict(x_test)</p> Source code in <code>scripts/estimator/many_class_classifier.py</code> <pre><code>class ManyClassClassifier(OutputCodeClassifier):\n    \"\"\"Output-Code multiclass strategy with deciary codebook.\n\n    This class extends the original OutputCodeClassifier to support n-ary codebooks (with n=alphabet_size),\n    allowing for handling more classes.\n\n    Parameters:\n        estimator : estimator object\n            An estimator object implementing :term:`fit` and one of\n            :term:`decision_function` or :term:`predict_proba`. The base classifier\n            should be able to handle up to `alphabet_size` classes.\n\n        random_state : int, RandomState instance, default=None\n            The generator used to initialize the codebook.\n            Pass an int for reproducible output across multiple function calls.\n            See :term:`Glossary &lt;random_state&gt;`.\n\n    Attributes:\n        estimators_ : list of `int(n_classes * code_size)` estimators\n            Estimators used for predictions.\n\n        classes_ : ndarray of shape (n_classes,)\n            Array containing labels.\n\n        code_book_ : ndarray of shape (n_classes, `len(estimators_)`)\n            Deciary array containing the code of each class.\n\n    Examples:\n    &gt;&gt;&gt; from sklearn.datasets import load_iris\n    &gt;&gt;&gt; from tabpfn.scripts.estimator import ManyClassClassifier, TabPFNClassifier\n    &gt;&gt;&gt; from sklearn.model_selection import train_test_split\n    &gt;&gt;&gt; x, y = load_iris(return_X_y=True)\n    &gt;&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n    &gt;&gt;&gt; clf = TabPFNClassifier()\n    &gt;&gt;&gt; clf = ManyClassClassifier(clf, alphabet_size=clf.max_num_classes_)\n    &gt;&gt;&gt; clf.fit(x_train, y_train)\n    &gt;&gt;&gt; clf.predict(x_test)\n    \"\"\"\n\n    def __init__(\n        self,\n        estimator,\n        *,\n        alphabet_size=None,\n        n_estimators=None,\n        random_state=None,\n    ):\n        self.estimator = estimator\n        self.random_state = random_state\n        self.alphabet_size = alphabet_size\n        self.n_estimators = n_estimators\n\n    def get_alphabet_size(self):\n        if self.alphabet_size is None:\n            return self.estimator.max_num_classes_\n\n    def get_n_estimators(self, n_classes):\n        if self.n_estimators is None:\n            return math.ceil(math.log(n_classes, self.get_alphabet_size())) * 2\n\n    def _generate_codebook(self, n_classes, n_estimators, alphabet_size):\n        \"\"\"Generate an efficient codebook using the provided alphabet size.\n\n        This function generates a codebook where for each codeword at most `alphabet_size - 1` classes\n        are mapped to unique classes, and the remaining classes are mapped to the \"rest\" class. The codebook\n        is generated to provide for each class a unique codeword with maximum reduncancy between codewords.\n        Greedy optimization is used to find the optimal codewords.\n\n        Parameters:\n            n_classes : int\n                Number of classes.\n            n_estimators : int\n                Number of estimators.\n            alphabet_size : int, default=10\n                Size of the alphabet for the codebook. The first `alphabet_size - 1` classes\n                will be mapped to unique classes, and the remaining classes will be mapped\n                to the \"rest\" class.\n\n        Returns:\n            codebook : ndarray of shape (n_estimators, n_classes)\n                Efficient n-ary codebook.\n        \"\"\"\n        n_classes_in_codebook = min(n_classes, alphabet_size - 1)\n        n_classes_remaining = n_classes - n_classes_in_codebook\n\n        # Initialize the codebook with zeros\n        codebook = np.zeros((n_estimators, n_classes), dtype=int)\n\n        def generate_codeword():\n            choices = list(range(0, n_classes_in_codebook)) + [\n                n_classes_in_codebook for _ in range(n_classes_remaining)\n            ]\n            return np.random.permutation(\n                choices,\n            )\n\n        # Generate the first codeword randomly\n        codebook[0] = generate_codeword()\n\n        # Generate the remaining codewords\n        for i in range(1, n_estimators):\n            # Initialize the current codeword with random values\n            current_codeword = generate_codeword()\n            max_distance, n_min_distance_min, max_distance_codeword = (\n                0,\n                1000,\n                current_codeword,\n            )\n\n            # Iteratively improve the current codeword\n            for _ in range(1000):  # Number of iterations can be adjusted\n                # Compute the Hamming distances between the current codeword and all previous codewords\n                distances = np.sum(\n                    (codebook[:i] == n_classes_in_codebook)\n                    != (current_codeword == n_classes_in_codebook),\n                    axis=0,\n                )\n\n                # Find the minimum Hamming distance\n                min_distance = np.min(distances, axis=0)\n\n                n_min_distance = np.sum(distances == min_distance)\n\n                # print(min_distance, n_min_distance)\n\n                if min_distance &gt; max_distance or (\n                    min_distance == max_distance and n_min_distance &lt; n_min_distance_min\n                ):\n                    max_distance_codeword = current_codeword\n                    max_distance = min_distance\n                    n_min_distance_min = n_min_distance\n\n                current_codeword = generate_codeword()\n\n            # Assign the optimized codeword to the codebook\n            codebook[i] = max_distance_codeword\n\n        return codebook\n\n    def fit(self, X, y, **fit_params):\n        \"\"\"Fit underlying estimators.\n\n        Parameters:\n            X : {array-like, sparse matrix} of shape (n_samples, n_features)\n                Data.\n\n            y : array-like of shape (n_samples,)\n                Multi-class targets.\n\n            **fit_params : dict\n                Parameters passed to the ``estimator.fit`` method of each\n                sub-estimator.\n\n        Returns:\n            self : object\n                Returns a fitted instance of self.\n        \"\"\"\n        self.estimator.init_model_and_get_model_config()\n\n        y = self._validate_data(X=\"no_validation\", y=y)\n\n        random_state = check_random_state(self.random_state)\n        check_classification_targets(y)\n\n        self.classes_ = np.unique(y)\n        n_classes = self.classes_.shape[0]\n        if n_classes == 0:\n            raise ValueError(\n                \"DeciaryOutputCodeClassifier can not be fit when no class is present.\"\n            )\n\n        self.no_mapping_needed_ = False\n        if n_classes &lt;= self.get_alphabet_size():\n            self.no_mapping_needed_ = True\n            return self.estimator.fit(X, y, **fit_params)\n\n        # Generate efficient deciary codebook\n        self.code_book_ = self._generate_codebook(\n            n_classes,\n            self.get_n_estimators(n_classes),\n            alphabet_size=self.get_alphabet_size(),\n        )\n\n        self.classes_index_ = {c: i for i, c in enumerate(self.classes_)}\n\n        self.Y_train = np.array(\n            [self.code_book_[:, y[i]] for i in range(_num_samples(y))],\n            dtype=int,\n        )\n        self.X_train = X\n\n        # print('CLF DIST IN', list(zip(np.unique(y), (np.unique(y, return_counts=True)[1] / y.shape[0]).tolist())))\n\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Predict probabilities using the underlying estimators.\n\n        Parameters:\n            X : {array-like, sparse matrix} of shape (n_samples, n_features)\n                Data.\n\n        Returns:\n            p : ndarray of shape (n_samples, n_classes)\n                Returns the probability of the samples for each class in the model,\n                where classes are ordered as they are in `self.classes_`.\n        \"\"\"\n        check_is_fitted(self)\n\n        if self.no_mapping_needed_:\n            return self.estimator.predict_proba(X)\n\n        Y = np.array(\n            [\n                _fit_and_predict_proba(\n                    self.estimator, self.X_train, self.Y_train[:, i], X\n                )\n                for i in range(self.code_book_.shape[0])\n            ],\n            dtype=np.float64,\n        )\n\n        # Y shape is (n_estimators, n_samples, n_classes_per_code)\n        n_estimators, n_samples, n_classes_per_code = Y.shape\n        rest_class = np.max(self.code_book_)\n\n        # Compute the weighted probabilities for each class\n        probabilities = np.zeros((n_samples, self.classes_.shape[0]))\n\n        for i in range(n_estimators):\n            for j in range(self.classes_.shape[0]):\n                if self.code_book_[i, j] != rest_class:\n                    j_remapped = self.code_book_[i, j]\n                    probabilities[:, j] += Y[\n                        i, :, j_remapped\n                    ]  # / (1 - Y[i, :, rest_class])\n                    # print(j, Y[i, :, j_remapped])\n\n        assert not (\n            (self.code_book_ != rest_class).sum(0) == 0\n        ).any(), f\"Some classes are not mapped to any codeword. {self.code_book_} {self.classes_} {((self.code_book_ != rest_class).sum(0) == 0)}\"\n\n        # Normalize the weighted probabilities to get the final class probabilities\n        probabilities /= (self.code_book_ != rest_class).sum(0)\n        probabilities /= probabilities.sum(axis=1, keepdims=True)\n\n        return probabilities\n\n    def set_categorical_features(self, categorical_features):\n        self.categorical_features = categorical_features\n</code></pre>"},{"location":"api_reference/#scripts.estimator.ManyClassClassifier.fit","title":"fit","text":"<pre><code>fit(X, y, **fit_params)\n</code></pre> <p>Fit underlying estimators.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>{array-like, sparse matrix} of shape (n_samples, n_features) Data.</p> required <code>y</code> <p>array-like of shape (n_samples,) Multi-class targets.</p> required <code>**fit_params</code> <p>dict Parameters passed to the <code>estimator.fit</code> method of each sub-estimator.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <p>object Returns a fitted instance of self.</p> Source code in <code>scripts/estimator/many_class_classifier.py</code> <pre><code>def fit(self, X, y, **fit_params):\n    \"\"\"Fit underlying estimators.\n\n    Parameters:\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data.\n\n        y : array-like of shape (n_samples,)\n            Multi-class targets.\n\n        **fit_params : dict\n            Parameters passed to the ``estimator.fit`` method of each\n            sub-estimator.\n\n    Returns:\n        self : object\n            Returns a fitted instance of self.\n    \"\"\"\n    self.estimator.init_model_and_get_model_config()\n\n    y = self._validate_data(X=\"no_validation\", y=y)\n\n    random_state = check_random_state(self.random_state)\n    check_classification_targets(y)\n\n    self.classes_ = np.unique(y)\n    n_classes = self.classes_.shape[0]\n    if n_classes == 0:\n        raise ValueError(\n            \"DeciaryOutputCodeClassifier can not be fit when no class is present.\"\n        )\n\n    self.no_mapping_needed_ = False\n    if n_classes &lt;= self.get_alphabet_size():\n        self.no_mapping_needed_ = True\n        return self.estimator.fit(X, y, **fit_params)\n\n    # Generate efficient deciary codebook\n    self.code_book_ = self._generate_codebook(\n        n_classes,\n        self.get_n_estimators(n_classes),\n        alphabet_size=self.get_alphabet_size(),\n    )\n\n    self.classes_index_ = {c: i for i, c in enumerate(self.classes_)}\n\n    self.Y_train = np.array(\n        [self.code_book_[:, y[i]] for i in range(_num_samples(y))],\n        dtype=int,\n    )\n    self.X_train = X\n\n    # print('CLF DIST IN', list(zip(np.unique(y), (np.unique(y, return_counts=True)[1] / y.shape[0]).tolist())))\n\n    return self\n</code></pre>"},{"location":"api_reference/#scripts.estimator.ManyClassClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict probabilities using the underlying estimators.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>{array-like, sparse matrix} of shape (n_samples, n_features) Data.</p> required <p>Returns:</p> Name Type Description <code>p</code> <p>ndarray of shape (n_samples, n_classes) Returns the probability of the samples for each class in the model, where classes are ordered as they are in <code>self.classes_</code>.</p> Source code in <code>scripts/estimator/many_class_classifier.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict probabilities using the underlying estimators.\n\n    Parameters:\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data.\n\n    Returns:\n        p : ndarray of shape (n_samples, n_classes)\n            Returns the probability of the samples for each class in the model,\n            where classes are ordered as they are in `self.classes_`.\n    \"\"\"\n    check_is_fitted(self)\n\n    if self.no_mapping_needed_:\n        return self.estimator.predict_proba(X)\n\n    Y = np.array(\n        [\n            _fit_and_predict_proba(\n                self.estimator, self.X_train, self.Y_train[:, i], X\n            )\n            for i in range(self.code_book_.shape[0])\n        ],\n        dtype=np.float64,\n    )\n\n    # Y shape is (n_estimators, n_samples, n_classes_per_code)\n    n_estimators, n_samples, n_classes_per_code = Y.shape\n    rest_class = np.max(self.code_book_)\n\n    # Compute the weighted probabilities for each class\n    probabilities = np.zeros((n_samples, self.classes_.shape[0]))\n\n    for i in range(n_estimators):\n        for j in range(self.classes_.shape[0]):\n            if self.code_book_[i, j] != rest_class:\n                j_remapped = self.code_book_[i, j]\n                probabilities[:, j] += Y[\n                    i, :, j_remapped\n                ]  # / (1 - Y[i, :, rest_class])\n                # print(j, Y[i, :, j_remapped])\n\n    assert not (\n        (self.code_book_ != rest_class).sum(0) == 0\n    ).any(), f\"Some classes are not mapped to any codeword. {self.code_book_} {self.classes_} {((self.code_book_ != rest_class).sum(0) == 0)}\"\n\n    # Normalize the weighted probabilities to get the final class probabilities\n    probabilities /= (self.code_book_ != rest_class).sum(0)\n    probabilities /= probabilities.sum(axis=1, keepdims=True)\n\n    return probabilities\n</code></pre>"},{"location":"api_reference/#scripts.estimator.ClassifierAsRegressor","title":"ClassifierAsRegressor","text":"<p>             Bases: <code>RegressorMixin</code></p> <p>Wrapper class to use a classifier as a regressor.</p> <p>This class takes a classifier estimator and converts it into a regressor by encoding the target labels and treating the regression problem as a classification task.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>object Classifier estimator to be used as a regressor.</p> required <p>Attributes:</p> Name Type Description <code>label_encoder_</code> <p>LabelEncoder Label encoder used to transform target regression labels to classes.</p> <code>y_train_</code> <p>array-like of shape (n_samples,) Transformed target labels used for training.</p> <code>categorical_features</code> <p>list List of categorical feature indices.</p> <p>from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from tabpfn.scripts.estimator import ManyClassClassifier, TabPFNClassifier, ClassifierAsRegressor x, y = load_diabetes(return_X_y=True) x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42) clf = TabPFNClassifier() clf = ManyClassClassifier(clf, n_estimators=10, alphabet_size=clf.max_num_classes_) reg = ClassifierAsRegressor(clf) reg.fit(x_train, y_train) y_pred = reg.predict(x_test)</p> Source code in <code>scripts/estimator/classifier_as_regressor.py</code> <pre><code>class ClassifierAsRegressor(RegressorMixin):\n    \"\"\"Wrapper class to use a classifier as a regressor.\n\n    This class takes a classifier estimator and converts it into a regressor by\n    encoding the target labels and treating the regression problem as a\n    classification task.\n\n    Parameters:\n        estimator : object\n            Classifier estimator to be used as a regressor.\n\n    Attributes:\n        label_encoder_ : LabelEncoder\n            Label encoder used to transform target regression labels to classes.\n        y_train_ : array-like of shape (n_samples,)\n            Transformed target labels used for training.\n        categorical_features : list\n            List of categorical feature indices.\n\n    Examples:\n    &gt;&gt;&gt; from sklearn.datasets import load_diabetes\n    &gt;&gt;&gt; from sklearn.model_selection import train_test_split\n    &gt;&gt;&gt; from tabpfn.scripts.estimator import ManyClassClassifier, TabPFNClassifier, ClassifierAsRegressor\n    &gt;&gt;&gt; x, y = load_diabetes(return_X_y=True)\n    &gt;&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n    &gt;&gt;&gt; clf = TabPFNClassifier()\n    &gt;&gt;&gt; clf = ManyClassClassifier(clf, n_estimators=10, alphabet_size=clf.max_num_classes_)\n    &gt;&gt;&gt; reg = ClassifierAsRegressor(clf)\n    &gt;&gt;&gt; reg.fit(x_train, y_train)\n    &gt;&gt;&gt; y_pred = reg.predict(x_test)\n    \"\"\"\n\n    def __init__(self, estimator):\n        assert hasattr(\n            estimator, \"predict_proba\"\n        ), \"The estimator must have a predict_proba method to be used as a regressor.\"\n        self.estimator = estimator\n\n    def fit(self, X, y):\n        \"\"\"Fit the classifier as a regressor.\n\n        Parameters:\n            X : array-like of shape (n_samples, n_features)\n                Training data.\n            y : array-like of shape (n_samples,)\n                Target labels.\n\n        Returns:\n            self : object\n                Fitted estimator.\n        \"\"\"\n        self.label_encoder_ = LabelEncoder()\n        y_transformed = self.label_encoder_.fit_transform(y)\n        self.y_train_ = y_transformed\n\n        return self.estimator.fit(X, y_transformed)\n\n    def _get_bar_dist(self):\n        \"\"\"Get the bar distribution for the target labels.\n\n        Returns:\n            BarDistribution\n                Bar distribution for the target labels.\n        \"\"\"\n        unique_y = self.label_encoder_.classes_\n        bucket_widths = unique_y[1:] - unique_y[:-1]\n\n        borders = (\n            unique_y - (np.array([bucket_widths[0]] + bucket_widths.tolist()) / 2)\n        ).tolist() + [unique_y[-1] + bucket_widths[-1] / 2]\n        return BarDistribution(borders=torch.tensor(borders).float())\n\n    def predict(self, X):\n        \"\"\"Predict the target values for the input data.\n\n        Parameters:\n            X : array-like of shape (n_samples, n_features)\n                Input data.\n\n        Returns:\n            y_pred : array-like of shape (n_samples,)\n                Predicted target values.\n        \"\"\"\n        return self.predict_full(X)[\"mean\"]\n\n    def set_categorical_features(self, categorical_features):\n        \"\"\"Set the categorical feature indices.\n\n        Parameters:\n            categorical_features : list\n                List of categorical feature indices.\n        \"\"\"\n        self.categorical_features = categorical_features\n\n    def get_optimization_mode(self):\n        \"\"\"Get the optimization mode for the regressor.\n\n        Returns:\n            str\n                Optimization mode (\"mean\").\n        \"\"\"\n        return \"mean\"\n\n    @staticmethod\n    def probabilities_to_logits_multiclass(probabilities, eps=1e-6):\n        \"\"\"Convert probabilities to logits for a multi-class problem.\n\n        Parameters:\n            probabilities : array-like of shape (n_samples, n_classes)\n                Input probabilities for each class.\n            eps : float, default=1e-6\n                Small value to avoid division by zero or taking logarithm of zero.\n\n        Returns:\n            logits : array-like of shape (n_samples, n_classes)\n                Output logits for each class.\n        \"\"\"\n        probabilities = np.clip(probabilities, eps, 1 - eps)\n        logits = np.log(probabilities) - np.log(\n            1 - probabilities.sum(axis=1, keepdims=True) + eps\n        )\n        return logits\n\n    def predict_full(self, X):\n        \"\"\"Predict the full set of output values for the input data.\n\n        Parameters:\n            X : array-like of shape (n_samples, n_features)\n                Input data.\n\n        Returns:\n            dict\n                Dictionary containing the predicted output values, including:\n                - \"mean\": Predicted mean values.\n                - \"median\": Predicted median values.\n                - \"mode\": Predicted mode values.\n                - \"logits\": Predicted logits.\n                - \"buckets\": Predicted bucket probabilities.\n                - \"quantile_{q:.2f}\": Predicted quantile values for each quantile q.\n        \"\"\"\n        class_probs = self.estimator.predict_proba(X)\n        class_logits = (\n            torch.tensor(\n                ClassifierAsRegressor.probabilities_to_logits_multiclass(class_probs)\n            )\n            .float()\n            .unsqueeze(0)\n        )\n        bar_dist = self._get_bar_dist()\n        r = TabPFNRegressor._post_process_predict_full(\n            prediction=class_logits, criterion=bar_dist\n        )\n\n        return r\n</code></pre>"},{"location":"api_reference/#scripts.estimator.ClassifierAsRegressor.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre> <p>Fit the classifier as a regressor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Training data.</p> required <code>y</code> <p>array-like of shape (n_samples,) Target labels.</p> required <p>Returns:</p> Name Type Description <code>self</code> <p>object Fitted estimator.</p> Source code in <code>scripts/estimator/classifier_as_regressor.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the classifier as a regressor.\n\n    Parameters:\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n        y : array-like of shape (n_samples,)\n            Target labels.\n\n    Returns:\n        self : object\n            Fitted estimator.\n    \"\"\"\n    self.label_encoder_ = LabelEncoder()\n    y_transformed = self.label_encoder_.fit_transform(y)\n    self.y_train_ = y_transformed\n\n    return self.estimator.fit(X, y_transformed)\n</code></pre>"},{"location":"api_reference/#scripts.estimator.ClassifierAsRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict the target values for the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Input data.</p> required <p>Returns:</p> Name Type Description <code>y_pred</code> <p>array-like of shape (n_samples,) Predicted target values.</p> Source code in <code>scripts/estimator/classifier_as_regressor.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict the target values for the input data.\n\n    Parameters:\n        X : array-like of shape (n_samples, n_features)\n            Input data.\n\n    Returns:\n        y_pred : array-like of shape (n_samples,)\n            Predicted target values.\n    \"\"\"\n    return self.predict_full(X)[\"mean\"]\n</code></pre>"},{"location":"cheat_sheet/","title":"Cheat Sheet / Best practices","text":"<p>Look at Autogluon cheat sheet [https://auto.gluon.ai/stable/cheatsheet.html]</p>"},{"location":"classification/","title":"Classification","text":"<p>TabPFN provides a powerful interface for handling classification tasks on tabular data. The <code>TabPFNClassifier</code> class can be used for binary and multi-class classification problems.</p>"},{"location":"classification/#example","title":"Example","text":"<p>Below is an example of how to use <code>TabPFNClassifier</code> for a multi-class classification task:</p> <pre><code>from tabpfn import TabPFNClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train classifier\nclassifier = TabPFNClassifier(device='cuda', N_ensemble_configurations=10)\nclassifier.fit(X_train, y_train)\n\n# Evaluate\ny_pred = classifier.predict(X_test)\nprint('Test Accuracy:', accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"contribute/","title":"Contribute","text":"<p>Put out project that people could contribute to and provide instructions for contributing</p>"},{"location":"example_competitions/","title":"Cheat Sheet / Best practices","text":"<p>Look at Autogluon</p>"},{"location":"getting_started/","title":"Installation","text":"<p>To install TabPFN, please use the notebooks provided in this review. After review we are going to release our models as a python package on the python repository pypi.</p>"},{"location":"getting_started/#example","title":"Example","text":"<p>A simple way to get started with TabPFN using our sklearn interface is demonstrated below. This example shows how to train a classifier on the breast cancer dataset and evaluate its accuracy.</p> <p><pre><code>from sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nfrom tabpfn import TabPFNClassifier\n\n# Load data\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Initialize classifier\nclassifier = TabPFNClassifier(device='cpu', N_ensemble_configurations=32)\n\n# Train classifier\nclassifier.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = classifier.predict(X_test)\nprint('Accuracy:', accuracy_score(y_test, y_pred))\n</code></pre> This example demonstrates the basic workflow of training and predicting with TabPFN models. For more advanced usage, including handling of categorical data, please refer to the Advanced Usage section.</p>"},{"location":"regression/","title":"Regression","text":"<p>TabPFN can also be applied to regression tasks using the <code>TabPFNRegressor</code> class. This allows for predictive modeling of continuous outcomes.</p>"},{"location":"regression/#example","title":"Example","text":"<p>An example usage of <code>TabPFNRegressor</code> is shown below:</p> <p><pre><code>from tabpfn import TabPFNRegressor\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load Boston housing dataset\nX, y = load_boston(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train regressor\nregressor = TabPFNRegressor(device='cuda', N_ensemble_configurations=10)\nregressor.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = regressor.predict(X_test)\nprint('Test RMSE:', mean_squared_error(y_test, y_pred, squared=False))\n</code></pre> This example demonstrates how to train and evaluate a regression model. For more details on TabPFNRegressor and its parameters, refer to the API Reference section.</p>"},{"location":"release_notes/","title":"Release notes","text":"<pre><code># Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [1.0.0] - 2017-06-20\n</code></pre>"},{"location":"api/model.encoders/","title":"Encoders","text":""},{"location":"api/model.encoders/#model.encoders.InputEncoder","title":"InputEncoder","text":"<p>             Bases: <code>Module</code></p> <p>Base class for input encoders.</p> <p>All input encoders should subclass this class and implement the <code>forward</code> method.</p> Source code in <code>model/encoders.py</code> <pre><code>class InputEncoder(nn.Module):\n    \"\"\"Base class for input encoders.\n\n    All input encoders should subclass this class and implement the `forward` method.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        single_eval_pos: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode the input tensor.\n\n        Parameters:\n            x (torch.Tensor): The input tensor to encode.\n            single_eval_pos (int): The position to use for single evaluation.\n\n        Returns:\n            torch.Tensor: The encoded tensor.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.InputEncoder.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> Source code in <code>model/encoders.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.InputEncoder.forward","title":"forward","text":"<pre><code>forward(x: Tensor, single_eval_pos: int) -&gt; Tensor\n</code></pre> <p>Encode the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to encode.</p> required <code>single_eval_pos</code> <code>int</code> <p>The position to use for single evaluation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The encoded tensor.</p> Source code in <code>model/encoders.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    single_eval_pos: int,\n) -&gt; torch.Tensor:\n    \"\"\"Encode the input tensor.\n\n    Parameters:\n        x (torch.Tensor): The input tensor to encode.\n        single_eval_pos (int): The position to use for single evaluation.\n\n    Returns:\n        torch.Tensor: The encoded tensor.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.SequentialEncoder","title":"SequentialEncoder","text":"<p>             Bases: <code>Sequential</code>, <code>InputEncoder</code></p> <p>An encoder that applies a sequence of encoder steps.</p> <p>SequentialEncoder allows building an encoder from a sequence of EncoderSteps. The input is passed through each step in the provided order.</p> Source code in <code>model/encoders.py</code> <pre><code>class SequentialEncoder(torch.nn.Sequential, InputEncoder):\n    \"\"\"An encoder that applies a sequence of encoder steps.\n\n    SequentialEncoder allows building an encoder from a sequence of EncoderSteps.\n    The input is passed through each step in the provided order.\n    \"\"\"\n\n    def __init__(self, *args, output_key: str = \"output\", **kwargs):\n        \"\"\"Initialize the SequentialEncoder.\n\n        Parameters:\n            *args: A list of SeqEncStep instances to apply in order.\n            output_key (str): The key to use for the output of the encoder in the state dict.\n                              Defaults to \"output\", i.e. `state[\"output\"]` will be returned.\n            **kwargs: Additional keyword arguments passed to `torch.nn.Sequential`.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.output_key = output_key\n\n    def forward(self, input: dict, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Apply the sequence of encoder steps to the input.\n\n        Parameters:\n            input (dict): The input state dictionary.\n                          If the input is not a dict and the first layer expects one input key,\n                          the input tensor is mapped to the key expected by the first layer.\n            **kwargs: Additional keyword arguments passed to each encoder step.\n\n        Returns:\n            torch.Tensor: The output of the final encoder step.\n        \"\"\"\n        if type(input) != dict:\n            # If the input is not a dict and the first layer expects one input, mapping the\n            #   input to the first input key must be correct\n            if len(self[0].in_keys) == 1:\n                input = {self[0].in_keys[0]: input}\n\n        for module in self:\n            input = module(input, **kwargs)\n\n        return input[self.output_key] if self.output_key is not None else input\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.SequentialEncoder.__init__","title":"__init__","text":"<pre><code>__init__(*args, output_key: str = 'output', **kwargs)\n</code></pre> <p>Initialize the SequentialEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>A list of SeqEncStep instances to apply in order.</p> <code>()</code> <code>output_key</code> <code>str</code> <p>The key to use for the output of the encoder in the state dict.               Defaults to \"output\", i.e. <code>state[\"output\"]</code> will be returned.</p> <code>'output'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>torch.nn.Sequential</code>.</p> <code>{}</code> Source code in <code>model/encoders.py</code> <pre><code>def __init__(self, *args, output_key: str = \"output\", **kwargs):\n    \"\"\"Initialize the SequentialEncoder.\n\n    Parameters:\n        *args: A list of SeqEncStep instances to apply in order.\n        output_key (str): The key to use for the output of the encoder in the state dict.\n                          Defaults to \"output\", i.e. `state[\"output\"]` will be returned.\n        **kwargs: Additional keyword arguments passed to `torch.nn.Sequential`.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.output_key = output_key\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.SequentialEncoder.forward","title":"forward","text":"<pre><code>forward(input: dict, **kwargs) -&gt; Tensor\n</code></pre> <p>Apply the sequence of encoder steps to the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>dict</code> <p>The input state dictionary.           If the input is not a dict and the first layer expects one input key,           the input tensor is mapped to the key expected by the first layer.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to each encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output of the final encoder step.</p> Source code in <code>model/encoders.py</code> <pre><code>def forward(self, input: dict, **kwargs) -&gt; torch.Tensor:\n    \"\"\"Apply the sequence of encoder steps to the input.\n\n    Parameters:\n        input (dict): The input state dictionary.\n                      If the input is not a dict and the first layer expects one input key,\n                      the input tensor is mapped to the key expected by the first layer.\n        **kwargs: Additional keyword arguments passed to each encoder step.\n\n    Returns:\n        torch.Tensor: The output of the final encoder step.\n    \"\"\"\n    if type(input) != dict:\n        # If the input is not a dict and the first layer expects one input, mapping the\n        #   input to the first input key must be correct\n        if len(self[0].in_keys) == 1:\n            input = {self[0].in_keys[0]: input}\n\n    for module in self:\n        input = module(input, **kwargs)\n\n    return input[self.output_key] if self.output_key is not None else input\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.LinearInputEncoderStep","title":"LinearInputEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>A simple linear input encoder step.</p> Source code in <code>model/encoders.py</code> <pre><code>class LinearInputEncoderStep(SeqEncStep):\n    \"\"\"A simple linear input encoder step.\"\"\"\n\n    def __init__(\n        self,\n        num_features: int,\n        emsize: int,\n        replace_nan_by_zero: bool = False,\n        bias: bool = True,\n        in_keys: tuple[str] = (\"main\",),\n        out_keys: tuple[str] = (\"output\",),\n    ):\n        \"\"\"Initialize the LinearInputEncoderStep.\n\n        Parameters:\n            num_features (int): The number of input features.\n            emsize (int): The embedding size, i.e. the number of output features.\n            replace_nan_by_zero (bool): Whether to replace NaN values in the input by zero. Defaults to False.\n            bias (bool): Whether to use a bias term in the linear layer. Defaults to True.\n            in_keys (tuple[str]): The keys of the input tensors. Defaults to (\"main\",).\n            out_keys (tuple[str]): The keys to assign the output tensors to. Defaults to (\"output\",).\n        \"\"\"\n        super().__init__(in_keys, out_keys)\n        self.layer = nn.Linear(num_features, emsize, bias=bias)\n        self.replace_nan_by_zero = replace_nan_by_zero\n\n    def _fit(self, *x, **kwargs):\n        \"\"\"Fit the encoder step. Does nothing for LinearInputEncoderStep.\"\"\"\n        pass\n\n    def _transform(self, *x, **kwargs):\n        \"\"\"Apply the linear transformation to the input.\n\n        Parameters:\n            *x: The input tensors to concatenate and transform.\n            **kwargs: Unused keyword arguments.\n\n        Returns:\n            A tuple containing the transformed tensor.\n        \"\"\"\n        x = torch.cat(x, dim=-1)\n        if self.replace_nan_by_zero:\n            x = torch.nan_to_num(x, nan=0.0)\n        return (self.layer(x),)\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.LinearInputEncoderStep.__init__","title":"__init__","text":"<pre><code>__init__(num_features: int, emsize: int, replace_nan_by_zero: bool = False, bias: bool = True, in_keys: tuple[str] = ('main'), out_keys: tuple[str] = ('output'))\n</code></pre> <p>Initialize the LinearInputEncoderStep.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>The number of input features.</p> required <code>emsize</code> <code>int</code> <p>The embedding size, i.e. the number of output features.</p> required <code>replace_nan_by_zero</code> <code>bool</code> <p>Whether to replace NaN values in the input by zero. Defaults to False.</p> <code>False</code> <code>bias</code> <code>bool</code> <p>Whether to use a bias term in the linear layer. Defaults to True.</p> <code>True</code> <code>in_keys</code> <code>tuple[str]</code> <p>The keys of the input tensors. Defaults to (\"main\",).</p> <code>('main')</code> <code>out_keys</code> <code>tuple[str]</code> <p>The keys to assign the output tensors to. Defaults to (\"output\",).</p> <code>('output')</code> Source code in <code>model/encoders.py</code> <pre><code>def __init__(\n    self,\n    num_features: int,\n    emsize: int,\n    replace_nan_by_zero: bool = False,\n    bias: bool = True,\n    in_keys: tuple[str] = (\"main\",),\n    out_keys: tuple[str] = (\"output\",),\n):\n    \"\"\"Initialize the LinearInputEncoderStep.\n\n    Parameters:\n        num_features (int): The number of input features.\n        emsize (int): The embedding size, i.e. the number of output features.\n        replace_nan_by_zero (bool): Whether to replace NaN values in the input by zero. Defaults to False.\n        bias (bool): Whether to use a bias term in the linear layer. Defaults to True.\n        in_keys (tuple[str]): The keys of the input tensors. Defaults to (\"main\",).\n        out_keys (tuple[str]): The keys to assign the output tensors to. Defaults to (\"output\",).\n    \"\"\"\n    super().__init__(in_keys, out_keys)\n    self.layer = nn.Linear(num_features, emsize, bias=bias)\n    self.replace_nan_by_zero = replace_nan_by_zero\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.LinearInputEncoderStep.forward","title":"forward","text":"<pre><code>forward(state: dict, cache_trainset_representation: bool = False, **kwargs)\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation.                                   Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The updated state dictionary with the output tensors assigned to the output keys.</p> Source code in <code>model/encoders.py</code> <pre><code>def forward(\n    self, state: dict, cache_trainset_representation: bool = False, **kwargs\n):\n    \"\"\"Perform the forward pass of the encoder step.\n\n    Parameters:\n        state (dict): The input state dictionary containing the input tensors.\n        cache_trainset_representation (bool): Whether to cache the training set representation.\n                                              Only supported for _fit and _transform (not _forward).\n        **kwargs: Additional keyword arguments passed to the encoder step.\n\n    Returns:\n        The updated state dictionary with the output tensors assigned to the output keys.\n    \"\"\"\n    try:\n        args = [state[in_key] for in_key in self.in_keys]\n    except KeyError:\n        raise KeyError(\n            f\"EncoderStep expected input keys {self.in_keys}, but got {list(state.keys())}\"\n        )\n\n    if hasattr(self, \"_fit\"):\n        if kwargs[\"single_eval_pos\"] or not cache_trainset_representation:\n            self._fit(*args, **kwargs)\n        out = self._transform(*args, **kwargs)\n    else:\n        assert (\n            not cache_trainset_representation\n        ), f\"cache_trainset_representation is not supported for _forward, as implemented in {self.__class__.__name__}\"\n        out = self._forward(*args, **kwargs)\n\n    assert (\n        type(out) == tuple\n    ), \"EncoderStep must return a tuple of values (can be size 1)\"\n    assert len(out) == len(\n        self.out_keys\n    ), f\"EncoderStep outputs don't match out_keys {len(out)} (out) != {len(self.out_keys)} (out_keys = {self.out_keys})\"\n\n    state.update({out_key: out[i] for i, out_key in enumerate(self.out_keys)})\n    return state\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.NanHandlingEncoderStep","title":"NanHandlingEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to handle NaN and infinite values in the input.</p> Source code in <code>model/encoders.py</code> <pre><code>class NanHandlingEncoderStep(SeqEncStep):\n    \"\"\"Encoder step to handle NaN and infinite values in the input.\"\"\"\n\n    nan_indicator = -2.0\n    inf_indicator = 2.0\n    neg_inf_indicator = 4.0\n\n    def __init__(\n        self,\n        keep_nans: bool = True,\n        in_keys: tuple[str] = (\"main\",),\n        out_keys: tuple[str] = (\"main\", \"nan_indicators\"),\n    ):\n        \"\"\"Initialize the NanHandlingEncoderStep.\n\n        Parameters:\n            keep_nans (bool): Whether to keep NaN values as separate indicators. Defaults to True.\n            in_keys (tuple[str]): The keys of the input tensors. Must be a single key.\n            out_keys (tuple[str]): The keys to assign the output tensors to.\n                                   Defaults to (\"main\", \"nan_indicators\").\n        \"\"\"\n        assert len(in_keys) == 1, \"NanHandlingEncoderStep expects a single input key\"\n        super().__init__(in_keys, out_keys)\n        self.keep_nans = keep_nans\n\n    def _fit(self, x: torch.Tensor, single_eval_pos: int, **kwargs):\n        \"\"\"Compute the feature means on the training set for replacing NaNs.\n\n        Parameters:\n            x (torch.Tensor): The input tensor.\n            single_eval_pos (int): The position to use for single evaluation.\n            **kwargs: Additional keyword arguments (unused).\n        \"\"\"\n        self.feature_means_ = torch_nanmean(\n            x[:single_eval_pos], axis=0, include_inf=True\n        )\n\n    def _transform(self, x: torch.Tensor, **kwargs):\n        \"\"\"Replace NaN and infinite values in the input tensor.\n\n        Parameters:\n            x (torch.Tensor): The input tensor.\n            **kwargs: Additional keyword arguments (unused).\n\n        Returns:\n            A tuple containing the transformed tensor and optionally the NaN indicators.\n        \"\"\"\n        nans_indicator = None\n        if self.keep_nans:\n            # TODO: There is a bug here: The values arriving here are already mapped to nan if they were inf before\n            nans_indicator = (\n                torch.isnan(x) * NanHandlingEncoderStep.nan_indicator\n                + torch.logical_and(torch.isinf(x), torch.sign(x) == 1)\n                * NanHandlingEncoderStep.inf_indicator\n                + torch.logical_and(torch.isinf(x), torch.sign(x) == -1)\n                * NanHandlingEncoderStep.neg_inf_indicator\n            ).to(x.dtype)\n\n        nan_mask = torch.logical_or(torch.isnan(x), torch.isinf(x))\n        # replace nans with the mean of the corresponding feature\n        x = x.clone()  # clone to avoid inplace operations\n        x[nan_mask] = self.feature_means_.unsqueeze(0).expand_as(x)[nan_mask]\n\n        return x, nans_indicator\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.NanHandlingEncoderStep.__init__","title":"__init__","text":"<pre><code>__init__(keep_nans: bool = True, in_keys: tuple[str] = ('main'), out_keys: tuple[str] = ('main', 'nan_indicators'))\n</code></pre> <p>Initialize the NanHandlingEncoderStep.</p> <p>Parameters:</p> Name Type Description Default <code>keep_nans</code> <code>bool</code> <p>Whether to keep NaN values as separate indicators. Defaults to True.</p> <code>True</code> <code>in_keys</code> <code>tuple[str]</code> <p>The keys of the input tensors. Must be a single key.</p> <code>('main')</code> <code>out_keys</code> <code>tuple[str]</code> <p>The keys to assign the output tensors to.                    Defaults to (\"main\", \"nan_indicators\").</p> <code>('main', 'nan_indicators')</code> Source code in <code>model/encoders.py</code> <pre><code>def __init__(\n    self,\n    keep_nans: bool = True,\n    in_keys: tuple[str] = (\"main\",),\n    out_keys: tuple[str] = (\"main\", \"nan_indicators\"),\n):\n    \"\"\"Initialize the NanHandlingEncoderStep.\n\n    Parameters:\n        keep_nans (bool): Whether to keep NaN values as separate indicators. Defaults to True.\n        in_keys (tuple[str]): The keys of the input tensors. Must be a single key.\n        out_keys (tuple[str]): The keys to assign the output tensors to.\n                               Defaults to (\"main\", \"nan_indicators\").\n    \"\"\"\n    assert len(in_keys) == 1, \"NanHandlingEncoderStep expects a single input key\"\n    super().__init__(in_keys, out_keys)\n    self.keep_nans = keep_nans\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.NanHandlingEncoderStep.forward","title":"forward","text":"<pre><code>forward(state: dict, cache_trainset_representation: bool = False, **kwargs)\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation.                                   Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The updated state dictionary with the output tensors assigned to the output keys.</p> Source code in <code>model/encoders.py</code> <pre><code>def forward(\n    self, state: dict, cache_trainset_representation: bool = False, **kwargs\n):\n    \"\"\"Perform the forward pass of the encoder step.\n\n    Parameters:\n        state (dict): The input state dictionary containing the input tensors.\n        cache_trainset_representation (bool): Whether to cache the training set representation.\n                                              Only supported for _fit and _transform (not _forward).\n        **kwargs: Additional keyword arguments passed to the encoder step.\n\n    Returns:\n        The updated state dictionary with the output tensors assigned to the output keys.\n    \"\"\"\n    try:\n        args = [state[in_key] for in_key in self.in_keys]\n    except KeyError:\n        raise KeyError(\n            f\"EncoderStep expected input keys {self.in_keys}, but got {list(state.keys())}\"\n        )\n\n    if hasattr(self, \"_fit\"):\n        if kwargs[\"single_eval_pos\"] or not cache_trainset_representation:\n            self._fit(*args, **kwargs)\n        out = self._transform(*args, **kwargs)\n    else:\n        assert (\n            not cache_trainset_representation\n        ), f\"cache_trainset_representation is not supported for _forward, as implemented in {self.__class__.__name__}\"\n        out = self._forward(*args, **kwargs)\n\n    assert (\n        type(out) == tuple\n    ), \"EncoderStep must return a tuple of values (can be size 1)\"\n    assert len(out) == len(\n        self.out_keys\n    ), f\"EncoderStep outputs don't match out_keys {len(out)} (out) != {len(self.out_keys)} (out_keys = {self.out_keys})\"\n\n    state.update({out_key: out[i] for i, out_key in enumerate(self.out_keys)})\n    return state\n</code></pre>"},{"location":"api/model.mlp/","title":"MLP","text":""},{"location":"api/model.mlp/#mlp_1","title":"MLP","text":""},{"location":"api/model.mlp/#model.mlp.MLP","title":"MLP","text":"<p>             Bases: <code>Module</code></p> <p>Multi-Layer Perceptron (MLP) module.</p> <p>This module consists of two linear layers with an activation function in between. It supports various configurations such as the hidden size, activation function, initializing the output to zero, and recomputing the forward pass during backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The input and output size of the MLP.</p> required <code>hidden_size</code> <code>int</code> <p>The size of the hidden layer.</p> required <code>activation</code> <code>Union[Activation, str]</code> <p>The activation function to use. Can be either an Activation enum or a string representing the activation name.</p> required <code>device</code> <code>device</code> <p>The device to use for the linear layers.</p> required <code>dtype</code> <code>dtype</code> <p>The data type to use for the linear layers.</p> required <code>initialize_output_to_zero</code> <code>bool</code> <p>Whether to initialize the output layer weights to zero. Default is False.</p> <code>False</code> <code>recompute</code> <code>bool</code> <p>Whether to recompute the forward pass during backpropagation. This can save memory but increase computation time. Default is False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>linear1</code> <code>Linear</code> <p>The first linear layer.</p> <code>linear2</code> <code>Linear</code> <p>The second linear layer.</p> <code>activation</code> <code>Activation</code> <p>The activation function to use.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Performs the forward pass of the MLP. - x (torch.Tensor): The input tensor. - add_input (bool): Whether to add the input to the output. Default is False. - allow_inplace (bool): Indicates that 'x' is not used after the call and its buffer     can be reused for the output. The operation is not guaranteed to be inplace.     Default is False. - save_peak_mem_factor (Optional[int]): If provided, enables a memory-saving technique     that reduces peak memory usage during the forward pass. This requires 'add_input'     and 'allow_inplace' to be True. See the documentation of the decorator     'support_save_peak_mem_factor' for details. Default is None.</p> Example <p>mlp = MLP(size=128, hidden_size=256, activation='gelu', device='cuda', dtype=torch.float32) x = torch.randn(32, 128, device='cuda', dtype=torch.float32) output = mlp(x)</p> Source code in <code>model/mlp.py</code> <pre><code>class MLP(torch.nn.Module):\n    \"\"\"\n    Multi-Layer Perceptron (MLP) module.\n\n    This module consists of two linear layers with an activation function in between.\n    It supports various configurations such as the hidden size, activation function,\n    initializing the output to zero, and recomputing the forward pass during backpropagation.\n\n    Args:\n        size (int): The input and output size of the MLP.\n        hidden_size (int): The size of the hidden layer.\n        activation (Union[Activation, str]): The activation function to use.\n            Can be either an Activation enum or a string representing the activation name.\n        device (torch.device): The device to use for the linear layers.\n        dtype (torch.dtype): The data type to use for the linear layers.\n        initialize_output_to_zero (bool): Whether to initialize the output layer weights to zero.\n            Default is False.\n        recompute (bool): Whether to recompute the forward pass during backpropagation.\n            This can save memory but increase computation time. Default is False.\n\n    Attributes:\n        linear1 (torch.nn.Linear): The first linear layer.\n        linear2 (torch.nn.Linear): The second linear layer.\n        activation (Activation): The activation function to use.\n\n    Methods:\n        forward(x, add_input=False, allow_inplace=False, save_peak_mem_factor=None):\n            Performs the forward pass of the MLP.\n            - x (torch.Tensor): The input tensor.\n            - add_input (bool): Whether to add the input to the output. Default is False.\n            - allow_inplace (bool): Indicates that 'x' is not used after the call and its buffer\n                can be reused for the output. The operation is not guaranteed to be inplace.\n                Default is False.\n            - save_peak_mem_factor (Optional[int]): If provided, enables a memory-saving technique\n                that reduces peak memory usage during the forward pass. This requires 'add_input'\n                and 'allow_inplace' to be True. See the documentation of the decorator\n                'support_save_peak_mem_factor' for details. Default is None.\n\n    Example:\n        &gt;&gt;&gt; mlp = MLP(size=128, hidden_size=256, activation='gelu', device='cuda', dtype=torch.float32)\n        &gt;&gt;&gt; x = torch.randn(32, 128, device='cuda', dtype=torch.float32)\n        &gt;&gt;&gt; output = mlp(x)\n    \"\"\"\n\n    linear1: torch.nn.Linear\n    linear2: torch.nn.Linear\n    activation: Activation\n\n    def __init__(\n        self,\n        size: int,\n        hidden_size: int,\n        activation: Union[Activation, str],\n        device,\n        dtype,\n        initialize_output_to_zero: bool = False,\n        recompute: bool = False,\n    ):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(\n            size, hidden_size, bias=False, device=device, dtype=dtype\n        )\n        self.linear2 = torch.nn.Linear(\n            hidden_size, size, bias=False, device=device, dtype=dtype\n        )\n        if isinstance(activation, str):\n            activation = Activation[activation.upper()]\n        self.activation = activation\n        if initialize_output_to_zero:\n            torch.nn.init.zeros_(self.linear2.weight)\n        if recompute:\n            self.forward = partial(checkpoint, self.forward, use_reentrant=False)\n\n    @support_save_peak_mem_factor\n    def _compute(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.linear1(x)\n        if self.activation is Activation.GELU:\n            x = torch.nn.functional.gelu(x)\n        elif self.activation is Activation.RELU:\n            x = torch.nn.functional.relu(x)\n        else:\n            raise NotImplementedError(\n                f\"Activation Function {self.activation} is not implemented.\"\n            )\n        x = self.linear2(x)\n        return x\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        add_input: bool = False,\n        allow_inplace: bool = False,\n        save_peak_mem_factor: Optional[int] = None,\n    ) -&gt; torch.Tensor:\n        input_shape = x.shape\n        x = x.reshape(-1, x.size(-1))\n        x = self._compute(\n            x,\n            add_input=add_input,\n            allow_inplace=allow_inplace,\n            save_peak_mem_factor=save_peak_mem_factor,\n        )\n        x = x.reshape(input_shape)\n        return x\n</code></pre>"},{"location":"api/model.mlp/#model.mlp.MLP.__init__","title":"__init__","text":"<pre><code>__init__(size: int, hidden_size: int, activation: Union[Activation, str], device, dtype, initialize_output_to_zero: bool = False, recompute: bool = False)\n</code></pre> Source code in <code>model/mlp.py</code> <pre><code>def __init__(\n    self,\n    size: int,\n    hidden_size: int,\n    activation: Union[Activation, str],\n    device,\n    dtype,\n    initialize_output_to_zero: bool = False,\n    recompute: bool = False,\n):\n    super().__init__()\n    self.linear1 = torch.nn.Linear(\n        size, hidden_size, bias=False, device=device, dtype=dtype\n    )\n    self.linear2 = torch.nn.Linear(\n        hidden_size, size, bias=False, device=device, dtype=dtype\n    )\n    if isinstance(activation, str):\n        activation = Activation[activation.upper()]\n    self.activation = activation\n    if initialize_output_to_zero:\n        torch.nn.init.zeros_(self.linear2.weight)\n    if recompute:\n        self.forward = partial(checkpoint, self.forward, use_reentrant=False)\n</code></pre>"},{"location":"api/model.mlp/#model.mlp.MLP.forward","title":"forward","text":"<pre><code>forward(x: Tensor, add_input: bool = False, allow_inplace: bool = False, save_peak_mem_factor: Optional[int] = None) -&gt; Tensor\n</code></pre> Source code in <code>model/mlp.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    add_input: bool = False,\n    allow_inplace: bool = False,\n    save_peak_mem_factor: Optional[int] = None,\n) -&gt; torch.Tensor:\n    input_shape = x.shape\n    x = x.reshape(-1, x.size(-1))\n    x = self._compute(\n        x,\n        add_input=add_input,\n        allow_inplace=allow_inplace,\n        save_peak_mem_factor=save_peak_mem_factor,\n    )\n    x = x.reshape(input_shape)\n    return x\n</code></pre>"},{"location":"api/model.transformer/","title":"Transformer","text":""},{"location":"api/model.transformer/#model.transformer.PerFeatureTransformer","title":"PerFeatureTransformer","text":"<p>             Bases: <code>Module</code></p> <p>A Transformer model processes a token per feature and sample.</p> <p>This model extends the standard Transformer architecture to operate on a per-feature basis. It allows for processing each feature separately while still leveraging the power of self-attention.</p> <p>The model consists of an encoder, decoder, and optional components such as a feature positional embedding and a separate decoder for each feature.</p> Source code in <code>model/transformer.py</code> <pre><code>class PerFeatureTransformer(Module):\n    \"\"\"\n    A Transformer model processes a token per feature and sample.\n\n\n    This model extends the standard Transformer architecture to operate on a per-feature basis.\n    It allows for processing each feature separately while still leveraging the power of self-attention.\n\n    The model consists of an encoder, decoder, and optional components such as a feature positional\n    embedding and a separate decoder for each feature.\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: nn.Module = encoders.SequentialEncoder(\n            encoders.LinearInputEncoderStep(\n                1, DEFAULT_EMSIZE, in_keys=[\"main\"], out_keys=[\"output\"]\n            ),\n        ),\n        ninp: int = DEFAULT_EMSIZE,\n        nhead: int = 4,\n        nhid: int = DEFAULT_EMSIZE * 4,\n        nlayers: int = 10,\n        y_encoder: nn.Module = encoders.SequentialEncoder(\n            encoders.NanHandlingEncoderStep(),\n            encoders.LinearInputEncoderStep(\n                2,\n                DEFAULT_EMSIZE,\n                out_keys=[\"output\"],\n                in_keys=[\"main\", \"nan_indicators\"],\n            ),\n        ),\n        decoder_dict: Dict[str, Tuple[Optional[Type[nn.Module]], int]] = {\n            \"standard\": (None, 1)\n        },\n        init_method: Optional[str] = None,\n        activation: str = \"gelu\",\n        recompute_layer: bool = False,\n        min_num_layers_layer_dropout: Optional[int] = None,\n        repeat_same_layer: bool = False,\n        dag_pos_enc_dim: int = 0,\n        features_per_group: int = 1,\n        feature_positional_embedding: Optional[str] = None,\n        zero_init: bool = True,\n        use_separate_decoder: bool = False,\n        nlayers_decoder: Optional[int] = None,\n        use_encoder_compression_layer: bool = False,\n        precomputed_kv: Optional[\n            List[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]]\n        ] = None,\n        cache_trainset_representation: bool = False,\n        **layer_kwargs: Any,\n    ):\n        \"\"\"\n        Parameters:\n           encoder: Pass a nn.Module that takes in a batch of sequences of inputs and returns something of the shape (seq_len, batch_size, ninp)\n           ninp: Input dimension, also called the embedding dimension\n           nhead: Number of attention heads\n           nhid: Hidden dimension in the MLP layers\n           nlayers: Number of layers, each consisting of a multi-head attention layer and an MLP layer\n           y_encoder: A nn.Module that takes in a batch of sequences of outputs and returns something of the shape (seq_len, batch_size, ninp)\n           decoder_dict:\n           activation: An activation function, e.g. \"gelu\" or \"relu\"\n           recompute_layer: If True, the transformer layers will be recomputed on each forward pass in training. This is useful to save memory.\n           min_num_layers_layer_dropout: if this is set, it enables to drop the last layers randomly during training up to this number.\n           repeat_same_layer: If True, the same layer will be used for all layers. This is useful to save memory on weights.\n           features_per_group: If &gt; 1, the features will be grouped into groups of this size and the attention is across groups.\n           feature_positional_embedding: There is a risk that our models confuse features with each other. This positional embedding is added to the features to help the model distinguish them.\n             We recommend setting this to \"subspace\".\n           zero_init: If True, the last sublayer of each attention and MLP layer will be initialized with zeros.\n             Thus, the layers will start out as identity functions.\n           use_separate_decoder: If True, the decoder will be separate from the encoder.\n           nlayers_decoder: If use_separate_decoder is True, this is the number of layers in the decoder. The default is to use 1/3 of the layers for the decoder and 2/3 for the encoder.\n           use_encoder_compression_layer: Experimental\n           precomputed_kv: Experimental\n           layer_kwargs: TODO: document, for now have a look at layer.py:PerFeatureEncoderLayer\n        \"\"\"\n        super().__init__()\n        self.encoder = encoder\n        self.y_encoder = y_encoder\n        self.ninp = ninp\n        self.nhead = nhead\n        self.nhid = nhid\n        self.init_method = init_method\n        self.features_per_group = features_per_group\n        self.cache_trainset_representation = cache_trainset_representation\n\n        layer_creator = lambda: PerFeatureEncoderLayer(\n            ninp,\n            nhead,\n            nhid,\n            activation,\n            zero_init=zero_init,\n            precomputed_kv=precomputed_kv.pop(0)\n            if precomputed_kv is not None\n            else None,\n            **layer_kwargs,\n        )\n        if repeat_same_layer:\n            layer = layer_creator()\n            layer_creator = lambda: layer\n\n        nlayers_encoder = nlayers\n        if use_separate_decoder and nlayers_decoder is None:\n            nlayers_decoder = max((nlayers // 3) * 1, 1)\n            nlayers_encoder = max((nlayers // 3) * 2, 1)\n\n        self.transformer_encoder = LayerStack(\n            layer_creator,\n            nlayers_encoder,\n            recompute_each_layer=recompute_layer,\n            min_num_layers_layer_dropout=min_num_layers_layer_dropout,\n        )\n\n        self.transformer_decoder = None\n        if use_separate_decoder:\n            self.transformer_decoder = LayerStack(\n                layer_creator,\n                nlayers_decoder,\n            )\n\n        self.global_att_embeddings_for_compression = None\n        if use_encoder_compression_layer:\n            utils.print_once(\n                \"ATTENTION: the use_encoder_compression_layer setting is experimental and not tested.\"\n            )\n            assert use_separate_decoder\n\n            num_global_att_tokens_for_compression = 512\n\n            self.global_att_embeddings_for_compression = nn.Embedding(\n                num_global_att_tokens_for_compression, ninp\n            )\n\n            self.encoder_compression_layer = LayerStack(\n                layer_creator,\n                2,\n            )\n\n        self.decoder_dict = make_decoder_dict(decoder_dict, ninp, nhid)\n\n        self.feature_positional_embedding = feature_positional_embedding\n        if feature_positional_embedding == \"learned\":\n            self.feature_positional_embedding_embeddings = nn.Embedding(1_000, ninp)\n        elif feature_positional_embedding == \"subspace\":\n            self.feature_positional_embedding_embeddings = nn.Linear(ninp // 4, ninp)\n\n        self.dag_pos_enc_dim = dag_pos_enc_dim\n        self.cached_feature_positional_embeddings = None\n        self.seed = random.randint(0, 1_000_000)\n        self.generator_device = (\n            \"cpu\"  # Device on which the generator was last initialized.\n        )\n        # If loading from a checkpoint, this might be false, but it will be set to the correct\n        # device on the first forward pass.\n        self._init_rnd()\n\n    def _init_rnd(self):\n        self.generator = SerializableGenerator(device=self.generator_device)\n        if self.seed:  # This can be none if set outside of the model.\n            self.generator.manual_seed(self.seed)\n\n    def reset_save_peak_mem_factor(self, factor=None):\n        \"\"\"\n        Sets the save_peak_mem_factor for all layers.\n\n        This factor controls how much memory is saved during the forward pass in inference mode.\n        Setting this factor &gt; 1 will cause the model to save more memory during the forward pass in inference mode.\n        A value of 8 is good for a 4x larger width in the fully-connected layers.\n        And yields a situation were we need around 2 * num_features * num_items * emsize * 2 bytes of memory for a forward pass (using mixed precision).\n        WARNING: It should only be used with post-norm.\n\n        Parameters:\n            factor: The save_peak_mem_factor to set. Recommended value is 8.\n        \"\"\"\n        for layer in self.transformer_encoder.layers:\n            assert hasattr(\n                layer, \"save_peak_mem_factor\"\n            ), \"Layer does not have save_peak_mem_factor\"\n            layer.save_peak_mem_factor = factor\n\n    def __setstate__(self, state):\n        state.setdefault(\"features_per_group\", 1)\n        state.setdefault(\"feature_positional_embedding\", None)\n        super().__setstate__(state)\n\n    def forward(self, *args, **kwargs):\n        \"\"\"\n        Performs a forward pass through the model.\n\n        This method supports multiple calling conventions:\n        - model(train_x, train_y, test_x, **kwargs)\n        - model((x,y), **kwargs)\n        - model((style,x,y), **kwargs)\n\n        Parameters:\n            train_x: The input data for the training set.\n            train_y: The target data for the training set.\n            test_x: The input data for the test set.\n            x: The input data.\n            y: The target data.\n            style: The style vector.\n            single_eval_pos: The position to evaluate at.\n            only_return_standard_out: Whether to only return the standard output.\n            data_dags: The data DAGs for each example.\n            categorical_inds: The indices of categorical features.\n            freeze_kv: Whether to freeze the key and value weights.\n\n        Returns:\n            The output of the model, which can be a tensor or a dictionary of tensors.\n        \"\"\"\n        supported_kwargs = {\n            \"single_eval_pos\",\n            \"only_return_standard_out\",\n            \"style\",\n            \"data_dags\",\n            \"categorical_inds\",\n            \"freeze_kv\",\n        }\n        if \"half_layers\" in kwargs:\n            assert not kwargs[\"half_layers\"]\n            del kwargs[\"half_layers\"]\n\n        if \"train_x\" in kwargs:\n            assert len(args) == 0\n            args = [kwargs[\"train_x\"], kwargs[\"train_y\"], kwargs[\"test_x\"]]\n            del kwargs[\"train_x\"]\n            del kwargs[\"train_y\"]\n            del kwargs[\"test_x\"]\n\n        if len(args) == 3:\n            supported_kwargs.remove(\"single_eval_pos\")\n        spurious_kwargs = set(kwargs.keys()) - supported_kwargs\n        assert not spurious_kwargs, spurious_kwargs\n\n        if len(args) == 3:\n            # case model(train_x, train_y, test_x, src_mask=None, style=None, only_return_standard_out=True)\n            x = args[0]\n            if args[2] is not None:\n                x = torch.cat((x, args[2]), dim=0)\n            return self._forward(x, args[1], single_eval_pos=len(args[0]), **kwargs)\n        elif len(args) == 1 and isinstance(args, tuple):\n            # case model((x,y), src_mask=None, single_eval_pos=None, only_return_standard_out=True)\n            # case model((style,x,y), src_mask=None, single_eval_pos=None, only_return_standard_out=True)\n            if len(args[0]) == 3:\n                return self._forward(*args[0][1:], style=args[0][0], **kwargs)\n            else:\n                assert (\n                    len(args[0]) == 2\n                ), f\"Expected tuple of length 2 or 3, got {len(args[0])}\"\n                return self._forward(*args[0], **kwargs)\n        else:\n            raise ValueError(\n                \"Unrecognized input. Please follow the doc string exactly.\"\n            )\n\n    def _forward(\n        self,\n        x: torch.Tensor,\n        y: torch.Tensor,\n        single_eval_pos: Optional[int] = None,\n        only_return_standard_out: bool = True,\n        style: Optional[torch.Tensor] = None,\n        data_dags: Optional[List[Any]] = None,\n        categorical_inds: Optional[List[List[int]]] = None,\n        half_layers: bool = False,\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        The core forward pass of the model.\n\n        Parameters:\n            x: The input data. Shape: (seq_len, batch_size, num_features)\n            y: The target data. Shape: (seq_len, batch_size)\n            single_eval_pos: The position to evaluate at. If None, evaluate at all positions.\n            only_return_standard_out: Whether to only return the standard output.\n            style: The style vector.\n            data_dags: The data DAGs for each example in the batch.\n            categorical_inds: The indices of categorical features.\n            half_layers: Whether to use half the layers.\n\n        Returns:\n            A dictionary of output tensors.\n        \"\"\"\n        assert style is None\n        if self.cache_trainset_representation:\n            if not single_eval_pos:  # none or 0\n                assert y is None\n        else:\n            assert y is not None and single_eval_pos\n\n        single_eval_pos_ = single_eval_pos or 0\n        if isinstance(x, dict):\n            assert \"main\" in set(x.keys()), f\"Main must be in input keys: {x.keys()}.\"\n        else:\n            x = {\"main\": x}\n        seq_len, batch_size, num_features = x[\"main\"].shape\n\n        if y is None:\n            # TODO: check dtype.\n            y = torch.zeros(\n                0, batch_size, device=x[\"main\"].device, dtype=x[\"main\"].dtype\n            )\n\n        if isinstance(y, dict):\n            assert \"main\" in set(y.keys()), f\"Main must be in input keys: {y.keys()}.\"\n        else:\n            y = {\"main\": y}\n\n        # print(f\"missing_to_next: {missing_to_next}\", 'num_features', num_features, 'features_per_group', self.features_per_group)\n        for k in x:\n            num_features_ = x[k].shape[2]\n\n            # pad to multiple of features_per_group\n            missing_to_next = (\n                self.features_per_group - (num_features_ % self.features_per_group)\n            ) % self.features_per_group\n\n            if missing_to_next &gt; 0:\n                x[k] = torch.cat(\n                    (\n                        x[k],\n                        torch.zeros(\n                            seq_len,\n                            batch_size,\n                            missing_to_next,\n                            device=x[k].device,\n                            dtype=x[k].dtype,\n                        ),\n                    ),\n                    dim=-1,\n                )\n        for k in x:\n            # print('x.shape', x.shape)\n            x[k] = einops.rearrange(\n                x[k], \"s b (f n) -&gt; b s f n\", n=self.features_per_group\n            )  # s b f -&gt; b s #groups #features_per_group\n\n        if categorical_inds is not None:\n            new_categorical_inds = []\n            for ci in categorical_inds:\n                num_subgroups = x[\"main\"].shape[2]\n                new_categorical_inds += [\n                    [\n                        i - subgroup * self.features_per_group\n                        for i in ci\n                        if (\n                            subgroup * self.features_per_group\n                            &lt;= i\n                            &lt; (subgroup + 1) * self.features_per_group\n                        )\n                    ]\n                    for subgroup in range(num_subgroups)\n                ]\n            categorical_inds = new_categorical_inds\n\n        for k in y:\n            if len(y[k].shape) == 2:\n                y[k] = y[k].unsqueeze(-1)  # s b -&gt; s b 1\n\n            y[k] = y[k].transpose(0, 1)  # s b 1 -&gt; b s 1\n\n            if y[k].shape[1] &lt; x[\"main\"].shape[1]:\n                assert (\n                    y[k].shape[1] == single_eval_pos_\n                    or y[k].shape[1] == x[\"main\"].shape[1]\n                )\n                assert (\n                    k != \"main\" or y[k].shape[1] == single_eval_pos_\n                ), \"For main y, y must not be given for target time steps (Otherwise the solution is leaked).\"\n                if y[k].shape[1] == single_eval_pos_:\n                    y[k] = torch.cat(\n                        (\n                            y[k],\n                            torch.nan\n                            * torch.zeros(\n                                y[k].shape[0],\n                                x[\"main\"].shape[1] - y[k].shape[1],\n                                y[k].shape[2],\n                                device=y[k].device,\n                                dtype=y[k].dtype,\n                            ),\n                        ),\n                        dim=1,\n                    )\n\n            y[k] = y[k].transpose(0, 1)  # b s 1 -&gt; s b 1\n\n        # making sure no label leakage ever happens\n        y[\"main\"][single_eval_pos_:] = torch.nan\n\n        embedded_y = self.y_encoder(\n            y,\n            single_eval_pos=single_eval_pos_,\n            cache_trainset_representation=self.cache_trainset_representation,\n        ).transpose(0, 1)\n        # print('embedded y', embedded_y.shape, embedded_y)\n        del y\n        assert not torch.isnan(\n            embedded_y\n        ).any(), f\"{torch.isnan(embedded_y).any()=}, make sure to add nan handlers to the ys that are not fully provided (test set missing)\"\n\n        extra_encoders_args = {}\n        if categorical_inds is not None and isinstance(\n            self.encoder, encoders.SequentialEncoder\n        ):\n            extra_encoders_args[\"categorical_inds\"] = categorical_inds\n\n        for k in x:\n            x[k] = einops.rearrange(x[k], \"b s f n -&gt; s (b f) n\")\n\n        embedded_x = einops.rearrange(\n            self.encoder(\n                x,\n                single_eval_pos=single_eval_pos_,\n                cache_trainset_representation=self.cache_trainset_representation,\n                **extra_encoders_args,\n            ),\n            \"s (b f) e -&gt; b s f e\",\n            b=embedded_y.shape[0],\n        )  # b s f 1 -&gt; b s f e\n        del x\n\n        embedded_x, embedded_y = self.add_embeddings(\n            embedded_x,\n            embedded_y,\n            data_dags,\n            num_features,\n            seq_len,\n            cache_embeddings=self.cache_trainset_representation and single_eval_pos,\n            use_cached_embeddings=self.cache_trainset_representation\n            and not single_eval_pos,\n        )\n        del data_dags\n\n        embedded_input = torch.cat(\n            (embedded_x, embedded_y.unsqueeze(2)), dim=2\n        )  # b s f e + b s 1 e -&gt; b s f+1 e\n\n        assert not torch.isnan(\n            embedded_input\n        ).any(), f\"There should be no NaNs in the encoded x and y. Check that you do not feed NaNs or use a NaN-handling enocder. Your embedded x and y returned the following: {torch.isnan(embedded_x).any()=} and {torch.isnan(embedded_y).any()=}.\"\n        del embedded_y, embedded_x\n\n        # print(f\"{embedded_input[:, -1, 0, :10]=}\")\n        # print(f\"{embedded_input[:, -1, -1, :10]=}\")\n\n        encoder_out = self.transformer_encoder(\n            (\n                embedded_input\n                if not self.transformer_decoder\n                else embedded_input[:, :single_eval_pos_]\n            ),\n            single_eval_pos=single_eval_pos,\n            half_layers=half_layers,\n            cache_trainset_representation=self.cache_trainset_representation,\n        )  # b s f+1 e -&gt; b s f+1 e\n\n        # If we are using a decoder\n        if self.transformer_decoder:\n            print_once(\"Using separate decoder\")\n            assert not half_layers\n            assert encoder_out.shape[1] == single_eval_pos_\n\n            if self.global_att_embeddings_for_compression is not None:\n                # TODO: fixed number of compression tokens\n                train_encoder_out = self.encoder_compression_layer(\n                    self.global_att_embeddings_for_compression,\n                    att_src=encoder_out[:, single_eval_pos_],\n                    single_eval_pos=single_eval_pos_,\n                )\n\n            test_encoder_out = self.transformer_decoder(\n                embedded_input[:, single_eval_pos_:],\n                single_eval_pos=0,\n                att_src=encoder_out,\n            )\n            encoder_out = torch.cat([encoder_out, test_encoder_out], 1)\n            del test_encoder_out\n\n        del embedded_input\n\n        test_encoder_out = encoder_out[:, single_eval_pos_:, -1].transpose(\n            0, 1\n        )  # out: s b e\n\n        if only_return_standard_out:\n            output_decoded = self.decoder_dict[\"standard\"](test_encoder_out)\n        else:\n            output_decoded = (\n                {k: v(test_encoder_out) for k, v in self.decoder_dict.items()}\n                if self.decoder_dict is not None\n                else {}\n            )\n\n            train_encoder_out = encoder_out[:, :single_eval_pos_, -1].transpose(\n                0, 1\n            )  # out: s b e\n\n            output_decoded[\"train_embeddings\"] = train_encoder_out\n            output_decoded[\"test_embeddings\"] = test_encoder_out\n\n        return output_decoded\n\n    def add_embeddings(\n        self,\n        x,\n        y,\n        data_dags,\n        num_features,\n        seq_len,\n        cache_embeddings=False,\n        use_cached_embeddings=False,\n    ):\n        if use_cached_embeddings and self.cached_embeddings is not None:\n            assert (\n                data_dags is None\n            ), \"Caching embeddings is not supported with data_dags at this point.\"\n            x += self.cached_embeddings[None, None]\n            return x, y\n\n        if (\n            self.generator_device != self.generator.device\n            or self.generator_device != x.device\n        ):\n            self.generator_device = x.device\n            self._init_rnd()\n\n        if self.feature_positional_embedding == \"normal_rand_vec\":\n            embs = torch.randn(\n                (x.shape[2], x.shape[3]),\n                device=x.device,\n                dtype=x.dtype,\n                generator=self.generator,\n            )\n            x += embs[None, None]\n        elif self.feature_positional_embedding == \"uni_rand_vec\":\n            embs = (\n                torch.rand(\n                    (x.shape[2], x.shape[3]),\n                    device=x.device,\n                    dtype=x.dtype,\n                    generator=self.generator,\n                )\n                * 2\n                - 1\n            )\n            x += embs[None, None]\n        elif self.feature_positional_embedding == \"learned\":\n            w = self.feature_positional_embedding_embeddings.weight\n            embs = w[\n                torch.randint(0, w.shape[0], (x.shape[2],), generator=self.generator)\n            ]\n            x += embs[None, None]\n        elif self.feature_positional_embedding == \"subspace\":\n            embs = torch.randn(\n                (x.shape[2], x.shape[3] // 4),\n                device=x.device,\n                dtype=x.dtype,\n                generator=self.generator,\n            )\n            embs = self.feature_positional_embedding_embeddings(embs)\n            x += embs[None, None]\n        else:\n            assert self.feature_positional_embedding is None\n\n        if cache_embeddings and self.feature_positional_embedding is not None:\n            assert (\n                data_dags is None\n            ), \"Caching embeddings is not supported with data_dags at this point.\"\n            self.cached_embeddings = embs\n        else:\n            self.cached_embeddings = None\n\n        # TODO should this go into encoder?\n        # could also be made a bit more concise by moving down to operate on full_x\n        if data_dags is not None:\n            for b_i, data_dag in enumerate(data_dags):\n                g_ = data_dag.copy()  # type: nx.DiGraph\n                while utils.add_direct_connections(g_):\n                    pass\n                subgraph = g_.subgraph(\n                    [\n                        n\n                        for n, info in g_.nodes.items()\n                        if (info[\"is_feature\"] or info[\"is_target\"])\n                    ]\n                )\n                k = self.dag_pos_enc_dim\n                assert k\n                utils.add_pos_emb(subgraph, k=k)\n                graph_pos_embs_features = torch.zeros(\n                    (num_features, k)\n                )  # shape: (num_features, k)\n                graph_pos_embs_targets = torch.zeros((1, k))  # shape: (num_targets, k)\n                for node, node_info in subgraph.nodes.items():\n                    for feature_idx in node_info.get(\"feature_idxs\", []):\n                        graph_pos_embs_features[feature_idx] = node_info[\n                            \"positional_encoding\"\n                        ]\n                    for target_idx in node_info.get(\"target_idxs\", []):\n                        graph_pos_embs_targets[target_idx] = node_info[\n                            \"positional_encoding\"\n                        ]\n\n                # assert ((graph_pos_embs_features == -1000)[:-1].int() &lt;= (graph_pos_embs_features == -1000)[1:].int()).all(), graph_pos_embs_features.mean(1)\n\n                # print('n',torch.isnan(graph_pos_embs_features).any(), torch.isnan(graph_pos_embs_targets).any())\n                # print('o', torch.isnan(x).any(), torch.isnan(y).any())\n\n                graph_pos_embs_targets -= graph_pos_embs_features.mean(0, keepdim=True)\n                graph_pos_embs_features -= graph_pos_embs_features.mean(0, keepdim=True)\n\n                graph_pos_embs_features = graph_pos_embs_features[None].expand(\n                    seq_len, -1, -1\n                )\n                x[b_i, :, :, :k] += graph_pos_embs_features.to(y.device, y.dtype)\n\n                graph_pos_embs_targets = (\n                    graph_pos_embs_targets[None].expand(seq_len, -1, -1).squeeze(-2)\n                )\n                y[b_i, :, :k] += graph_pos_embs_targets.to(y.device, y.dtype)\n        else:\n            assert not hasattr(self, \"dag_pos_enc_dim\") or not self.dag_pos_enc_dim\n\n        return x, y\n\n    def empty_trainset_representation_cache(self):\n        for layer in (self.transformer_decoder or self.transformer_encoder).layers:\n            layer.empty_trainset_representation_cache()\n</code></pre>"},{"location":"api/model.transformer/#model.transformer.PerFeatureTransformer.__init__","title":"__init__","text":"<pre><code>__init__(encoder: Module = encoders.SequentialEncoder(encoders.LinearInputEncoderStep(1, DEFAULT_EMSIZE, in_keys=['main'], out_keys=['output'])), ninp: int = DEFAULT_EMSIZE, nhead: int = 4, nhid: int = DEFAULT_EMSIZE * 4, nlayers: int = 10, y_encoder: Module = encoders.SequentialEncoder(encoders.NanHandlingEncoderStep(), encoders.LinearInputEncoderStep(2, DEFAULT_EMSIZE, out_keys=['output'], in_keys=['main', 'nan_indicators'])), decoder_dict: Dict[str, Tuple[Optional[Type[Module]], int]] = {'standard': (None, 1)}, init_method: Optional[str] = None, activation: str = 'gelu', recompute_layer: bool = False, min_num_layers_layer_dropout: Optional[int] = None, repeat_same_layer: bool = False, dag_pos_enc_dim: int = 0, features_per_group: int = 1, feature_positional_embedding: Optional[str] = None, zero_init: bool = True, use_separate_decoder: bool = False, nlayers_decoder: Optional[int] = None, use_encoder_compression_layer: bool = False, precomputed_kv: Optional[List[Union[Tensor, Tuple[Tensor, Tensor]]]] = None, cache_trainset_representation: bool = False, **layer_kwargs: Any)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Module</code> <p>Pass a nn.Module that takes in a batch of sequences of inputs and returns something of the shape (seq_len, batch_size, ninp)</p> <code>SequentialEncoder(LinearInputEncoderStep(1, DEFAULT_EMSIZE, in_keys=['main'], out_keys=['output']))</code> <code>ninp</code> <code>int</code> <p>Input dimension, also called the embedding dimension</p> <code>DEFAULT_EMSIZE</code> <code>nhead</code> <code>int</code> <p>Number of attention heads</p> <code>4</code> <code>nhid</code> <code>int</code> <p>Hidden dimension in the MLP layers</p> <code>DEFAULT_EMSIZE * 4</code> <code>nlayers</code> <code>int</code> <p>Number of layers, each consisting of a multi-head attention layer and an MLP layer</p> <code>10</code> <code>y_encoder</code> <code>Module</code> <p>A nn.Module that takes in a batch of sequences of outputs and returns something of the shape (seq_len, batch_size, ninp)</p> <code>SequentialEncoder(NanHandlingEncoderStep(), LinearInputEncoderStep(2, DEFAULT_EMSIZE, out_keys=['output'], in_keys=['main', 'nan_indicators']))</code> <code>decoder_dict</code> <code>Dict[str, Tuple[Optional[Type[Module]], int]]</code> <code>{'standard': (None, 1)}</code> <code>activation</code> <code>str</code> <p>An activation function, e.g. \"gelu\" or \"relu\"</p> <code>'gelu'</code> <code>recompute_layer</code> <code>bool</code> <p>If True, the transformer layers will be recomputed on each forward pass in training. This is useful to save memory.</p> <code>False</code> <code>min_num_layers_layer_dropout</code> <code>Optional[int]</code> <p>if this is set, it enables to drop the last layers randomly during training up to this number.</p> <code>None</code> <code>repeat_same_layer</code> <code>bool</code> <p>If True, the same layer will be used for all layers. This is useful to save memory on weights.</p> <code>False</code> <code>features_per_group</code> <code>int</code> <p>If &gt; 1, the features will be grouped into groups of this size and the attention is across groups.</p> <code>1</code> <code>feature_positional_embedding</code> <code>Optional[str]</code> <p>There is a risk that our models confuse features with each other. This positional embedding is added to the features to help the model distinguish them. We recommend setting this to \"subspace\".</p> <code>None</code> <code>zero_init</code> <code>bool</code> <p>If True, the last sublayer of each attention and MLP layer will be initialized with zeros. Thus, the layers will start out as identity functions.</p> <code>True</code> <code>use_separate_decoder</code> <code>bool</code> <p>If True, the decoder will be separate from the encoder.</p> <code>False</code> <code>nlayers_decoder</code> <code>Optional[int]</code> <p>If use_separate_decoder is True, this is the number of layers in the decoder. The default is to use \u2153 of the layers for the decoder and \u2154 for the encoder.</p> <code>None</code> <code>use_encoder_compression_layer</code> <code>bool</code> <p>Experimental</p> <code>False</code> <code>precomputed_kv</code> <code>Optional[List[Union[Tensor, Tuple[Tensor, Tensor]]]]</code> <p>Experimental</p> <code>None</code> <code>layer_kwargs</code> <code>Any</code> <code>{}</code> Source code in <code>model/transformer.py</code> <pre><code>def __init__(\n    self,\n    encoder: nn.Module = encoders.SequentialEncoder(\n        encoders.LinearInputEncoderStep(\n            1, DEFAULT_EMSIZE, in_keys=[\"main\"], out_keys=[\"output\"]\n        ),\n    ),\n    ninp: int = DEFAULT_EMSIZE,\n    nhead: int = 4,\n    nhid: int = DEFAULT_EMSIZE * 4,\n    nlayers: int = 10,\n    y_encoder: nn.Module = encoders.SequentialEncoder(\n        encoders.NanHandlingEncoderStep(),\n        encoders.LinearInputEncoderStep(\n            2,\n            DEFAULT_EMSIZE,\n            out_keys=[\"output\"],\n            in_keys=[\"main\", \"nan_indicators\"],\n        ),\n    ),\n    decoder_dict: Dict[str, Tuple[Optional[Type[nn.Module]], int]] = {\n        \"standard\": (None, 1)\n    },\n    init_method: Optional[str] = None,\n    activation: str = \"gelu\",\n    recompute_layer: bool = False,\n    min_num_layers_layer_dropout: Optional[int] = None,\n    repeat_same_layer: bool = False,\n    dag_pos_enc_dim: int = 0,\n    features_per_group: int = 1,\n    feature_positional_embedding: Optional[str] = None,\n    zero_init: bool = True,\n    use_separate_decoder: bool = False,\n    nlayers_decoder: Optional[int] = None,\n    use_encoder_compression_layer: bool = False,\n    precomputed_kv: Optional[\n        List[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]]\n    ] = None,\n    cache_trainset_representation: bool = False,\n    **layer_kwargs: Any,\n):\n    \"\"\"\n    Parameters:\n       encoder: Pass a nn.Module that takes in a batch of sequences of inputs and returns something of the shape (seq_len, batch_size, ninp)\n       ninp: Input dimension, also called the embedding dimension\n       nhead: Number of attention heads\n       nhid: Hidden dimension in the MLP layers\n       nlayers: Number of layers, each consisting of a multi-head attention layer and an MLP layer\n       y_encoder: A nn.Module that takes in a batch of sequences of outputs and returns something of the shape (seq_len, batch_size, ninp)\n       decoder_dict:\n       activation: An activation function, e.g. \"gelu\" or \"relu\"\n       recompute_layer: If True, the transformer layers will be recomputed on each forward pass in training. This is useful to save memory.\n       min_num_layers_layer_dropout: if this is set, it enables to drop the last layers randomly during training up to this number.\n       repeat_same_layer: If True, the same layer will be used for all layers. This is useful to save memory on weights.\n       features_per_group: If &gt; 1, the features will be grouped into groups of this size and the attention is across groups.\n       feature_positional_embedding: There is a risk that our models confuse features with each other. This positional embedding is added to the features to help the model distinguish them.\n         We recommend setting this to \"subspace\".\n       zero_init: If True, the last sublayer of each attention and MLP layer will be initialized with zeros.\n         Thus, the layers will start out as identity functions.\n       use_separate_decoder: If True, the decoder will be separate from the encoder.\n       nlayers_decoder: If use_separate_decoder is True, this is the number of layers in the decoder. The default is to use 1/3 of the layers for the decoder and 2/3 for the encoder.\n       use_encoder_compression_layer: Experimental\n       precomputed_kv: Experimental\n       layer_kwargs: TODO: document, for now have a look at layer.py:PerFeatureEncoderLayer\n    \"\"\"\n    super().__init__()\n    self.encoder = encoder\n    self.y_encoder = y_encoder\n    self.ninp = ninp\n    self.nhead = nhead\n    self.nhid = nhid\n    self.init_method = init_method\n    self.features_per_group = features_per_group\n    self.cache_trainset_representation = cache_trainset_representation\n\n    layer_creator = lambda: PerFeatureEncoderLayer(\n        ninp,\n        nhead,\n        nhid,\n        activation,\n        zero_init=zero_init,\n        precomputed_kv=precomputed_kv.pop(0)\n        if precomputed_kv is not None\n        else None,\n        **layer_kwargs,\n    )\n    if repeat_same_layer:\n        layer = layer_creator()\n        layer_creator = lambda: layer\n\n    nlayers_encoder = nlayers\n    if use_separate_decoder and nlayers_decoder is None:\n        nlayers_decoder = max((nlayers // 3) * 1, 1)\n        nlayers_encoder = max((nlayers // 3) * 2, 1)\n\n    self.transformer_encoder = LayerStack(\n        layer_creator,\n        nlayers_encoder,\n        recompute_each_layer=recompute_layer,\n        min_num_layers_layer_dropout=min_num_layers_layer_dropout,\n    )\n\n    self.transformer_decoder = None\n    if use_separate_decoder:\n        self.transformer_decoder = LayerStack(\n            layer_creator,\n            nlayers_decoder,\n        )\n\n    self.global_att_embeddings_for_compression = None\n    if use_encoder_compression_layer:\n        utils.print_once(\n            \"ATTENTION: the use_encoder_compression_layer setting is experimental and not tested.\"\n        )\n        assert use_separate_decoder\n\n        num_global_att_tokens_for_compression = 512\n\n        self.global_att_embeddings_for_compression = nn.Embedding(\n            num_global_att_tokens_for_compression, ninp\n        )\n\n        self.encoder_compression_layer = LayerStack(\n            layer_creator,\n            2,\n        )\n\n    self.decoder_dict = make_decoder_dict(decoder_dict, ninp, nhid)\n\n    self.feature_positional_embedding = feature_positional_embedding\n    if feature_positional_embedding == \"learned\":\n        self.feature_positional_embedding_embeddings = nn.Embedding(1_000, ninp)\n    elif feature_positional_embedding == \"subspace\":\n        self.feature_positional_embedding_embeddings = nn.Linear(ninp // 4, ninp)\n\n    self.dag_pos_enc_dim = dag_pos_enc_dim\n    self.cached_feature_positional_embeddings = None\n    self.seed = random.randint(0, 1_000_000)\n    self.generator_device = (\n        \"cpu\"  # Device on which the generator was last initialized.\n    )\n    # If loading from a checkpoint, this might be false, but it will be set to the correct\n    # device on the first forward pass.\n    self._init_rnd()\n</code></pre>"},{"location":"api/model.transformer/#model.transformer.PerFeatureTransformer.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Performs a forward pass through the model.</p> <p>This method supports multiple calling conventions: - model(train_x, train_y, test_x, **kwargs) - model((x,y), **kwargs) - model((style,x,y), **kwargs)</p> <p>Parameters:</p> Name Type Description Default <code>train_x</code> <p>The input data for the training set.</p> required <code>train_y</code> <p>The target data for the training set.</p> required <code>test_x</code> <p>The input data for the test set.</p> required <code>x</code> <p>The input data.</p> required <code>y</code> <p>The target data.</p> required <code>style</code> <p>The style vector.</p> required <code>single_eval_pos</code> <p>The position to evaluate at.</p> required <code>only_return_standard_out</code> <p>Whether to only return the standard output.</p> required <code>data_dags</code> <p>The data DAGs for each example.</p> required <code>categorical_inds</code> <p>The indices of categorical features.</p> required <code>freeze_kv</code> <p>Whether to freeze the key and value weights.</p> required <p>Returns:</p> Type Description <p>The output of the model, which can be a tensor or a dictionary of tensors.</p> Source code in <code>model/transformer.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"\n    Performs a forward pass through the model.\n\n    This method supports multiple calling conventions:\n    - model(train_x, train_y, test_x, **kwargs)\n    - model((x,y), **kwargs)\n    - model((style,x,y), **kwargs)\n\n    Parameters:\n        train_x: The input data for the training set.\n        train_y: The target data for the training set.\n        test_x: The input data for the test set.\n        x: The input data.\n        y: The target data.\n        style: The style vector.\n        single_eval_pos: The position to evaluate at.\n        only_return_standard_out: Whether to only return the standard output.\n        data_dags: The data DAGs for each example.\n        categorical_inds: The indices of categorical features.\n        freeze_kv: Whether to freeze the key and value weights.\n\n    Returns:\n        The output of the model, which can be a tensor or a dictionary of tensors.\n    \"\"\"\n    supported_kwargs = {\n        \"single_eval_pos\",\n        \"only_return_standard_out\",\n        \"style\",\n        \"data_dags\",\n        \"categorical_inds\",\n        \"freeze_kv\",\n    }\n    if \"half_layers\" in kwargs:\n        assert not kwargs[\"half_layers\"]\n        del kwargs[\"half_layers\"]\n\n    if \"train_x\" in kwargs:\n        assert len(args) == 0\n        args = [kwargs[\"train_x\"], kwargs[\"train_y\"], kwargs[\"test_x\"]]\n        del kwargs[\"train_x\"]\n        del kwargs[\"train_y\"]\n        del kwargs[\"test_x\"]\n\n    if len(args) == 3:\n        supported_kwargs.remove(\"single_eval_pos\")\n    spurious_kwargs = set(kwargs.keys()) - supported_kwargs\n    assert not spurious_kwargs, spurious_kwargs\n\n    if len(args) == 3:\n        # case model(train_x, train_y, test_x, src_mask=None, style=None, only_return_standard_out=True)\n        x = args[0]\n        if args[2] is not None:\n            x = torch.cat((x, args[2]), dim=0)\n        return self._forward(x, args[1], single_eval_pos=len(args[0]), **kwargs)\n    elif len(args) == 1 and isinstance(args, tuple):\n        # case model((x,y), src_mask=None, single_eval_pos=None, only_return_standard_out=True)\n        # case model((style,x,y), src_mask=None, single_eval_pos=None, only_return_standard_out=True)\n        if len(args[0]) == 3:\n            return self._forward(*args[0][1:], style=args[0][0], **kwargs)\n        else:\n            assert (\n                len(args[0]) == 2\n            ), f\"Expected tuple of length 2 or 3, got {len(args[0])}\"\n            return self._forward(*args[0], **kwargs)\n    else:\n        raise ValueError(\n            \"Unrecognized input. Please follow the doc string exactly.\"\n        )\n</code></pre>"},{"location":"api/model.transformer/#model.layer.PerFeatureEncoderLayer","title":"PerFeatureEncoderLayer","text":"<p>             Bases: <code>Module</code></p> <p>Transformer encoder layer that processes each feature block separately.</p> <p>This layer consists of multi-head attention between features, multi-head attention between items, and feedforward neural networks (MLPs). It supports various configurations and optimization options.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The dimensionality of the input and output embeddings.</p> required <code>nhead</code> <code>int</code> <p>The number of attention heads.</p> required <code>dim_feedforward</code> <code>Optional[int]</code> <p>The dimensionality of the feedforward network. Default is None (2 * d_model).</p> <code>None</code> <code>activation</code> <code>str</code> <p>The activation function to use in the MLPs. Default is \"relu\".</p> <code>'relu'</code> <code>layer_norm_eps</code> <code>float</code> <p>The epsilon value for layer normalization. Default is 1e-5.</p> <code>1e-05</code> <code>pre_norm</code> <code>bool</code> <p>Whether to apply layer normalization before or after the attention and MLPs. Default is False.</p> <code>False</code> <code>device</code> <code>Optional[device]</code> <p>The device to use for the layer parameters. Default is None.</p> <code>None</code> <code>dtype</code> <code>Optional[dtype]</code> <p>The data type to use for the layer parameters. Default is None.</p> <code>None</code> <code>recompute_attn</code> <code>bool</code> <p>Whether to recompute attention during backpropagation. Default is False.</p> <code>False</code> <code>second_mlp</code> <code>bool</code> <p>Whether to include a second MLP in the layer. Default is False.</p> <code>False</code> <code>layer_norm_with_elementwise_affine</code> <code>bool</code> <p>Whether to use elementwise affine parameters in layer normalization. Default is False.</p> <code>False</code> <code>zero_init</code> <code>bool</code> <p>Whether to initialize the output of the MLPs to zero. Default is False.</p> <code>False</code> <code>save_peak_mem_factor</code> <code>Optional[int]</code> <p>The factor to save peak memory, only effective with post-norm. Default is None.</p> <code>None</code> <code>attention_between_features</code> <code>bool</code> <p>Whether to apply attention between feature blocks. Default is True.</p> <code>True</code> <code>multiquery_item_attention</code> <code>bool</code> <p>Whether to use multiquery attention for items. Default is False.</p> <code>False</code> <code>multiquery_item_attention_for_test_set</code> <code>bool</code> <p>Whether to use multiquery attention for the test set. Default is False.</p> <code>False</code> <code>attention_init_gain</code> <code>float</code> <p>The gain value for initializing attention parameters. Default is 1.0.</p> <code>1.0</code> <code>d_k</code> <code>Optional[int]</code> <p>The dimensionality of the query and key vectors. Default is None (d_model // nhead).</p> <code>None</code> <code>d_v</code> <code>Optional[int]</code> <p>The dimensionality of the value vectors. Default is None (d_model // nhead).</p> <code>None</code> <code>precomputed_kv</code> <code>Union[None, Tensor, Tuple[Tensor, Tensor]]</code> <p>Precomputed key-value pairs for attention. Default is None.</p> <code>None</code> Source code in <code>model/layer.py</code> <pre><code>class PerFeatureEncoderLayer(Module):\n    \"\"\"\n    Transformer encoder layer that processes each feature block separately.\n\n    This layer consists of multi-head attention between features, multi-head attention between items,\n    and feedforward neural networks (MLPs). It supports various configurations and optimization options.\n\n    Args:\n        d_model: The dimensionality of the input and output embeddings.\n        nhead: The number of attention heads.\n        dim_feedforward: The dimensionality of the feedforward network. Default is None (2 * d_model).\n        activation: The activation function to use in the MLPs. Default is \"relu\".\n        layer_norm_eps: The epsilon value for layer normalization. Default is 1e-5.\n        pre_norm: Whether to apply layer normalization before or after the attention and MLPs. Default is False.\n        device: The device to use for the layer parameters. Default is None.\n        dtype: The data type to use for the layer parameters. Default is None.\n        recompute_attn: Whether to recompute attention during backpropagation. Default is False.\n        second_mlp: Whether to include a second MLP in the layer. Default is False.\n        layer_norm_with_elementwise_affine: Whether to use elementwise affine parameters in layer normalization. Default is False.\n        zero_init: Whether to initialize the output of the MLPs to zero. Default is False.\n        save_peak_mem_factor: The factor to save peak memory, only effective with post-norm. Default is None.\n        attention_between_features: Whether to apply attention between feature blocks. Default is True.\n        multiquery_item_attention: Whether to use multiquery attention for items. Default is False.\n        multiquery_item_attention_for_test_set: Whether to use multiquery attention for the test set. Default is False.\n        attention_init_gain: The gain value for initializing attention parameters. Default is 1.0.\n        d_k: The dimensionality of the query and key vectors. Default is None (d_model // nhead).\n        d_v: The dimensionality of the value vectors. Default is None (d_model // nhead).\n        precomputed_kv: Precomputed key-value pairs for attention. Default is None.\n    \"\"\"\n\n    __constants__ = [\"batch_first\"]\n\n    def __init__(\n        self,\n        d_model: int,\n        nhead: int,\n        dim_feedforward: Optional[int] = None,\n        activation: str = \"relu\",\n        layer_norm_eps: float = 1e-5,\n        pre_norm: bool = False,\n        device: Optional[torch.device] = None,\n        dtype: Optional[torch.dtype] = None,\n        recompute_attn: bool = False,\n        second_mlp: bool = False,\n        layer_norm_with_elementwise_affine: bool = False,\n        zero_init: bool = False,\n        save_peak_mem_factor: Optional[int] = None,\n        attention_between_features: bool = True,\n        multiquery_item_attention: bool = False,\n        multiquery_item_attention_for_test_set: bool = False,\n        two_sets_of_queries: bool = False,\n        attention_init_gain: float = 1.0,\n        d_k: Optional[int] = None,\n        d_v: Optional[int] = None,\n        precomputed_kv: Union[\n            None, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]\n        ] = None,\n    ) -&gt; None:\n        super().__init__()\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        assert d_model % nhead == 0 or d_k is not None and d_v is not None\n        assert not (\n            multiquery_item_attention_for_test_set and multiquery_item_attention\n        ), \"Cannot use both multiquery_item_attention_for_test_set and multiquery_item_attention\"\n        if two_sets_of_queries:\n            assert (\n                multiquery_item_attention_for_test_set\n            ), \"two_sets_of_queries requires multiquery_item_attention_for_test_set\"\n        if d_k is None:\n            d_k = d_model // nhead\n        if d_v is None:\n            d_v = d_model // nhead\n        if multiquery_item_attention:\n            print(\"Using multiquery in item attention.\")\n        if attention_between_features:\n            self.self_attn_between_features = MultiHeadAttention(\n                d_model,\n                d_model,\n                d_k,\n                d_v,\n                nhead,\n                device,\n                dtype,\n                initialize_output_to_zero=zero_init,\n                recompute=recompute_attn,\n                init_gain=attention_init_gain,\n            )\n        else:\n            self.self_attn_between_features = None\n\n        if isinstance(precomputed_kv, tuple):\n            precomputed_k, precomputed_v = precomputed_kv\n            precomputed_kv = None\n        else:\n            assert precomputed_kv is None or isinstance(precomputed_kv, torch.Tensor)\n            precomputed_k = precomputed_v = None\n        self.self_attn_between_items = MultiHeadAttention(\n            d_model,\n            d_model,\n            d_k,\n            d_v,\n            nhead,\n            device,\n            dtype,\n            share_kv_across_n_heads=nhead if multiquery_item_attention else 1,\n            initialize_output_to_zero=zero_init,\n            recompute=recompute_attn,\n            precomputed_k=precomputed_k,\n            precomputed_v=precomputed_v,\n            precomputed_kv=precomputed_kv,\n            init_gain=attention_init_gain,\n            two_sets_of_queries=multiquery_item_attention_for_test_set\n            and two_sets_of_queries,\n        )\n\n        if dim_feedforward is None:\n            dim_feedforward = 2 * d_model\n        self.mlp = MLP(\n            d_model,\n            dim_feedforward,\n            activation,\n            device,\n            dtype,\n            initialize_output_to_zero=zero_init,\n            recompute=recompute_attn,\n        )\n\n        self.layer_norms = nn.ModuleList(\n            [\n                LayerNorm(\n                    d_model,\n                    layer_norm_eps,\n                    elementwise_affine=layer_norm_with_elementwise_affine,\n                    **factory_kwargs,\n                )\n                for _ in range(4 if second_mlp else 3)\n            ]\n        )\n\n        if second_mlp:\n            self.second_mlp = MLP(\n                d_model,\n                dim_feedforward,\n                activation,\n                device,\n                dtype,\n                initialize_output_to_zero=zero_init,\n                recompute=recompute_attn,\n            )\n        else:\n            self.second_mlp = None\n\n        self.pre_norm = pre_norm\n        self.recompute_attn = recompute_attn\n        self.save_peak_mem_factor = save_peak_mem_factor\n        self.multiquery_item_attention_for_test_set = (\n            multiquery_item_attention_for_test_set\n        )\n        self.two_sets_of_queries = two_sets_of_queries\n\n    def __setstate__(self, state):\n        state.setdefault(\"save_peak_mem_factor\", None)\n        super().__setstate__(state)\n\n    def forward(\n        self,\n        state: Tensor,\n        single_eval_pos: Optional[int] = None,\n        cache_trainset_representation: bool = False,\n        att_src: Optional[Tensor] = None,\n    ) -&gt; Tensor:\n        \"\"\"\n        Pass the input through the encoder layer.\n\n        Args:\n            state: The transformer state passed as input to the layer of shape\n                (batch_size, num_items, num_feature_blocks, d_model).\n            single_eval_pos: The position from which on everything is treated as test set. Default is None.\n            cache_trainset_representation: Whether to cache the trainset representation.\n                If single_eval_pos is set (&gt; 0 and not None), create a cache of the trainset KV.\n                This may require a lot of memory. Otherwise, use cached KV representations for inference.\n                Default is False.\n            att_src: The tensor to attend to from the final layer of the encoder. It has a shape of\n                (batch_size, num_train_items, num_feature_blocks, d_model). This does not work with\n                multiquery_item_attention_for_test_set and cache_trainset_representation at this point.\n                Combining would be possible, however.\n                Default is None.\n\n        Returns:\n            The transformer state passed through the encoder layer.\n        \"\"\"\n        assert (\n            len(state.shape) == 4\n        ), \"src must be of shape (batch_size, num_items, num feature blocks, d_model)\"\n        if single_eval_pos is None:\n            single_eval_pos = 0\n        if cache_trainset_representation and not single_eval_pos:\n            assert self.self_attn_between_items.has_cached_kv\n\n        if att_src is not None:\n            assert (\n                not self.multiquery_item_attention_for_test_set\n            ), \"Not implemented yet.\"\n            assert not cache_trainset_representation, \"Not implemented yet.\"\n            assert (\n                not single_eval_pos\n            ), \"single_eval_pos should not be set, as the train representation is in att_src\"\n\n        def attn_between_features(x):\n            return self.self_attn_between_features(\n                x,\n                save_peak_mem_factor=self.save_peak_mem_factor,\n                add_input=True,\n                allow_inplace=True,\n            )\n\n        def attn_between_items(x):\n            # we need to transpose as self attention always treats dim -2 as the sequence dimension\n            if self.multiquery_item_attention_for_test_set:\n                if single_eval_pos &lt; x.shape[1]:\n                    new_x_test = self.self_attn_between_items(\n                        x[:, single_eval_pos:].transpose(1, 2),\n                        x[:, :single_eval_pos].transpose(1, 2)\n                        if single_eval_pos\n                        else None,\n                        save_peak_mem_factor=self.save_peak_mem_factor,\n                        cache_kv=False,\n                        add_input=True,\n                        allow_inplace=True,\n                        use_cached_kv=not single_eval_pos,\n                        reuse_first_head_kv=True,\n                        use_second_set_of_queries=self.two_sets_of_queries,\n                    ).transpose(1, 2)\n                else:\n                    new_x_test = None\n\n                if single_eval_pos:\n                    new_x_train = self.self_attn_between_items(\n                        x[:, :single_eval_pos].transpose(1, 2),\n                        x[:, :single_eval_pos].transpose(1, 2),\n                        save_peak_mem_factor=self.save_peak_mem_factor,\n                        cache_kv=cache_trainset_representation,\n                        add_input=True,\n                        allow_inplace=True,\n                        use_cached_kv=False,\n                    ).transpose(1, 2)\n                else:\n                    new_x_train = None\n                return torch.cat(\n                    [x_ for x_ in [new_x_train, new_x_test] if x_ is not None], dim=1\n                )\n            else:\n                attention_src_x = None\n                if att_src is not None:\n                    attention_src_x = att_src.transpose(1, 2)\n                elif single_eval_pos:\n                    attention_src_x = x[:, :single_eval_pos].transpose(1, 2)\n\n                return self.self_attn_between_items(\n                    x.transpose(1, 2),\n                    attention_src_x,\n                    save_peak_mem_factor=self.save_peak_mem_factor,\n                    cache_kv=cache_trainset_representation and single_eval_pos,\n                    add_input=True,\n                    allow_inplace=True,\n                    use_cached_kv=cache_trainset_representation and not single_eval_pos,\n                ).transpose(1, 2)\n\n        mlp_save_peak_mem_factor = (\n            self.save_peak_mem_factor * 8\n            if self.save_peak_mem_factor is not None\n            else None\n        )\n\n        sublayers = []\n        if self.self_attn_between_features is not None:\n            sublayers.append(attn_between_features)\n        else:\n            assert (\n                state.shape[2] == 1\n            ), \"If there is no attention between features, the number of feature blocks must be 1.\"\n\n        sublayers += [\n            attn_between_items,\n            partial(\n                self.mlp,\n                save_peak_mem_factor=mlp_save_peak_mem_factor\n                if (\n                    mlp_save_peak_mem_factor is not None\n                    and state.numel() // state.shape[-1] // mlp_save_peak_mem_factor\n                )\n                &gt; 32\n                else None,  # this is a hot fix, since it seems the matmul kernels for small batch sizes yield different results...\n                add_input=True,\n                allow_inplace=True,\n            ),\n        ]\n\n        if self.second_mlp is not None:\n            sublayers.insert(\n                1,\n                partial(\n                    self.second_mlp,\n                    save_peak_mem_factor=mlp_save_peak_mem_factor,\n                    add_input=True,\n                    allow_inplace=True,\n                ),\n            )\n\n        for sublayer, layer_norm in zip(sublayers, self.layer_norms):\n            if self.pre_norm:\n                assert (\n                    False\n                ), \"Pre-norm implementation is wrong, as the residual should never be layer normed here.\"\n                state = layer_norm(\n                    state,\n                    allow_inplace=True,\n                    save_peak_mem_factor=self.save_peak_mem_factor,\n                )\n            state = sublayer(state)\n            if not self.pre_norm:\n                state = layer_norm(\n                    state,\n                    allow_inplace=True,\n                    save_peak_mem_factor=self.save_peak_mem_factor,\n                )\n\n        return state\n\n    def empty_trainset_representation_cache(self):\n        self.self_attn_between_items.empty_kv_cache()\n        self.self_attn_between_features.empty_kv_cache()  # not necessary, but just in case\n</code></pre>"},{"location":"api/model.transformer/#model.layer.PerFeatureEncoderLayer.__init__","title":"__init__","text":"<pre><code>__init__(d_model: int, nhead: int, dim_feedforward: Optional[int] = None, activation: str = 'relu', layer_norm_eps: float = 1e-05, pre_norm: bool = False, device: Optional[device] = None, dtype: Optional[dtype] = None, recompute_attn: bool = False, second_mlp: bool = False, layer_norm_with_elementwise_affine: bool = False, zero_init: bool = False, save_peak_mem_factor: Optional[int] = None, attention_between_features: bool = True, multiquery_item_attention: bool = False, multiquery_item_attention_for_test_set: bool = False, two_sets_of_queries: bool = False, attention_init_gain: float = 1.0, d_k: Optional[int] = None, d_v: Optional[int] = None, precomputed_kv: Union[None, Tensor, Tuple[Tensor, Tensor]] = None) -&gt; None\n</code></pre> Source code in <code>model/layer.py</code> <pre><code>def __init__(\n    self,\n    d_model: int,\n    nhead: int,\n    dim_feedforward: Optional[int] = None,\n    activation: str = \"relu\",\n    layer_norm_eps: float = 1e-5,\n    pre_norm: bool = False,\n    device: Optional[torch.device] = None,\n    dtype: Optional[torch.dtype] = None,\n    recompute_attn: bool = False,\n    second_mlp: bool = False,\n    layer_norm_with_elementwise_affine: bool = False,\n    zero_init: bool = False,\n    save_peak_mem_factor: Optional[int] = None,\n    attention_between_features: bool = True,\n    multiquery_item_attention: bool = False,\n    multiquery_item_attention_for_test_set: bool = False,\n    two_sets_of_queries: bool = False,\n    attention_init_gain: float = 1.0,\n    d_k: Optional[int] = None,\n    d_v: Optional[int] = None,\n    precomputed_kv: Union[\n        None, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]\n    ] = None,\n) -&gt; None:\n    super().__init__()\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    assert d_model % nhead == 0 or d_k is not None and d_v is not None\n    assert not (\n        multiquery_item_attention_for_test_set and multiquery_item_attention\n    ), \"Cannot use both multiquery_item_attention_for_test_set and multiquery_item_attention\"\n    if two_sets_of_queries:\n        assert (\n            multiquery_item_attention_for_test_set\n        ), \"two_sets_of_queries requires multiquery_item_attention_for_test_set\"\n    if d_k is None:\n        d_k = d_model // nhead\n    if d_v is None:\n        d_v = d_model // nhead\n    if multiquery_item_attention:\n        print(\"Using multiquery in item attention.\")\n    if attention_between_features:\n        self.self_attn_between_features = MultiHeadAttention(\n            d_model,\n            d_model,\n            d_k,\n            d_v,\n            nhead,\n            device,\n            dtype,\n            initialize_output_to_zero=zero_init,\n            recompute=recompute_attn,\n            init_gain=attention_init_gain,\n        )\n    else:\n        self.self_attn_between_features = None\n\n    if isinstance(precomputed_kv, tuple):\n        precomputed_k, precomputed_v = precomputed_kv\n        precomputed_kv = None\n    else:\n        assert precomputed_kv is None or isinstance(precomputed_kv, torch.Tensor)\n        precomputed_k = precomputed_v = None\n    self.self_attn_between_items = MultiHeadAttention(\n        d_model,\n        d_model,\n        d_k,\n        d_v,\n        nhead,\n        device,\n        dtype,\n        share_kv_across_n_heads=nhead if multiquery_item_attention else 1,\n        initialize_output_to_zero=zero_init,\n        recompute=recompute_attn,\n        precomputed_k=precomputed_k,\n        precomputed_v=precomputed_v,\n        precomputed_kv=precomputed_kv,\n        init_gain=attention_init_gain,\n        two_sets_of_queries=multiquery_item_attention_for_test_set\n        and two_sets_of_queries,\n    )\n\n    if dim_feedforward is None:\n        dim_feedforward = 2 * d_model\n    self.mlp = MLP(\n        d_model,\n        dim_feedforward,\n        activation,\n        device,\n        dtype,\n        initialize_output_to_zero=zero_init,\n        recompute=recompute_attn,\n    )\n\n    self.layer_norms = nn.ModuleList(\n        [\n            LayerNorm(\n                d_model,\n                layer_norm_eps,\n                elementwise_affine=layer_norm_with_elementwise_affine,\n                **factory_kwargs,\n            )\n            for _ in range(4 if second_mlp else 3)\n        ]\n    )\n\n    if second_mlp:\n        self.second_mlp = MLP(\n            d_model,\n            dim_feedforward,\n            activation,\n            device,\n            dtype,\n            initialize_output_to_zero=zero_init,\n            recompute=recompute_attn,\n        )\n    else:\n        self.second_mlp = None\n\n    self.pre_norm = pre_norm\n    self.recompute_attn = recompute_attn\n    self.save_peak_mem_factor = save_peak_mem_factor\n    self.multiquery_item_attention_for_test_set = (\n        multiquery_item_attention_for_test_set\n    )\n    self.two_sets_of_queries = two_sets_of_queries\n</code></pre>"},{"location":"api/model.transformer/#model.layer.PerFeatureEncoderLayer.forward","title":"forward","text":"<pre><code>forward(state: Tensor, single_eval_pos: Optional[int] = None, cache_trainset_representation: bool = False, att_src: Optional[Tensor] = None) -&gt; Tensor\n</code></pre> <p>Pass the input through the encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Tensor</code> <p>The transformer state passed as input to the layer of shape (batch_size, num_items, num_feature_blocks, d_model).</p> required <code>single_eval_pos</code> <code>Optional[int]</code> <p>The position from which on everything is treated as test set. Default is None.</p> <code>None</code> <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the trainset representation. If single_eval_pos is set (&gt; 0 and not None), create a cache of the trainset KV. This may require a lot of memory. Otherwise, use cached KV representations for inference. Default is False.</p> <code>False</code> <code>att_src</code> <code>Optional[Tensor]</code> <p>The tensor to attend to from the final layer of the encoder. It has a shape of (batch_size, num_train_items, num_feature_blocks, d_model). This does not work with multiquery_item_attention_for_test_set and cache_trainset_representation at this point. Combining would be possible, however. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The transformer state passed through the encoder layer.</p> Source code in <code>model/layer.py</code> <pre><code>def forward(\n    self,\n    state: Tensor,\n    single_eval_pos: Optional[int] = None,\n    cache_trainset_representation: bool = False,\n    att_src: Optional[Tensor] = None,\n) -&gt; Tensor:\n    \"\"\"\n    Pass the input through the encoder layer.\n\n    Args:\n        state: The transformer state passed as input to the layer of shape\n            (batch_size, num_items, num_feature_blocks, d_model).\n        single_eval_pos: The position from which on everything is treated as test set. Default is None.\n        cache_trainset_representation: Whether to cache the trainset representation.\n            If single_eval_pos is set (&gt; 0 and not None), create a cache of the trainset KV.\n            This may require a lot of memory. Otherwise, use cached KV representations for inference.\n            Default is False.\n        att_src: The tensor to attend to from the final layer of the encoder. It has a shape of\n            (batch_size, num_train_items, num_feature_blocks, d_model). This does not work with\n            multiquery_item_attention_for_test_set and cache_trainset_representation at this point.\n            Combining would be possible, however.\n            Default is None.\n\n    Returns:\n        The transformer state passed through the encoder layer.\n    \"\"\"\n    assert (\n        len(state.shape) == 4\n    ), \"src must be of shape (batch_size, num_items, num feature blocks, d_model)\"\n    if single_eval_pos is None:\n        single_eval_pos = 0\n    if cache_trainset_representation and not single_eval_pos:\n        assert self.self_attn_between_items.has_cached_kv\n\n    if att_src is not None:\n        assert (\n            not self.multiquery_item_attention_for_test_set\n        ), \"Not implemented yet.\"\n        assert not cache_trainset_representation, \"Not implemented yet.\"\n        assert (\n            not single_eval_pos\n        ), \"single_eval_pos should not be set, as the train representation is in att_src\"\n\n    def attn_between_features(x):\n        return self.self_attn_between_features(\n            x,\n            save_peak_mem_factor=self.save_peak_mem_factor,\n            add_input=True,\n            allow_inplace=True,\n        )\n\n    def attn_between_items(x):\n        # we need to transpose as self attention always treats dim -2 as the sequence dimension\n        if self.multiquery_item_attention_for_test_set:\n            if single_eval_pos &lt; x.shape[1]:\n                new_x_test = self.self_attn_between_items(\n                    x[:, single_eval_pos:].transpose(1, 2),\n                    x[:, :single_eval_pos].transpose(1, 2)\n                    if single_eval_pos\n                    else None,\n                    save_peak_mem_factor=self.save_peak_mem_factor,\n                    cache_kv=False,\n                    add_input=True,\n                    allow_inplace=True,\n                    use_cached_kv=not single_eval_pos,\n                    reuse_first_head_kv=True,\n                    use_second_set_of_queries=self.two_sets_of_queries,\n                ).transpose(1, 2)\n            else:\n                new_x_test = None\n\n            if single_eval_pos:\n                new_x_train = self.self_attn_between_items(\n                    x[:, :single_eval_pos].transpose(1, 2),\n                    x[:, :single_eval_pos].transpose(1, 2),\n                    save_peak_mem_factor=self.save_peak_mem_factor,\n                    cache_kv=cache_trainset_representation,\n                    add_input=True,\n                    allow_inplace=True,\n                    use_cached_kv=False,\n                ).transpose(1, 2)\n            else:\n                new_x_train = None\n            return torch.cat(\n                [x_ for x_ in [new_x_train, new_x_test] if x_ is not None], dim=1\n            )\n        else:\n            attention_src_x = None\n            if att_src is not None:\n                attention_src_x = att_src.transpose(1, 2)\n            elif single_eval_pos:\n                attention_src_x = x[:, :single_eval_pos].transpose(1, 2)\n\n            return self.self_attn_between_items(\n                x.transpose(1, 2),\n                attention_src_x,\n                save_peak_mem_factor=self.save_peak_mem_factor,\n                cache_kv=cache_trainset_representation and single_eval_pos,\n                add_input=True,\n                allow_inplace=True,\n                use_cached_kv=cache_trainset_representation and not single_eval_pos,\n            ).transpose(1, 2)\n\n    mlp_save_peak_mem_factor = (\n        self.save_peak_mem_factor * 8\n        if self.save_peak_mem_factor is not None\n        else None\n    )\n\n    sublayers = []\n    if self.self_attn_between_features is not None:\n        sublayers.append(attn_between_features)\n    else:\n        assert (\n            state.shape[2] == 1\n        ), \"If there is no attention between features, the number of feature blocks must be 1.\"\n\n    sublayers += [\n        attn_between_items,\n        partial(\n            self.mlp,\n            save_peak_mem_factor=mlp_save_peak_mem_factor\n            if (\n                mlp_save_peak_mem_factor is not None\n                and state.numel() // state.shape[-1] // mlp_save_peak_mem_factor\n            )\n            &gt; 32\n            else None,  # this is a hot fix, since it seems the matmul kernels for small batch sizes yield different results...\n            add_input=True,\n            allow_inplace=True,\n        ),\n    ]\n\n    if self.second_mlp is not None:\n        sublayers.insert(\n            1,\n            partial(\n                self.second_mlp,\n                save_peak_mem_factor=mlp_save_peak_mem_factor,\n                add_input=True,\n                allow_inplace=True,\n            ),\n        )\n\n    for sublayer, layer_norm in zip(sublayers, self.layer_norms):\n        if self.pre_norm:\n            assert (\n                False\n            ), \"Pre-norm implementation is wrong, as the residual should never be layer normed here.\"\n            state = layer_norm(\n                state,\n                allow_inplace=True,\n                save_peak_mem_factor=self.save_peak_mem_factor,\n            )\n        state = sublayer(state)\n        if not self.pre_norm:\n            state = layer_norm(\n                state,\n                allow_inplace=True,\n                save_peak_mem_factor=self.save_peak_mem_factor,\n            )\n\n    return state\n</code></pre>"},{"location":"api/tabpfn_classifier/","title":"TabPFNClassifier","text":""},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier","title":"TabPFNClassifier","text":"<p>             Bases: <code>TabPFNBaseModel</code>, <code>ClassifierMixin</code></p> Source code in <code>scripts/estimator/base.py</code> <pre><code>class TabPFNClassifier(TabPFNBaseModel, ClassifierMixin):\n    semisupervised_indicator = -100\n    metric_type = ClassificationOptimizationMetricType\n\n    predict_function_for_shap = \"predict_proba\"\n\n    def __init__(\n        self,\n        model_path: str = Path(local_model_path) / \"model_hans_classification.ckpt\",\n        n_estimators: int = 4,\n        preprocess_transforms: Tuple[PreprocessorConfig, ...] = (\n            PreprocessorConfig(\n                \"quantile_uni_coarse\",\n                append_original=True,\n                categorical_name=\"ordinal_very_common_categories_shuffled\",\n                global_transformer_name=\"svd\",\n                subsample_features=-1,\n            ),\n            PreprocessorConfig(\n                \"none\", categorical_name=\"numeric\", subsample_features=-1\n            ),\n        ),\n        feature_shift_decoder: str = \"shuffle\",\n        normalize_with_test: bool = False,\n        average_logits: bool = False,\n        optimize_metric: ClassificationOptimizationMetricType = \"roc\",\n        transformer_predict_kwargs: Optional[Dict] = None,\n        multiclass_decoder=\"shuffle\",\n        softmax_temperature: Optional[float] = -0.1,\n        use_poly_features=False,\n        max_poly_features=50,\n        transductive=False,\n        remove_outliers=12.0,\n        add_fingerprint_features=True,\n        subsample_samples=-1,\n        # you can also pass the model directly as torch module and a config dict\n        model: Optional[torch.nn.Module] = None,\n        model_config: Optional[Dict] = None,\n        # The following parameters are not tunable, but specify the execution mode\n        fit_at_predict_time: bool = True,\n        device: tp.Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n        seed: Optional[int] = 0,\n        show_progress: bool = True,\n        batch_size_inference: int = 1,\n        fp16_inference: bool = True,\n        save_peak_memory: Literal[\"True\", \"False\", \"auto\"] = \"True\",\n    ):\n        \"\"\"\n        Parameters:\n            model_path: The model string is the path to the model.\n            n_estimators: The number of ensemble configurations to use, the most important setting.\n            preprocess_transforms: A tuple of strings, specifying the preprocessing steps to use.\n                You can use the following strings as elements '(none|power|quantile|robust)[_all][_and_none]', where the first\n                part specifies the preprocessing step and the second part specifies the features to apply it to and\n                finally '_and_none' specifies that the original features should be added back to the features in plain.\n                Finally, you can combine all strings without `_all` with `_onehot` to apply one-hot encoding to the categorical\n                features specified with `self.fit(..., categorical_features=...)`.\n            feature_shift_decoder: [\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.\n            normalize_with_test: If True, the test set is used to normalize the data, otherwise the training set is used only.\n            average_logits: Whether to average logits or probabilities for ensemble members.\n            optimize_metric: The optimization metric to use.\n            transformer_predict_kwargs: Additional keyword arguments to pass to the transformer predict method.\n            multiclass_decoder: The multiclass decoder to use.\n            softmax_temperature: A log spaced temperature, it will be applied as logits &lt;- logits/exp(softmax_temperature).\n            use_poly_features: Whether to use polynomial features as the last preprocessing step.\n            max_poly_features: Maximum number of polynomial features to use, None means unlimited.\n            transductive: Whether to use transductive learning.\n            remove_outliers: If not 0.0, will remove outliers from the input features, where values with a standard deviation\n                larger than remove_outliers will be removed.\n            add_fingerprint_features: If True, will add one feature of random values, that will be added to\n                the input features. This helps discern duplicated samples in the transformer model.\n            subsample_samples: If not None, will use a random subset of the samples for training in each ensemble configuration.\n                If 1 or above, this will subsample to the specified number of samples.\n                If in 0 to 1, the value is viewed as a fraction of the training set size.\n            model: The model, if you want to specify it directly, this is used in combination with model_config.\n            model_config: The config, if you want to specify it directly, this is used in combination with model.\n            fit_at_predict_time: Whether to train the model lazily, i.e. only when it is needed for inference in predict[_proba].\n            device: The device to use for inference, \"auto\" means that it will use cuda if available, otherwise cpu.\n            seed: The default seed to use for the order of the ensemble configurations, a seed of None will not.\n            show_progress: Whether to show progress bars during training and inference.\n            batch_size_inference: The batch size to use for inference, this does not affect the results, just the\n                memory usage and speed. A higher batch size is faster but uses more memory. Setting the batch size to None\n                means that the batch size is automatically determined based on the memory usage and the maximum free memory\n                specified with `maximum_free_memory_in_gb`.\n            fp16_inference: Whether to use fp16 for inference on GPU, does not affect CPU inference.\n            save_peak_memory: Whether to save the peak memory usage of the model, can enable up to 8 times larger datasets to fit into memory.\n                \"True\", means always enabled, \"False\", means always disabled, \"auto\" means that it will be set based on the memory usage.\n        \"\"\"\n        assert optimize_metric in tp.get_args(self.metric_type)\n        self.multiclass_decoder = multiclass_decoder\n\n        # Pass all parameters to super class constructor\n        super().__init__(\n            model=model,\n            device=device,\n            model_path=model_path,\n            batch_size_inference=batch_size_inference,\n            fp16_inference=fp16_inference,\n            model_config=model_config,\n            n_estimators=n_estimators,\n            preprocess_transforms=preprocess_transforms,\n            feature_shift_decoder=feature_shift_decoder,\n            normalize_with_test=normalize_with_test,\n            average_logits=average_logits,\n            optimize_metric=optimize_metric,\n            seed=seed,\n            transformer_predict_kwargs=transformer_predict_kwargs,\n            show_progress=show_progress,\n            softmax_temperature=softmax_temperature,\n            save_peak_memory=save_peak_memory,\n            use_poly_features=use_poly_features,\n            max_poly_features=max_poly_features,\n            transductive=transductive,\n            remove_outliers=remove_outliers,\n            add_fingerprint_features=add_fingerprint_features,\n            subsample_samples=subsample_samples,\n            fit_at_predict_time=fit_at_predict_time,\n        )\n\n    def _validate_targets(self, y) -&gt; np.ndarray:\n        y_ = column_or_1d(y, warn=True)\n        check_classification_targets(y)\n\n        # Get classes and encode before type conversion to guarantee correct class labels.\n        not_nan_mask = ~np.isnan(y)\n        cls, y_[not_nan_mask] = np.unique(y_[not_nan_mask], return_inverse=True)\n\n        if len(cls) &lt; 2:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got %d class\"\n                % len(cls)\n            )\n\n        self.classes_ = cls\n\n        # convert type to align with the negative value of the indicator (e.g., avoid uint8)\n        y_ = y_.astype(np.float32)\n        y_[~not_nan_mask] = TabPFNClassifier.semisupervised_indicator\n\n        return np.asarray(y_, dtype=np.float32, order=\"C\")\n\n    @staticmethod\n    def check_training_data(\n        clf: TabPFNClassifier, X: np.ndarray, y: np.ndarray\n    ) -&gt; Tuple:\n        y[np.isnan(y)] = clf.semisupervised_indicator\n\n        unique_labels_ = unique_labels(y)\n\n        has_nan_class_count = 0\n        if clf.semisupervised_indicator in unique_labels_:\n            assert clf.c_processed_.get(\n                \"semisupervised_enabled\", False\n            ), \"Semisupervised not enabled for this model\"\n            has_nan_class_count = 1\n            print_once(\n                \"Found nan class in training data, will be used as semisupervsied\"\n            )\n\n        if len(unique_labels_) &gt; clf.max_num_classes_ + has_nan_class_count:\n            raise ValueError(\n                f\"The number of classes for this classifier is restricted to {clf.max_num_classes_}. \"\n                f\"Consider wrapping the estimator with `tabpfn.estimator.ManyClassClassifier` to be \"\n                f\"able to predict more classes\"\n            )\n\n        clf.num_classes_ = len(unique_labels_) - has_nan_class_count\n\n        # Check that X and y have correct shape\n        X, y = check_X_y(X, y, force_all_finite=False)\n\n        # Store the classes seen during fit\n        y = clf._validate_targets(y)\n        clf.label_encoder_ = LabelEncoder()\n        y[\n            y != TabPFNClassifier.semisupervised_indicator\n        ] = clf.label_encoder_.fit_transform(\n            y[y != TabPFNClassifier.semisupervised_indicator]\n        )\n\n        X, y = TabPFNBaseModel.check_training_data(clf, X, y)\n\n        return X, y\n\n    def init_model_and_get_model_config(self):\n        super().init_model_and_get_model_config()\n        assert self.is_classification_, \"This should be a classification model\"\n        self.max_num_classes_ = self.c_processed_[\"max_num_classes\"]\n\n    def _post_process_predict_proba(self, prediction: torch.Tensor) -&gt; np.ndarray:\n        prediction = prediction.squeeze(0)\n        if self.inference_mode:\n            prediction = prediction.detach().cpu().numpy()\n\n        if self.sklearn_compatible_precision:\n            print(\"SKLEARN COMPATIBLE PREDICTION FOR DEBUG PURPOSE\")\n            prediction = np.around(\n                prediction, decimals=16\n            )  # TODO: Do we want this, its just for sklearn\n            prediction[prediction &lt; 0.001] = 0\n            prediction = prediction / prediction.sum(axis=1, keepdims=True)\n        return prediction\n\n    def fit(self, X, y, additional_y=None) -&gt; TabPFNClassifier:\n        \"\"\"\n        Fits the TabPFNClassifier model to the input data `X` and `y`.\n\n        The actual training logic is delegated to the `_fit` method, which should be implemented by subclasses.\n\n        Parameters:\n            X (Union[ndarray, torch.Tensor]): The input feature matrix of shape (n_samples, n_features).\n            y (Union[ndarray, torch.Tensor]): The target labels of shape (n_samples,).\n            additional_y (Optional[Dict[str, torch.Tensor]]): Additional labels to use during training.\n\n        Returns:\n            TabPFNClassifier: The fitted model object (self).\n        \"\"\"\n        return super().fit(X, y, additional_y)\n\n    def _fit(self):\n        self.transformer_predict(\n            self.X_[:, None].float(),\n            self.y_[:, None].float(),\n            len(self.X_),\n            additional_ys=self.additional_y_,\n            cache_trainset_representations=not self.fit_at_predict_time,  # this will always be true here\n            **get_params_from_config(self.c_processed_),\n            **(self.transformer_predict_kwargs or {}),\n        )\n\n    def predict_full(self, X, additional_y=None, get_additional_outputs=None) -&gt; dict:\n        timing_start(\"predict\")\n        X_full, y_full, additional_y, eval_pos = self.predict_common_setup(\n            X, additional_y_eval=additional_y\n        )\n\n        prediction, additional_outputs = self.transformer_predict(\n            X_full,\n            y_full,\n            eval_pos if self.fit_at_predict_time else 0,\n            additional_ys=additional_y,\n            cache_trainset_representations=not self.fit_at_predict_time,\n            reweight_probs_based_on_train=self.optimizes_balanced_metric(),\n            get_additional_outputs=get_additional_outputs,\n            **get_params_from_config(self.c_processed_),\n            **(self.transformer_predict_kwargs or {}),\n        )\n\n        prediction = self._post_process_predict_proba(prediction)\n        timing_end(\"predict\")\n        if LOG_COMPUTE_TIME_PATH:\n            self._log_compute_time(X)\n\n        return {\"proba\": prediction, **additional_outputs}\n\n    def predict_proba(self, X, additional_y=None):\n        \"\"\"\n        Calls the transformer to predict the probabilities of the classes of the X test inputs given the previous set\n        training dataset\n\n        Parameters:\n            X: test datapoints\n        \"\"\"\n        return self.predict_full(X, additional_y=additional_y)[\"proba\"]\n\n    def predict(self, X, return_winning_probability=False):\n        \"\"\"\n        Predict the class labels for the input samples.\n\n        Parameters:\n            X (array-like): The input samples.\n            return_winning_probability (bool): Whether to return the winning probability.\n\n        Returns:\n            array: The predicted class labels.\n        \"\"\"\n        p = self.predict_proba(X)\n        y = np.argmax(p, axis=-1)\n        y = self.classes_.take(np.asarray(y, dtype=int))\n        if return_winning_probability:\n            return y, p.max(axis=-1)\n        return y\n\n    def predict_y_proba(self, X, y):\n        \"\"\"\n        Predict the probability of the target labels `y` given the input samples `X`.\n\n        Parameters:\n            X (array-like): The input samples.\n            y (array-like): The target labels.\n\n        Returns:\n            array: The predicted probabilities of the target labels.\n        \"\"\"\n        prediction = self.predict_proba(X)\n        y_prob = prediction[np.arange(len(y)), y.astype(int)]\n        return y_prob\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"\n        Compute the score of the model on the given test data and labels.\n\n        Parameters:\n            X (array-like): The input samples.\n            y (array-like): The true labels for `X`.\n            sample_weight (array-like, optional): Sample weights.\n\n        Returns:\n            float: The computed score.\n        \"\"\"\n        if self.optimize_metric in [\"roc\", \"auroc\", \"log_loss\"]:\n            pred = self.predict_proba(X)\n\n            if len(np.unique(y)) == 2:\n                pred = pred[:, 1]\n        else:\n            pred = self.predict(X)\n\n        return score_classification(\n            self.optimize_metric, y, pred, sample_weight=sample_weight\n        )\n</code></pre>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.__init__","title":"__init__","text":"<pre><code>__init__(model_path: str = Path(local_model_path) / 'model_hans_classification.ckpt', n_estimators: int = 4, preprocess_transforms: Tuple[PreprocessorConfig, ...] = (PreprocessorConfig('quantile_uni_coarse', append_original=True, categorical_name='ordinal_very_common_categories_shuffled', global_transformer_name='svd', subsample_features=-1), PreprocessorConfig('none', categorical_name='numeric', subsample_features=-1)), feature_shift_decoder: str = 'shuffle', normalize_with_test: bool = False, average_logits: bool = False, optimize_metric: ClassificationOptimizationMetricType = 'roc', transformer_predict_kwargs: Optional[Dict] = None, multiclass_decoder='shuffle', softmax_temperature: Optional[float] = -0.1, use_poly_features=False, max_poly_features=50, transductive=False, remove_outliers=12.0, add_fingerprint_features=True, subsample_samples=-1, model: Optional[Module] = None, model_config: Optional[Dict] = None, fit_at_predict_time: bool = True, device: Literal['cuda', 'cpu', 'auto'] = 'auto', seed: Optional[int] = 0, show_progress: bool = True, batch_size_inference: int = 1, fp16_inference: bool = True, save_peak_memory: Literal['True', 'False', 'auto'] = 'True')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The model string is the path to the model.</p> <code>Path(local_model_path) / 'model_hans_classification.ckpt'</code> <code>n_estimators</code> <code>int</code> <p>The number of ensemble configurations to use, the most important setting.</p> <code>4</code> <code>preprocess_transforms</code> <code>Tuple[PreprocessorConfig, ...]</code> <p>A tuple of strings, specifying the preprocessing steps to use. You can use the following strings as elements '(none|power|quantile|robust)_all', where the first part specifies the preprocessing step and the second part specifies the features to apply it to and finally '_and_none' specifies that the original features should be added back to the features in plain. Finally, you can combine all strings without <code>_all</code> with <code>_onehot</code> to apply one-hot encoding to the categorical features specified with <code>self.fit(..., categorical_features=...)</code>.</p> <code>(PreprocessorConfig('quantile_uni_coarse', append_original=True, categorical_name='ordinal_very_common_categories_shuffled', global_transformer_name='svd', subsample_features=-1), PreprocessorConfig('none', categorical_name='numeric', subsample_features=-1))</code> <code>feature_shift_decoder</code> <code>str</code> <p>[\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.</p> <code>'shuffle'</code> <code>normalize_with_test</code> <code>bool</code> <p>If True, the test set is used to normalize the data, otherwise the training set is used only.</p> <code>False</code> <code>average_logits</code> <code>bool</code> <p>Whether to average logits or probabilities for ensemble members.</p> <code>False</code> <code>optimize_metric</code> <code>ClassificationOptimizationMetricType</code> <p>The optimization metric to use.</p> <code>'roc'</code> <code>transformer_predict_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments to pass to the transformer predict method.</p> <code>None</code> <code>multiclass_decoder</code> <p>The multiclass decoder to use.</p> <code>'shuffle'</code> <code>softmax_temperature</code> <code>Optional[float]</code> <p>A log spaced temperature, it will be applied as logits &lt;- logits/exp(softmax_temperature).</p> <code>-0.1</code> <code>use_poly_features</code> <p>Whether to use polynomial features as the last preprocessing step.</p> <code>False</code> <code>max_poly_features</code> <p>Maximum number of polynomial features to use, None means unlimited.</p> <code>50</code> <code>transductive</code> <p>Whether to use transductive learning.</p> <code>False</code> <code>remove_outliers</code> <p>If not 0.0, will remove outliers from the input features, where values with a standard deviation larger than remove_outliers will be removed.</p> <code>12.0</code> <code>add_fingerprint_features</code> <p>If True, will add one feature of random values, that will be added to the input features. This helps discern duplicated samples in the transformer model.</p> <code>True</code> <code>subsample_samples</code> <p>If not None, will use a random subset of the samples for training in each ensemble configuration. If 1 or above, this will subsample to the specified number of samples. If in 0 to 1, the value is viewed as a fraction of the training set size.</p> <code>-1</code> <code>model</code> <code>Optional[Module]</code> <p>The model, if you want to specify it directly, this is used in combination with model_config.</p> <code>None</code> <code>model_config</code> <code>Optional[Dict]</code> <p>The config, if you want to specify it directly, this is used in combination with model.</p> <code>None</code> <code>fit_at_predict_time</code> <code>bool</code> <p>Whether to train the model lazily, i.e. only when it is needed for inference in predict[_proba].</p> <code>True</code> <code>device</code> <code>Literal['cuda', 'cpu', 'auto']</code> <p>The device to use for inference, \"auto\" means that it will use cuda if available, otherwise cpu.</p> <code>'auto'</code> <code>seed</code> <code>Optional[int]</code> <p>The default seed to use for the order of the ensemble configurations, a seed of None will not.</p> <code>0</code> <code>show_progress</code> <code>bool</code> <p>Whether to show progress bars during training and inference.</p> <code>True</code> <code>batch_size_inference</code> <code>int</code> <p>The batch size to use for inference, this does not affect the results, just the memory usage and speed. A higher batch size is faster but uses more memory. Setting the batch size to None means that the batch size is automatically determined based on the memory usage and the maximum free memory specified with <code>maximum_free_memory_in_gb</code>.</p> <code>1</code> <code>fp16_inference</code> <code>bool</code> <p>Whether to use fp16 for inference on GPU, does not affect CPU inference.</p> <code>True</code> <code>save_peak_memory</code> <code>Literal['True', 'False', 'auto']</code> <p>Whether to save the peak memory usage of the model, can enable up to 8 times larger datasets to fit into memory. \"True\", means always enabled, \"False\", means always disabled, \"auto\" means that it will be set based on the memory usage.</p> <code>'True'</code> Source code in <code>scripts/estimator/base.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = Path(local_model_path) / \"model_hans_classification.ckpt\",\n    n_estimators: int = 4,\n    preprocess_transforms: Tuple[PreprocessorConfig, ...] = (\n        PreprocessorConfig(\n            \"quantile_uni_coarse\",\n            append_original=True,\n            categorical_name=\"ordinal_very_common_categories_shuffled\",\n            global_transformer_name=\"svd\",\n            subsample_features=-1,\n        ),\n        PreprocessorConfig(\n            \"none\", categorical_name=\"numeric\", subsample_features=-1\n        ),\n    ),\n    feature_shift_decoder: str = \"shuffle\",\n    normalize_with_test: bool = False,\n    average_logits: bool = False,\n    optimize_metric: ClassificationOptimizationMetricType = \"roc\",\n    transformer_predict_kwargs: Optional[Dict] = None,\n    multiclass_decoder=\"shuffle\",\n    softmax_temperature: Optional[float] = -0.1,\n    use_poly_features=False,\n    max_poly_features=50,\n    transductive=False,\n    remove_outliers=12.0,\n    add_fingerprint_features=True,\n    subsample_samples=-1,\n    # you can also pass the model directly as torch module and a config dict\n    model: Optional[torch.nn.Module] = None,\n    model_config: Optional[Dict] = None,\n    # The following parameters are not tunable, but specify the execution mode\n    fit_at_predict_time: bool = True,\n    device: tp.Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n    seed: Optional[int] = 0,\n    show_progress: bool = True,\n    batch_size_inference: int = 1,\n    fp16_inference: bool = True,\n    save_peak_memory: Literal[\"True\", \"False\", \"auto\"] = \"True\",\n):\n    \"\"\"\n    Parameters:\n        model_path: The model string is the path to the model.\n        n_estimators: The number of ensemble configurations to use, the most important setting.\n        preprocess_transforms: A tuple of strings, specifying the preprocessing steps to use.\n            You can use the following strings as elements '(none|power|quantile|robust)[_all][_and_none]', where the first\n            part specifies the preprocessing step and the second part specifies the features to apply it to and\n            finally '_and_none' specifies that the original features should be added back to the features in plain.\n            Finally, you can combine all strings without `_all` with `_onehot` to apply one-hot encoding to the categorical\n            features specified with `self.fit(..., categorical_features=...)`.\n        feature_shift_decoder: [\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.\n        normalize_with_test: If True, the test set is used to normalize the data, otherwise the training set is used only.\n        average_logits: Whether to average logits or probabilities for ensemble members.\n        optimize_metric: The optimization metric to use.\n        transformer_predict_kwargs: Additional keyword arguments to pass to the transformer predict method.\n        multiclass_decoder: The multiclass decoder to use.\n        softmax_temperature: A log spaced temperature, it will be applied as logits &lt;- logits/exp(softmax_temperature).\n        use_poly_features: Whether to use polynomial features as the last preprocessing step.\n        max_poly_features: Maximum number of polynomial features to use, None means unlimited.\n        transductive: Whether to use transductive learning.\n        remove_outliers: If not 0.0, will remove outliers from the input features, where values with a standard deviation\n            larger than remove_outliers will be removed.\n        add_fingerprint_features: If True, will add one feature of random values, that will be added to\n            the input features. This helps discern duplicated samples in the transformer model.\n        subsample_samples: If not None, will use a random subset of the samples for training in each ensemble configuration.\n            If 1 or above, this will subsample to the specified number of samples.\n            If in 0 to 1, the value is viewed as a fraction of the training set size.\n        model: The model, if you want to specify it directly, this is used in combination with model_config.\n        model_config: The config, if you want to specify it directly, this is used in combination with model.\n        fit_at_predict_time: Whether to train the model lazily, i.e. only when it is needed for inference in predict[_proba].\n        device: The device to use for inference, \"auto\" means that it will use cuda if available, otherwise cpu.\n        seed: The default seed to use for the order of the ensemble configurations, a seed of None will not.\n        show_progress: Whether to show progress bars during training and inference.\n        batch_size_inference: The batch size to use for inference, this does not affect the results, just the\n            memory usage and speed. A higher batch size is faster but uses more memory. Setting the batch size to None\n            means that the batch size is automatically determined based on the memory usage and the maximum free memory\n            specified with `maximum_free_memory_in_gb`.\n        fp16_inference: Whether to use fp16 for inference on GPU, does not affect CPU inference.\n        save_peak_memory: Whether to save the peak memory usage of the model, can enable up to 8 times larger datasets to fit into memory.\n            \"True\", means always enabled, \"False\", means always disabled, \"auto\" means that it will be set based on the memory usage.\n    \"\"\"\n    assert optimize_metric in tp.get_args(self.metric_type)\n    self.multiclass_decoder = multiclass_decoder\n\n    # Pass all parameters to super class constructor\n    super().__init__(\n        model=model,\n        device=device,\n        model_path=model_path,\n        batch_size_inference=batch_size_inference,\n        fp16_inference=fp16_inference,\n        model_config=model_config,\n        n_estimators=n_estimators,\n        preprocess_transforms=preprocess_transforms,\n        feature_shift_decoder=feature_shift_decoder,\n        normalize_with_test=normalize_with_test,\n        average_logits=average_logits,\n        optimize_metric=optimize_metric,\n        seed=seed,\n        transformer_predict_kwargs=transformer_predict_kwargs,\n        show_progress=show_progress,\n        softmax_temperature=softmax_temperature,\n        save_peak_memory=save_peak_memory,\n        use_poly_features=use_poly_features,\n        max_poly_features=max_poly_features,\n        transductive=transductive,\n        remove_outliers=remove_outliers,\n        add_fingerprint_features=add_fingerprint_features,\n        subsample_samples=subsample_samples,\n        fit_at_predict_time=fit_at_predict_time,\n    )\n</code></pre>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.fit","title":"fit","text":"<pre><code>fit(X, y, additional_y=None) -&gt; TabPFNClassifier\n</code></pre> <p>Fits the TabPFNClassifier model to the input data <code>X</code> and <code>y</code>.</p> <p>The actual training logic is delegated to the <code>_fit</code> method, which should be implemented by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, Tensor]</code> <p>The input feature matrix of shape (n_samples, n_features).</p> required <code>y</code> <code>Union[ndarray, Tensor]</code> <p>The target labels of shape (n_samples,).</p> required <code>additional_y</code> <code>Optional[Dict[str, Tensor]]</code> <p>Additional labels to use during training.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TabPFNClassifier</code> <code>TabPFNClassifier</code> <p>The fitted model object (self).</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def fit(self, X, y, additional_y=None) -&gt; TabPFNClassifier:\n    \"\"\"\n    Fits the TabPFNClassifier model to the input data `X` and `y`.\n\n    The actual training logic is delegated to the `_fit` method, which should be implemented by subclasses.\n\n    Parameters:\n        X (Union[ndarray, torch.Tensor]): The input feature matrix of shape (n_samples, n_features).\n        y (Union[ndarray, torch.Tensor]): The target labels of shape (n_samples,).\n        additional_y (Optional[Dict[str, torch.Tensor]]): Additional labels to use during training.\n\n    Returns:\n        TabPFNClassifier: The fitted model object (self).\n    \"\"\"\n    return super().fit(X, y, additional_y)\n</code></pre>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X, return_winning_probability=False)\n</code></pre> <p>Predict the class labels for the input samples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The input samples.</p> required <code>return_winning_probability</code> <code>bool</code> <p>Whether to return the winning probability.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>array</code> <p>The predicted class labels.</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def predict(self, X, return_winning_probability=False):\n    \"\"\"\n    Predict the class labels for the input samples.\n\n    Parameters:\n        X (array-like): The input samples.\n        return_winning_probability (bool): Whether to return the winning probability.\n\n    Returns:\n        array: The predicted class labels.\n    \"\"\"\n    p = self.predict_proba(X)\n    y = np.argmax(p, axis=-1)\n    y = self.classes_.take(np.asarray(y, dtype=int))\n    if return_winning_probability:\n        return y, p.max(axis=-1)\n    return y\n</code></pre>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X, additional_y=None)\n</code></pre> <p>Calls the transformer to predict the probabilities of the classes of the X test inputs given the previous set training dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>test datapoints</p> required Source code in <code>scripts/estimator/base.py</code> <pre><code>def predict_proba(self, X, additional_y=None):\n    \"\"\"\n    Calls the transformer to predict the probabilities of the classes of the X test inputs given the previous set\n    training dataset\n\n    Parameters:\n        X: test datapoints\n    \"\"\"\n    return self.predict_full(X, additional_y=additional_y)[\"proba\"]\n</code></pre>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.predict_y_proba","title":"predict_y_proba","text":"<pre><code>predict_y_proba(X, y)\n</code></pre> <p>Predict the probability of the target labels <code>y</code> given the input samples <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The input samples.</p> required <code>y</code> <code>array - like</code> <p>The target labels.</p> required <p>Returns:</p> Name Type Description <code>array</code> <p>The predicted probabilities of the target labels.</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def predict_y_proba(self, X, y):\n    \"\"\"\n    Predict the probability of the target labels `y` given the input samples `X`.\n\n    Parameters:\n        X (array-like): The input samples.\n        y (array-like): The target labels.\n\n    Returns:\n        array: The predicted probabilities of the target labels.\n    \"\"\"\n    prediction = self.predict_proba(X)\n    y_prob = prediction[np.arange(len(y)), y.astype(int)]\n    return y_prob\n</code></pre>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features: List[int])\n</code></pre> <p>Set the categorical features to use for the model.</p> <p>These categorical features might be overridden by the preprocessing steps. This is controlled by i) <code>max_unique_values_as_categorical_feature</code>, the maximum number of unique values a feature can have to be considered a categorical feature. Features with more unique values are considered numerical features. ii) <code>min_unique_values_as_numerical_feature</code> the minimum number of unique values a feature can have to be considered a numerical feature. Features with less unique values are considered categorical features.</p> <p>:param categorical_features: The feature indices of the categorical features</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def set_categorical_features(self, categorical_features: List[int]):\n    \"\"\"\n    Set the categorical features to use for the model.\n\n    These categorical features might be overridden by the preprocessing steps.\n    This is controlled by\n    i) `max_unique_values_as_categorical_feature`, the maximum number of unique values\n    a feature can have to be considered a categorical feature. Features with more unique values\n    are considered numerical features.\n    ii) `min_unique_values_as_numerical_feature` the minimum number of unique values\n    a feature can have to be considered a numerical feature. Features with less unique values\n    are considered categorical features.\n\n    :param categorical_features: The feature indices of the categorical features\n    \"\"\"\n    self.categorical_features = categorical_features\n</code></pre>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.score","title":"score","text":"<pre><code>score(X, y, sample_weight=None)\n</code></pre> <p>Compute the score of the model on the given test data and labels.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The input samples.</p> required <code>y</code> <code>array - like</code> <p>The true labels for <code>X</code>.</p> required <code>sample_weight</code> <code>array - like</code> <p>Sample weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The computed score.</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def score(self, X, y, sample_weight=None):\n    \"\"\"\n    Compute the score of the model on the given test data and labels.\n\n    Parameters:\n        X (array-like): The input samples.\n        y (array-like): The true labels for `X`.\n        sample_weight (array-like, optional): Sample weights.\n\n    Returns:\n        float: The computed score.\n    \"\"\"\n    if self.optimize_metric in [\"roc\", \"auroc\", \"log_loss\"]:\n        pred = self.predict_proba(X)\n\n        if len(np.unique(y)) == 2:\n            pred = pred[:, 1]\n    else:\n        pred = self.predict(X)\n\n    return score_classification(\n        self.optimize_metric, y, pred, sample_weight=sample_weight\n    )\n</code></pre>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.estimate_memory_usage","title":"estimate_memory_usage","text":"<pre><code>estimate_memory_usage(X: ndarray | tensor, unit: Literal['b', 'mb', 'gb'] = 'gb', **overwrite_params) -&gt; float | None\n</code></pre> <p>Estimates the memory usage of the model.</p> <p>Peak memory usage is accurate for \u00b4save_peak_mem_factor\u00b4 in O(n_feats, n_samples) on average but with significant outliers (2x). Also this calculation does not include baseline usage and constant offsets. Baseline memory usage can be ignored if we set the maximum memory usage to the default None which uses the free memory of the system. The constant offsets are not significant for large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if <code>self.fit_at_predict_time</code> and train only otherwise. If you add a batch dimension at position 1 to the table this is used as the batch size used during inference, otherwise this depends on the <code>batch_size_inference</code> and <code>n_estimators</code>.</p> required <code>unit</code> <code>Literal['b', 'mb', 'gb']</code> <p>The unit to return the memory usage in (bytes, megabytes, or gigabytes).</p> <code>'gb'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated memory usage in bytes.</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def estimate_memory_usage(\n    self,\n    X: np.ndarray | torch.tensor,\n    unit: Literal[\"b\", \"mb\", \"gb\"] = \"gb\",\n    **overwrite_params,\n) -&gt; float | None:\n    \"\"\"\n    Estimates the memory usage of the model.\n\n    Peak memory usage is accurate for \u00b4save_peak_mem_factor\u00b4 in O(n_feats, n_samples) on average but with\n    significant outliers (2x). Also this calculation does not include baseline usage and constant offsets.\n    Baseline memory usage can be ignored if we set the maximum memory usage to the default None which uses\n    the free memory of the system. The constant offsets are not significant for large datasets.\n\n    Parameters:\n        X (ndarray): The feature matrix. X should represent the concat of train and test in if\n            `self.fit_at_predict_time` and train only otherwise. If you add a batch dimension at position 1 to the\n            table this is used as the batch size used during inference, otherwise this depends on the\n            `batch_size_inference` and `n_estimators`.\n        unit (Literal[\"b\", \"mb\", \"gb\"]): The unit to return the memory usage in (bytes, megabytes, or gigabytes).\n\n    Returns:\n        int: The estimated memory usage in bytes.\n    \"\"\"\n    byte_usage = self._estimate_model_usage(X, \"memory\", **overwrite_params)\n    if byte_usage is None:\n        return None\n\n    if unit == \"mb\":\n        return byte_usage / 1e6\n    elif unit == \"gb\":\n        return byte_usage / 1e9\n    elif unit == \"b\":\n        return byte_usage\n    else:\n        raise ValueError(f\"Unknown unit {unit}\")\n</code></pre>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.estimate_computation_usage","title":"estimate_computation_usage","text":"<pre><code>estimate_computation_usage(X: ndarray, unit: Literal['sequential_flops', 's'] = 'sequential_flops', **overwrite_params) -&gt; float | None\n</code></pre> <p>Estimates the sequential computation usage of the model. Those are the operations that are not parallelizable and are the main bottleneck for the computation time.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if</p> required <code>unit</code> <code>str</code> <p>The unit to return the computation usage in.</p> <code>'sequential_flops'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated computation usage in unit of choice.</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def estimate_computation_usage(\n    self,\n    X: np.ndarray,\n    unit: Literal[\"sequential_flops\", \"s\"] = \"sequential_flops\",\n    **overwrite_params,\n) -&gt; float | None:\n    \"\"\"\n    Estimates the sequential computation usage of the model. Those are the operations that are not parallelizable\n    and are the main bottleneck for the computation time.\n\n    Parameters:\n        X (ndarray): The feature matrix. X should represent the concat of train and test in if\n        ` self.fit_at_predict_time` and train only otherwise. If you add a batch dimension at position 1 to the\n          table this is used as the batch size used during inference, otherwise this depends on the\n         `batch_size_inference` and `n_estimators`.\n        unit (str): The unit to return the computation usage in.\n\n    Returns:\n        int: The estimated computation usage in unit of choice.\n    \"\"\"\n    computation = self._estimate_model_usage(X, \"computation\", **overwrite_params)\n    if unit == \"s\":\n        computation = computation / self.get_processor_speed() / 2e11\n    return computation\n</code></pre>"},{"location":"api/tabpfn_regressor/","title":"TabPFNRegressor","text":""},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor","title":"TabPFNRegressor","text":"<p>             Bases: <code>TabPFNBaseModel</code>, <code>RegressorMixin</code></p> Source code in <code>scripts/estimator/base.py</code> <pre><code>class TabPFNRegressor(TabPFNBaseModel, RegressorMixin):\n    metric_type = RegressionOptimizationMetricType\n\n    _predict_criterion = None\n\n    data_mean_ = None\n    data_std_ = None\n    criterion_ = None\n\n    predict_function_for_shap = \"predict\"\n\n    def __init__(\n        self,\n        model_path: str = str(\n            Path(local_model_path).resolve() / \"model_hans_regression.ckpt\"\n        ),\n        n_estimators: int = 8,\n        preprocess_transforms: Tuple[PreprocessorConfig, ...] = (\n            PreprocessorConfig(\n                \"quantile_uni\",\n                append_original=True,\n                categorical_name=\"ordinal_very_common_categories_shuffled\",\n                global_transformer_name=\"svd\",\n            ),\n            PreprocessorConfig(\"safepower\", categorical_name=\"onehot\"),\n        ),\n        feature_shift_decoder: str = \"shuffle\",\n        normalize_with_test: bool = False,\n        average_logits: bool = False,\n        optimize_metric: RegressionOptimizationMetricType = \"rmse\",\n        transformer_predict_kwargs: Optional[Dict] = None,\n        softmax_temperature: Optional[float] = -0.1,\n        use_poly_features=False,\n        max_poly_features=50,\n        transductive=False,\n        remove_outliers=-1,\n        regression_y_preprocess_transforms: Optional[\n            Tuple[\n                None\n                | tp.Literal[\n                    \"safepower\",\n                    \"power\",\n                    \"quantile_norm\",\n                ],\n                ...,\n            ]\n        ] = (\n            None,\n            \"safepower\",\n        ),\n        add_fingerprint_features: bool = True,\n        cancel_nan_borders: bool = True,\n        super_bar_dist_averaging: bool = False,\n        subsample_samples: float = -1,\n        # you can also pass the model directly as torch module and a config dict\n        model: Optional[torch.nn.Module] = None,\n        model_config: Optional[Dict] = None,\n        # The following parameters are not tunable, but specify the execution mode\n        fit_at_predict_time: bool = True,\n        device: tp.Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n        seed: Optional[int] = 0,\n        show_progress: bool = True,\n        batch_size_inference: int = 1,\n        fp16_inference: bool = True,\n        save_peak_memory: Literal[\"True\", \"False\", \"auto\"] = \"True\",\n    ):\n        \"\"\"\n        Parameters:\n            model_path: The model string is the path to the model.\n            n_estimators: The number of ensemble configurations to use, the most important setting.\n            preprocess_transforms: A tuple of strings, specifying the preprocessing steps to use.\n                You can use the following strings as elements '(none|power|quantile_norm|quantile_uni|quantile_uni_coarse|robust...)[_all][_and_none]', where the first\n                part specifies the preprocessing step (see `.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()`) and the second part specifies the features to apply it to and\n                finally '_and_none' specifies that the original features should be added back to the features in plain.\n                Finally, you can combine all strings without `_all` with `_onehot` to apply one-hot encoding to the categorical\n                features specified with `self.fit(..., categorical_features=...)`.\n            feature_shift_decoder: [\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.\n            normalize_with_test: If True, the test set is used to normalize the data, otherwise the training set is used only.\n            average_logits: Whether to average logits or probabilities for ensemble members.\n            optimize_metric: The optimization metric to use.\n            transformer_predict_kwargs: Additional keyword arguments to pass to the transformer predict method.\n            softmax_temperature: A log spaced temperature, it will be applied as logits &lt;- logits/exp(softmax_temperature).\n            use_poly_features: Whether to use polynomial features as the last preprocessing step.\n            max_poly_features: Maximum number of polynomial features to use, None means unlimited.\n            transductive: Whether to use transductive learning.\n            remove_outliers: If not 0.0, will remove outliers from the input features, where values with a standard deviation\n                larger than remove_outliers will be removed.\n            regression_y_preprocess_transforms: Preprocessing transforms for the target variable. This can be one from `.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()`, e.g. \"power\".\n                This can also be None to not transform the targets, beside a simple mean/variance normalization.\n            add_fingerprint_features: If True, will add one feature of random values, that will be added to\n                the input features. This helps discern duplicated samples in the transformer model.\n            cancel_nan_borders: Whether to ignore buckets that are tranformed to nan values by inverting a `regression_y_preprocess_transform`.\n                This should be set to True, only set this to False if you know what you are doing.\n            super_bar_dist_averaging: If we use `regression_y_preprocess_transforms` we need to average the predictions over the different configurations.\n                The different configurations all come with different bar_distributions (Riemann distributions), though.\n                The default is for us to aggregate all bar distributions using simply scaled borders in the bar distribution, scaled by the mean and std of the target variable.\n                If you set this to True, a new bar distribution will be built using all the borders generated in the different configurations.\n            subsample_samples: If not None, will use a random subset of the samples for training in each ensemble configuration.\n                If 1 or above, this will subsample to the specified number of samples.\n                If in 0 to 1, the value is viewed as a fraction of the training set size.\n            model: The model, if you want to specify it directly, this is used in combination with model_config.\n            model_config: The config, if you want to specify it directly, this is used in combination with model.\n            fit_at_predict_time: Whether to train the model lazily, i.e. only when it is needed for inference in predict[_proba].\n            device: The device to use for inference, \"auto\" means that it will use cuda if available, otherwise cpu.\n            seed: The default seed to use for the order of the ensemble configurations, a seed of None will not.\n            show_progress: Whether to show progress bars during training and inference.\n            batch_size_inference: The batch size to use for inference, this does not affect the results, just the\n                memory usage and speed. A higher batch size is faster but uses more memory. Setting the batch size to None\n                means that the batch size is automatically determined based on the memory usage and the maximum free memory\n                specified with `maximum_free_memory_in_gb`.\n            fp16_inference: Whether to use fp16 for inference on GPU, does not affect CPU inference.\n            save_peak_memory: Whether to save the peak memory usage of the model, can enable up to 8 times larger datasets to fit into memory.\n                \"True\", means always enabled, \"False\", means always disabled, \"auto\" means that it will be set based on the memory usage.\n        \"\"\"\n        # print(f\"{optimize_metric=}\")\n        assert optimize_metric in tp.get_args(\n            self.metric_type\n        ), f\"Unknown metric {optimize_metric}\"\n\n        self.regression_y_preprocess_transforms = regression_y_preprocess_transforms\n        self.cancel_nan_borders = cancel_nan_borders\n        self.super_bar_dist_averaging = super_bar_dist_averaging\n\n        super().__init__(\n            model=model,\n            device=device,\n            model_path=model_path,\n            batch_size_inference=batch_size_inference,\n            fp16_inference=fp16_inference,\n            model_config=model_config,\n            n_estimators=n_estimators,\n            preprocess_transforms=preprocess_transforms,\n            feature_shift_decoder=feature_shift_decoder,\n            normalize_with_test=normalize_with_test,\n            average_logits=average_logits,\n            seed=seed,\n            optimize_metric=optimize_metric,\n            transformer_predict_kwargs=transformer_predict_kwargs,\n            show_progress=show_progress,\n            save_peak_memory=save_peak_memory,\n            softmax_temperature=softmax_temperature,\n            use_poly_features=use_poly_features,\n            max_poly_features=max_poly_features,\n            transductive=transductive,\n            remove_outliers=remove_outliers,\n            add_fingerprint_features=add_fingerprint_features,\n            fit_at_predict_time=fit_at_predict_time,\n            subsample_samples=subsample_samples,\n        )\n\n    def get_optimization_mode(self):\n        if self.optimize_metric is None:\n            return \"mean\"\n        elif self.optimize_metric in [\"rmse\", \"mse\", \"r2\", \"mean\"]:\n            return \"mean\"\n        elif self.optimize_metric in [\"mae\", \"median\"]:\n            return \"median\"\n        elif self.optimize_metric in [\"mode\", \"exact_match\"]:\n            return \"mode\"\n        else:\n            raise ValueError(f\"Unknown metric {self.optimize_metric}\")\n\n    def score(self, X, y, sample_weight=None):\n        y_pred = self.predict(X)\n\n        opt_metric = (\n            self.optimize_metric if self.optimize_metric is not None else \"rmse\"\n        )\n\n        return score_regression(opt_metric, y, y_pred, sample_weight=sample_weight)\n\n    def init_model_and_get_model_config(self):\n        super().init_model_and_get_model_config()\n        assert not self.is_classification_, \"This should not be a classification model\"\n\n    def _fit(self):\n        y_train, (self.data_mean_, self.data_std_) = normalize_data(\n            self.y_[:, None].float(),\n            normalize_positions=len(self.y_),\n            return_scaling=True,\n            std_only=self.normalize_std_only_,\n        )\n\n        self.criterion_ = deepcopy(self.model_processed_.criterion)\n\n        self.transformer_predict(\n            self.X_[:, None].float(),\n            y_train,\n            len(self.X_),\n            additional_ys=self.additional_y_,\n            bar_distribution=self.criterion_,\n            cache_trainset_representations=not self.fit_at_predict_time,  # this will always be true here\n            **get_params_from_config(self.c_processed_),\n        )\n\n    def predict(self, X, additional_y=None) -&gt; np.ndarray:\n        timing_start(\"predict\")\n        prediction = self.predict_full(X, additional_y=additional_y)\n        timing_end(\"predict\")\n        if LOG_COMPUTE_TIME_PATH:\n            self._log_compute_time(X)\n\n        if LOG_MEMORY_USAGE_PATH:\n            self._log_memory_usage(self.X_, self.X_.shape[0])\n\n        return prediction[self.get_optimization_mode()]\n\n    def predict_y_proba(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predicts the probability of the target y given the input X.\n        \"\"\"\n        prediction = self.predict_full(X)\n        return prediction[\"criterion\"].pdf(\n            torch.tensor(prediction[\"logits\"]), torch.tensor(y)\n        )\n\n    def predict_full(self, X, additional_y=None, get_additional_outputs=None) -&gt; dict:\n        \"\"\"\n        Predicts the target y given the input X.\n\n        Parameters:\n            X:\n            additional_y: Additional inputs\n            get_additional_outputs: Keys for additional outputs to return.\n\n\n        Returns:\n             (dict: The predictions, dict: Additional outputs)\n        \"\"\"\n        X_full, y_full, additional_y, eval_position = self.predict_common_setup(\n            X, additional_y_eval=additional_y\n        )\n\n        y_full, (data_mean, data_std) = normalize_data(\n            y_full,\n            normalize_positions=eval_position,\n            return_scaling=True,\n            std_only=self.normalize_std_only_,\n            mean=None if self.fit_at_predict_time else self.data_mean_,\n            std=None if self.fit_at_predict_time else self.data_std_,\n            clip=False,\n        )\n\n        if self.fit_at_predict_time:\n            criterion = deepcopy(self.model_processed_.criterion)\n        else:\n            criterion = self.criterion_\n\n        prediction, additional_outputs = self.transformer_predict(\n            eval_xs=X_full,\n            eval_ys=y_full,\n            eval_position=eval_position if self.fit_at_predict_time else 0,\n            additional_ys=additional_y,\n            bar_distribution=criterion,\n            cache_trainset_representations=not self.fit_at_predict_time,\n            get_additional_outputs=get_additional_outputs,\n            **get_params_from_config(self.c_processed_),\n        )\n        self._predict_criterion = self._post_process_predict_criterion(\n            criterion=criterion, data_mean=data_mean, data_std=data_std\n        )\n        return {\n            **self._post_process_predict_full(\n                prediction=prediction,\n                criterion=self._predict_criterion,\n            ),\n            **additional_outputs,\n        }\n\n    @staticmethod\n    def _post_process_predict_full(\n        prediction: torch.Tensor,\n        criterion: FullSupportBarDistribution,\n    ) -&gt; dict:\n        prediction_ = prediction.squeeze(0)\n\n        predictions = {\n            \"criterion\": criterion.cpu(),\n            \"mean\": criterion.mean(prediction_.cpu()).detach().numpy(),\n            \"median\": criterion.median(prediction_.cpu()).detach().numpy(),\n            \"mode\": criterion.mode(prediction_.cpu()).detach().numpy(),\n            \"logits\": prediction_.cpu().detach().numpy(),\n            \"buckets\": torch.nn.functional.softmax(prediction_.cpu(), dim=-1)\n            .detach()\n            .numpy(),\n        }\n\n        predictions.update(\n            {\n                f\"quantile_{q:.2f}\": criterion.icdf(prediction_.cpu(), q)\n                .detach()\n                .numpy()\n                for q in tuple(i / 10 for i in range(1, 10))\n            }\n        )\n\n        return predictions\n\n    def _post_process_predict_criterion(\n        self, criterion: FullSupportBarDistribution, data_mean, data_std\n    ) -&gt; FullSupportBarDistribution:\n        data_mean_added = (\n            data_mean.to(criterion.borders.device)\n            if not self.normalize_std_only_\n            else 0\n        )\n        criterion.borders = (\n            criterion.borders * data_std.to(criterion.borders.device) + data_mean_added\n        ).float()\n        return criterion\n</code></pre>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.__init__","title":"__init__","text":"<pre><code>__init__(model_path: str = str(Path(local_model_path).resolve() / 'model_hans_regression.ckpt'), n_estimators: int = 8, preprocess_transforms: Tuple[PreprocessorConfig, ...] = (PreprocessorConfig('quantile_uni', append_original=True, categorical_name='ordinal_very_common_categories_shuffled', global_transformer_name='svd'), PreprocessorConfig('safepower', categorical_name='onehot')), feature_shift_decoder: str = 'shuffle', normalize_with_test: bool = False, average_logits: bool = False, optimize_metric: RegressionOptimizationMetricType = 'rmse', transformer_predict_kwargs: Optional[Dict] = None, softmax_temperature: Optional[float] = -0.1, use_poly_features=False, max_poly_features=50, transductive=False, remove_outliers=-1, regression_y_preprocess_transforms: Optional[Tuple[None | Literal['safepower', 'power', 'quantile_norm'], ...]] = (None, 'safepower'), add_fingerprint_features: bool = True, cancel_nan_borders: bool = True, super_bar_dist_averaging: bool = False, subsample_samples: float = -1, model: Optional[Module] = None, model_config: Optional[Dict] = None, fit_at_predict_time: bool = True, device: Literal['cuda', 'cpu', 'auto'] = 'auto', seed: Optional[int] = 0, show_progress: bool = True, batch_size_inference: int = 1, fp16_inference: bool = True, save_peak_memory: Literal['True', 'False', 'auto'] = 'True')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The model string is the path to the model.</p> <code>str(resolve() / 'model_hans_regression.ckpt')</code> <code>n_estimators</code> <code>int</code> <p>The number of ensemble configurations to use, the most important setting.</p> <code>8</code> <code>preprocess_transforms</code> <code>Tuple[PreprocessorConfig, ...]</code> <p>A tuple of strings, specifying the preprocessing steps to use. You can use the following strings as elements '(none|power|quantile_norm|quantile_uni|quantile_uni_coarse|robust...)_all', where the first part specifies the preprocessing step (see <code>.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()</code>) and the second part specifies the features to apply it to and finally '_and_none' specifies that the original features should be added back to the features in plain. Finally, you can combine all strings without <code>_all</code> with <code>_onehot</code> to apply one-hot encoding to the categorical features specified with <code>self.fit(..., categorical_features=...)</code>.</p> <code>(PreprocessorConfig('quantile_uni', append_original=True, categorical_name='ordinal_very_common_categories_shuffled', global_transformer_name='svd'), PreprocessorConfig('safepower', categorical_name='onehot'))</code> <code>feature_shift_decoder</code> <code>str</code> <p>[\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.</p> <code>'shuffle'</code> <code>normalize_with_test</code> <code>bool</code> <p>If True, the test set is used to normalize the data, otherwise the training set is used only.</p> <code>False</code> <code>average_logits</code> <code>bool</code> <p>Whether to average logits or probabilities for ensemble members.</p> <code>False</code> <code>optimize_metric</code> <code>RegressionOptimizationMetricType</code> <p>The optimization metric to use.</p> <code>'rmse'</code> <code>transformer_predict_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments to pass to the transformer predict method.</p> <code>None</code> <code>softmax_temperature</code> <code>Optional[float]</code> <p>A log spaced temperature, it will be applied as logits &lt;- logits/exp(softmax_temperature).</p> <code>-0.1</code> <code>use_poly_features</code> <p>Whether to use polynomial features as the last preprocessing step.</p> <code>False</code> <code>max_poly_features</code> <p>Maximum number of polynomial features to use, None means unlimited.</p> <code>50</code> <code>transductive</code> <p>Whether to use transductive learning.</p> <code>False</code> <code>remove_outliers</code> <p>If not 0.0, will remove outliers from the input features, where values with a standard deviation larger than remove_outliers will be removed.</p> <code>-1</code> <code>regression_y_preprocess_transforms</code> <code>Optional[Tuple[None | Literal['safepower', 'power', 'quantile_norm'], ...]]</code> <p>Preprocessing transforms for the target variable. This can be one from <code>.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()</code>, e.g. \"power\". This can also be None to not transform the targets, beside a simple mean/variance normalization.</p> <code>(None, 'safepower')</code> <code>add_fingerprint_features</code> <code>bool</code> <p>If True, will add one feature of random values, that will be added to the input features. This helps discern duplicated samples in the transformer model.</p> <code>True</code> <code>cancel_nan_borders</code> <code>bool</code> <p>Whether to ignore buckets that are tranformed to nan values by inverting a <code>regression_y_preprocess_transform</code>. This should be set to True, only set this to False if you know what you are doing.</p> <code>True</code> <code>super_bar_dist_averaging</code> <code>bool</code> <p>If we use <code>regression_y_preprocess_transforms</code> we need to average the predictions over the different configurations. The different configurations all come with different bar_distributions (Riemann distributions), though. The default is for us to aggregate all bar distributions using simply scaled borders in the bar distribution, scaled by the mean and std of the target variable. If you set this to True, a new bar distribution will be built using all the borders generated in the different configurations.</p> <code>False</code> <code>subsample_samples</code> <code>float</code> <p>If not None, will use a random subset of the samples for training in each ensemble configuration. If 1 or above, this will subsample to the specified number of samples. If in 0 to 1, the value is viewed as a fraction of the training set size.</p> <code>-1</code> <code>model</code> <code>Optional[Module]</code> <p>The model, if you want to specify it directly, this is used in combination with model_config.</p> <code>None</code> <code>model_config</code> <code>Optional[Dict]</code> <p>The config, if you want to specify it directly, this is used in combination with model.</p> <code>None</code> <code>fit_at_predict_time</code> <code>bool</code> <p>Whether to train the model lazily, i.e. only when it is needed for inference in predict[_proba].</p> <code>True</code> <code>device</code> <code>Literal['cuda', 'cpu', 'auto']</code> <p>The device to use for inference, \"auto\" means that it will use cuda if available, otherwise cpu.</p> <code>'auto'</code> <code>seed</code> <code>Optional[int]</code> <p>The default seed to use for the order of the ensemble configurations, a seed of None will not.</p> <code>0</code> <code>show_progress</code> <code>bool</code> <p>Whether to show progress bars during training and inference.</p> <code>True</code> <code>batch_size_inference</code> <code>int</code> <p>The batch size to use for inference, this does not affect the results, just the memory usage and speed. A higher batch size is faster but uses more memory. Setting the batch size to None means that the batch size is automatically determined based on the memory usage and the maximum free memory specified with <code>maximum_free_memory_in_gb</code>.</p> <code>1</code> <code>fp16_inference</code> <code>bool</code> <p>Whether to use fp16 for inference on GPU, does not affect CPU inference.</p> <code>True</code> <code>save_peak_memory</code> <code>Literal['True', 'False', 'auto']</code> <p>Whether to save the peak memory usage of the model, can enable up to 8 times larger datasets to fit into memory. \"True\", means always enabled, \"False\", means always disabled, \"auto\" means that it will be set based on the memory usage.</p> <code>'True'</code> Source code in <code>scripts/estimator/base.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = str(\n        Path(local_model_path).resolve() / \"model_hans_regression.ckpt\"\n    ),\n    n_estimators: int = 8,\n    preprocess_transforms: Tuple[PreprocessorConfig, ...] = (\n        PreprocessorConfig(\n            \"quantile_uni\",\n            append_original=True,\n            categorical_name=\"ordinal_very_common_categories_shuffled\",\n            global_transformer_name=\"svd\",\n        ),\n        PreprocessorConfig(\"safepower\", categorical_name=\"onehot\"),\n    ),\n    feature_shift_decoder: str = \"shuffle\",\n    normalize_with_test: bool = False,\n    average_logits: bool = False,\n    optimize_metric: RegressionOptimizationMetricType = \"rmse\",\n    transformer_predict_kwargs: Optional[Dict] = None,\n    softmax_temperature: Optional[float] = -0.1,\n    use_poly_features=False,\n    max_poly_features=50,\n    transductive=False,\n    remove_outliers=-1,\n    regression_y_preprocess_transforms: Optional[\n        Tuple[\n            None\n            | tp.Literal[\n                \"safepower\",\n                \"power\",\n                \"quantile_norm\",\n            ],\n            ...,\n        ]\n    ] = (\n        None,\n        \"safepower\",\n    ),\n    add_fingerprint_features: bool = True,\n    cancel_nan_borders: bool = True,\n    super_bar_dist_averaging: bool = False,\n    subsample_samples: float = -1,\n    # you can also pass the model directly as torch module and a config dict\n    model: Optional[torch.nn.Module] = None,\n    model_config: Optional[Dict] = None,\n    # The following parameters are not tunable, but specify the execution mode\n    fit_at_predict_time: bool = True,\n    device: tp.Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n    seed: Optional[int] = 0,\n    show_progress: bool = True,\n    batch_size_inference: int = 1,\n    fp16_inference: bool = True,\n    save_peak_memory: Literal[\"True\", \"False\", \"auto\"] = \"True\",\n):\n    \"\"\"\n    Parameters:\n        model_path: The model string is the path to the model.\n        n_estimators: The number of ensemble configurations to use, the most important setting.\n        preprocess_transforms: A tuple of strings, specifying the preprocessing steps to use.\n            You can use the following strings as elements '(none|power|quantile_norm|quantile_uni|quantile_uni_coarse|robust...)[_all][_and_none]', where the first\n            part specifies the preprocessing step (see `.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()`) and the second part specifies the features to apply it to and\n            finally '_and_none' specifies that the original features should be added back to the features in plain.\n            Finally, you can combine all strings without `_all` with `_onehot` to apply one-hot encoding to the categorical\n            features specified with `self.fit(..., categorical_features=...)`.\n        feature_shift_decoder: [\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.\n        normalize_with_test: If True, the test set is used to normalize the data, otherwise the training set is used only.\n        average_logits: Whether to average logits or probabilities for ensemble members.\n        optimize_metric: The optimization metric to use.\n        transformer_predict_kwargs: Additional keyword arguments to pass to the transformer predict method.\n        softmax_temperature: A log spaced temperature, it will be applied as logits &lt;- logits/exp(softmax_temperature).\n        use_poly_features: Whether to use polynomial features as the last preprocessing step.\n        max_poly_features: Maximum number of polynomial features to use, None means unlimited.\n        transductive: Whether to use transductive learning.\n        remove_outliers: If not 0.0, will remove outliers from the input features, where values with a standard deviation\n            larger than remove_outliers will be removed.\n        regression_y_preprocess_transforms: Preprocessing transforms for the target variable. This can be one from `.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()`, e.g. \"power\".\n            This can also be None to not transform the targets, beside a simple mean/variance normalization.\n        add_fingerprint_features: If True, will add one feature of random values, that will be added to\n            the input features. This helps discern duplicated samples in the transformer model.\n        cancel_nan_borders: Whether to ignore buckets that are tranformed to nan values by inverting a `regression_y_preprocess_transform`.\n            This should be set to True, only set this to False if you know what you are doing.\n        super_bar_dist_averaging: If we use `regression_y_preprocess_transforms` we need to average the predictions over the different configurations.\n            The different configurations all come with different bar_distributions (Riemann distributions), though.\n            The default is for us to aggregate all bar distributions using simply scaled borders in the bar distribution, scaled by the mean and std of the target variable.\n            If you set this to True, a new bar distribution will be built using all the borders generated in the different configurations.\n        subsample_samples: If not None, will use a random subset of the samples for training in each ensemble configuration.\n            If 1 or above, this will subsample to the specified number of samples.\n            If in 0 to 1, the value is viewed as a fraction of the training set size.\n        model: The model, if you want to specify it directly, this is used in combination with model_config.\n        model_config: The config, if you want to specify it directly, this is used in combination with model.\n        fit_at_predict_time: Whether to train the model lazily, i.e. only when it is needed for inference in predict[_proba].\n        device: The device to use for inference, \"auto\" means that it will use cuda if available, otherwise cpu.\n        seed: The default seed to use for the order of the ensemble configurations, a seed of None will not.\n        show_progress: Whether to show progress bars during training and inference.\n        batch_size_inference: The batch size to use for inference, this does not affect the results, just the\n            memory usage and speed. A higher batch size is faster but uses more memory. Setting the batch size to None\n            means that the batch size is automatically determined based on the memory usage and the maximum free memory\n            specified with `maximum_free_memory_in_gb`.\n        fp16_inference: Whether to use fp16 for inference on GPU, does not affect CPU inference.\n        save_peak_memory: Whether to save the peak memory usage of the model, can enable up to 8 times larger datasets to fit into memory.\n            \"True\", means always enabled, \"False\", means always disabled, \"auto\" means that it will be set based on the memory usage.\n    \"\"\"\n    # print(f\"{optimize_metric=}\")\n    assert optimize_metric in tp.get_args(\n        self.metric_type\n    ), f\"Unknown metric {optimize_metric}\"\n\n    self.regression_y_preprocess_transforms = regression_y_preprocess_transforms\n    self.cancel_nan_borders = cancel_nan_borders\n    self.super_bar_dist_averaging = super_bar_dist_averaging\n\n    super().__init__(\n        model=model,\n        device=device,\n        model_path=model_path,\n        batch_size_inference=batch_size_inference,\n        fp16_inference=fp16_inference,\n        model_config=model_config,\n        n_estimators=n_estimators,\n        preprocess_transforms=preprocess_transforms,\n        feature_shift_decoder=feature_shift_decoder,\n        normalize_with_test=normalize_with_test,\n        average_logits=average_logits,\n        seed=seed,\n        optimize_metric=optimize_metric,\n        transformer_predict_kwargs=transformer_predict_kwargs,\n        show_progress=show_progress,\n        save_peak_memory=save_peak_memory,\n        softmax_temperature=softmax_temperature,\n        use_poly_features=use_poly_features,\n        max_poly_features=max_poly_features,\n        transductive=transductive,\n        remove_outliers=remove_outliers,\n        add_fingerprint_features=add_fingerprint_features,\n        fit_at_predict_time=fit_at_predict_time,\n        subsample_samples=subsample_samples,\n    )\n</code></pre>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.fit","title":"fit","text":"<pre><code>fit(X, y, additional_y=None) -&gt; TabPFNBaseModel\n</code></pre> <p>Fits the model to the input data <code>X</code> and <code>y</code>.</p> <p>The actual training logic is delegated to the <code>_fit</code> method, which should be implemented by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, Tensor]</code> <p>The input feature matrix of shape (n_samples, n_features).</p> required <code>y</code> <code>Union[ndarray, Tensor]</code> <p>The target labels of shape (n_samples,).</p> required <code>additional_y</code> <code>Optional[Dict[str, Tensor]]</code> <p>Additional labels to use during training.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TabPFNBaseModel</code> <code>TabPFNBaseModel</code> <p>The fitted model object (self).</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def fit(self, X, y, additional_y=None) -&gt; TabPFNBaseModel:\n    \"\"\"\n    Fits the model to the input data `X` and `y`.\n\n    The actual training logic is delegated to the `_fit` method, which should be implemented by subclasses.\n\n    Parameters:\n        X (Union[ndarray, torch.Tensor]): The input feature matrix of shape (n_samples, n_features).\n        y (Union[ndarray, torch.Tensor]): The target labels of shape (n_samples,).\n        additional_y (Optional[Dict[str, torch.Tensor]]): Additional labels to use during training.\n\n    Returns:\n        TabPFNBaseModel: The fitted model object (self).\n    \"\"\"\n    timing_clear()\n    timing_start(\"fit\")\n\n    # TODO: Transform input data to ordinals if not numbers (e.g. strings)\n\n    # Prediction fit caching\n    if self.cached_models_:\n        for m in self.cached_models_:\n            m.empty_trainset_representation_cache()\n    self.cached_preprocessors_ = None\n    self.cached_models_ = None\n    self.cached_shuffles_ = None\n\n    # Must not modify any input parameters\n    self.init_model_and_get_model_config()\n\n    if isinstance(X, torch.Tensor):\n        X = X.numpy()\n    if isinstance(y, torch.Tensor):\n        y = y.numpy()\n\n    X, y = self.check_training_data(self, X, y)\n    self.n_features_in_ = X.shape[1]\n\n    X = X.astype(np.float32)\n    self.X_ = X if torch.is_tensor(X) else torch.tensor(X)\n    self.y_ = y if torch.is_tensor(y) else torch.tensor(y)\n\n    if additional_y is not None:\n        assert type(additional_y) == dict\n        for k, v in additional_y.items():\n            additional_y[k] = v if torch.is_tensor(v) else torch.tensor(v)\n    self.additional_y_ = additional_y\n\n    self.infer_categorical_features(X)\n\n    if not self.fit_at_predict_time:\n        self._fit()\n\n    timing_end(\"fit\")\n\n    # Return the classifier\n    return self\n</code></pre>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(X, additional_y=None) -&gt; ndarray\n</code></pre> Source code in <code>scripts/estimator/base.py</code> <pre><code>def predict(self, X, additional_y=None) -&gt; np.ndarray:\n    timing_start(\"predict\")\n    prediction = self.predict_full(X, additional_y=additional_y)\n    timing_end(\"predict\")\n    if LOG_COMPUTE_TIME_PATH:\n        self._log_compute_time(X)\n\n    if LOG_MEMORY_USAGE_PATH:\n        self._log_memory_usage(self.X_, self.X_.shape[0])\n\n    return prediction[self.get_optimization_mode()]\n</code></pre>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.predict_y_proba","title":"predict_y_proba","text":"<pre><code>predict_y_proba(X: ndarray, y: ndarray) -&gt; ndarray\n</code></pre> <p>Predicts the probability of the target y given the input X.</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def predict_y_proba(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Predicts the probability of the target y given the input X.\n    \"\"\"\n    prediction = self.predict_full(X)\n    return prediction[\"criterion\"].pdf(\n        torch.tensor(prediction[\"logits\"]), torch.tensor(y)\n    )\n</code></pre>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features: List[int])\n</code></pre> <p>Set the categorical features to use for the model.</p> <p>These categorical features might be overridden by the preprocessing steps. This is controlled by i) <code>max_unique_values_as_categorical_feature</code>, the maximum number of unique values a feature can have to be considered a categorical feature. Features with more unique values are considered numerical features. ii) <code>min_unique_values_as_numerical_feature</code> the minimum number of unique values a feature can have to be considered a numerical feature. Features with less unique values are considered categorical features.</p> <p>:param categorical_features: The feature indices of the categorical features</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def set_categorical_features(self, categorical_features: List[int]):\n    \"\"\"\n    Set the categorical features to use for the model.\n\n    These categorical features might be overridden by the preprocessing steps.\n    This is controlled by\n    i) `max_unique_values_as_categorical_feature`, the maximum number of unique values\n    a feature can have to be considered a categorical feature. Features with more unique values\n    are considered numerical features.\n    ii) `min_unique_values_as_numerical_feature` the minimum number of unique values\n    a feature can have to be considered a numerical feature. Features with less unique values\n    are considered categorical features.\n\n    :param categorical_features: The feature indices of the categorical features\n    \"\"\"\n    self.categorical_features = categorical_features\n</code></pre>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.score","title":"score","text":"<pre><code>score(X, y, sample_weight=None)\n</code></pre> Source code in <code>scripts/estimator/base.py</code> <pre><code>def score(self, X, y, sample_weight=None):\n    y_pred = self.predict(X)\n\n    opt_metric = (\n        self.optimize_metric if self.optimize_metric is not None else \"rmse\"\n    )\n\n    return score_regression(opt_metric, y, y_pred, sample_weight=sample_weight)\n</code></pre>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.estimate_memory_usage","title":"estimate_memory_usage","text":"<pre><code>estimate_memory_usage(X: ndarray | tensor, unit: Literal['b', 'mb', 'gb'] = 'gb', **overwrite_params) -&gt; float | None\n</code></pre> <p>Estimates the memory usage of the model.</p> <p>Peak memory usage is accurate for \u00b4save_peak_mem_factor\u00b4 in O(n_feats, n_samples) on average but with significant outliers (2x). Also this calculation does not include baseline usage and constant offsets. Baseline memory usage can be ignored if we set the maximum memory usage to the default None which uses the free memory of the system. The constant offsets are not significant for large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if <code>self.fit_at_predict_time</code> and train only otherwise. If you add a batch dimension at position 1 to the table this is used as the batch size used during inference, otherwise this depends on the <code>batch_size_inference</code> and <code>n_estimators</code>.</p> required <code>unit</code> <code>Literal['b', 'mb', 'gb']</code> <p>The unit to return the memory usage in (bytes, megabytes, or gigabytes).</p> <code>'gb'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated memory usage in bytes.</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def estimate_memory_usage(\n    self,\n    X: np.ndarray | torch.tensor,\n    unit: Literal[\"b\", \"mb\", \"gb\"] = \"gb\",\n    **overwrite_params,\n) -&gt; float | None:\n    \"\"\"\n    Estimates the memory usage of the model.\n\n    Peak memory usage is accurate for \u00b4save_peak_mem_factor\u00b4 in O(n_feats, n_samples) on average but with\n    significant outliers (2x). Also this calculation does not include baseline usage and constant offsets.\n    Baseline memory usage can be ignored if we set the maximum memory usage to the default None which uses\n    the free memory of the system. The constant offsets are not significant for large datasets.\n\n    Parameters:\n        X (ndarray): The feature matrix. X should represent the concat of train and test in if\n            `self.fit_at_predict_time` and train only otherwise. If you add a batch dimension at position 1 to the\n            table this is used as the batch size used during inference, otherwise this depends on the\n            `batch_size_inference` and `n_estimators`.\n        unit (Literal[\"b\", \"mb\", \"gb\"]): The unit to return the memory usage in (bytes, megabytes, or gigabytes).\n\n    Returns:\n        int: The estimated memory usage in bytes.\n    \"\"\"\n    byte_usage = self._estimate_model_usage(X, \"memory\", **overwrite_params)\n    if byte_usage is None:\n        return None\n\n    if unit == \"mb\":\n        return byte_usage / 1e6\n    elif unit == \"gb\":\n        return byte_usage / 1e9\n    elif unit == \"b\":\n        return byte_usage\n    else:\n        raise ValueError(f\"Unknown unit {unit}\")\n</code></pre>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.estimate_computation_usage","title":"estimate_computation_usage","text":"<pre><code>estimate_computation_usage(X: ndarray, unit: Literal['sequential_flops', 's'] = 'sequential_flops', **overwrite_params) -&gt; float | None\n</code></pre> <p>Estimates the sequential computation usage of the model. Those are the operations that are not parallelizable and are the main bottleneck for the computation time.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if</p> required <code>unit</code> <code>str</code> <p>The unit to return the computation usage in.</p> <code>'sequential_flops'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated computation usage in unit of choice.</p> Source code in <code>scripts/estimator/base.py</code> <pre><code>def estimate_computation_usage(\n    self,\n    X: np.ndarray,\n    unit: Literal[\"sequential_flops\", \"s\"] = \"sequential_flops\",\n    **overwrite_params,\n) -&gt; float | None:\n    \"\"\"\n    Estimates the sequential computation usage of the model. Those are the operations that are not parallelizable\n    and are the main bottleneck for the computation time.\n\n    Parameters:\n        X (ndarray): The feature matrix. X should represent the concat of train and test in if\n        ` self.fit_at_predict_time` and train only otherwise. If you add a batch dimension at position 1 to the\n          table this is used as the batch size used during inference, otherwise this depends on the\n         `batch_size_inference` and `n_estimators`.\n        unit (str): The unit to return the computation usage in.\n\n    Returns:\n        int: The estimated computation usage in unit of choice.\n    \"\"\"\n    computation = self._estimate_model_usage(X, \"computation\", **overwrite_params)\n    if unit == \"s\":\n        computation = computation / self.get_processor_speed() / 2e11\n    return computation\n</code></pre>"},{"location":"api/tabpfn_unsupervised/","title":"Tabpfn unsupervised","text":""},{"location":"api/tabpfn_unsupervised/#scripts.estimator.TabPFNUnsupervisedModel","title":"TabPFNUnsupervisedModel","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>TabPFN unsupervised model for imputation, outlier detection, and synthetic data generation.</p> <p>This model combines a TabPFNClassifier for categorical features and a TabPFNRegressor for numerical features to perform various unsupervised learning tasks on tabular data.</p> <p>Parameters:</p> Name Type Description Default <code>tabpfn_clf</code> <p>TabPFNClassifier, optional TabPFNClassifier instance for handling categorical features. If not provided, the model assumes that there are no categorical features in the data.</p> <code>None</code> <code>tabpfn_reg</code> <p>TabPFNRegressor, optional TabPFNRegressor instance for handling numerical features. If not provided, the model assumes that there are no numerical features in the data.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>categorical_features</code> <p>list List of indices of categorical features in the input data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tabpfn_clf = TabPFNClassifier()\n&gt;&gt;&gt; tabpfn_reg = TabPFNRegressor()\n&gt;&gt;&gt; model = TabPFNUnsupervisedModel(tabpfn_clf, tabpfn_reg)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\n&gt;&gt;&gt; model.fit(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_imputed = model.impute(X)\n&gt;&gt;&gt; X_outliers = model.outliers(X)\n&gt;&gt;&gt; X_synthetic = model.generate_synthetic_data(n_samples=100)\n</code></pre> Source code in <code>scripts/estimator/unsupervised.py</code> <pre><code>class TabPFNUnsupervisedModel(BaseEstimator):\n    \"\"\"TabPFN unsupervised model for imputation, outlier detection, and synthetic data generation.\n\n    This model combines a TabPFNClassifier for categorical features and a TabPFNRegressor for\n    numerical features to perform various unsupervised learning tasks on tabular data.\n\n    Parameters:\n        tabpfn_clf : TabPFNClassifier, optional\n            TabPFNClassifier instance for handling categorical features. If not provided, the model\n            assumes that there are no categorical features in the data.\n\n        tabpfn_reg : TabPFNRegressor, optional\n            TabPFNRegressor instance for handling numerical features. If not provided, the model\n            assumes that there are no numerical features in the data.\n\n    Attributes:\n        categorical_features : list\n            List of indices of categorical features in the input data.\n\n    Examples:\n        &gt;&gt;&gt; tabpfn_clf = TabPFNClassifier()\n        &gt;&gt;&gt; tabpfn_reg = TabPFNRegressor()\n        &gt;&gt;&gt; model = TabPFNUnsupervisedModel(tabpfn_clf, tabpfn_reg)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; X = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\n        &gt;&gt;&gt; model.fit(X)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; X_imputed = model.impute(X)\n        &gt;&gt;&gt; X_outliers = model.outliers(X)\n        &gt;&gt;&gt; X_synthetic = model.generate_synthetic_data(n_samples=100)\n    \"\"\"\n\n    def _more_tags(self):\n        return {\"allow_nan\": True}\n\n    def __init__(\n        self,\n        tabpfn_clf: Optional[TabPFNClassifier] = None,\n        tabpfn_reg: Optional[TabPFNRegressor] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the TabPFNUnsupervisedModel.\n\n        Parameters:\n            tabpfn_clf : TabPFNClassifier, optional\n                TabPFNClassifier instance for handling categorical features. If not provided, the model\n                assumes that there are no categorical features in the data.\n\n            tabpfn_reg : TabPFNRegressor, optional\n                TabPFNRegressor instance for handling numerical features. If not provided, the model\n                assumes that there are no numerical features in the data.\n\n        Raises:\n            AssertionError\n                If both tabpfn_clf and tabpfn_reg are None.\n        \"\"\"\n        assert (\n            tabpfn_clf is not None or tabpfn_reg is not None\n        ), \"You cannot set both `tabpfn_clf` and `tabpfn_reg` to None. You can set one to None, if your table exclusively consists of categoricals/numericals.\"\n\n        self.tabpfn_clf = tabpfn_clf\n        self.tabpfn_reg = tabpfn_reg\n        self.estimators = [self.tabpfn_clf, self.tabpfn_reg]\n\n        self.categorical_features = []\n\n    def set_categorical_features(self, categorical_features):\n        self.categorical_features = categorical_features\n        for estimator in self.estimators:\n            estimator.set_categorical_features(categorical_features)\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; None:\n        \"\"\"Fit the model to the input data.\n\n        Parameters:\n            X : array-like of shape (n_samples, n_features)\n                Input data to fit the model.\n\n            y : array-like of shape (n_samples,), optional\n                Target values.\n\n        Returns:\n            self : TabPFNUnsupervisedModel\n                Fitted model.\n        \"\"\"\n        self.X_ = copy.deepcopy(X)\n        self.y = copy.deepcopy(y)\n        self.tabpfn_clf.infer_categorical_features(X)\n        self.categorical_features = self.tabpfn_clf.categorical_features\n\n    def impute_for_gen(self, X: torch.tensor, t: float = 0.000000001) -&gt; torch.tensor:\n        \"\"\"\n        Impute missing values (np.nan) in X by sampling all cells independently from the trained models\n\n        :param X: Input data of the shape (num_examples, num_features) with missing values encoded as np.nan\n        :param t: Temperature for sampling from the imputation distribution, lower values are more deterministic\n        :return: Imputed data, with missing values replaced\n        \"\"\"\n        train_X = self.X_\n        imputed_X = copy.deepcopy(X)\n        from tqdm import tqdm\n\n        categorical_features = self.tabpfn_clf.infer_categorical_features(self.X_)\n\n        for column_idx in tqdm(range(0, X.shape[1])):\n            if column_idx &gt; 0:\n                # If not the first feature, use all previous features\n                mask = torch.zeros_like(train_X).bool()\n                mask[:, :column_idx] = True\n                X_, y_ = (\n                    train_X[mask].reshape(train_X.shape[0], -1),\n                    train_X[:, column_idx],\n                )\n\n                mask = torch.zeros_like(X).bool()\n                mask[:, column_idx:] = True\n                X_pred, y_pred = (\n                    imputed_X[mask].reshape(imputed_X.shape[0], -1),\n                    imputed_X[:, column_idx],\n                )\n            else:\n                # If the first feature, use a zero feature as input\n                # Because of preprocessing, we can't use a zero feature, so we use a random feature\n                X_, y_ = torch.rand_like(train_X[:, 0:1]), train_X[:, 0]\n                X_pred, y_pred = torch.rand_like(imputed_X[:, 0:1]), imputed_X[:, 0]\n\n            if torch.isnan(y_pred).sum() == 0:\n                continue\n\n            cat_column = column_idx in categorical_features\n            model = self.tabpfn_clf if cat_column else self.tabpfn_reg\n\n            X_where_y_is_nan = X_pred[torch.isnan(y_pred)]\n\n            # TODO: If the model can't do semisupervised, need to remove missing ys\n            model.fit(X_, y_)\n\n            if not cat_column:\n                pred = model.predict_full(X_where_y_is_nan)\n                pred = pred[\"criterion\"].sample(torch.tensor(pred[\"logits\"]), t=t)\n            else:\n                pred = model.predict_proba(X_where_y_is_nan)\n                # sample from the predicted distribution\n                pred = (\n                    torch.distributions.Categorical(probs=torch.tensor(pred))\n                    .sample()\n                    .float()\n                )\n            imputed_X[torch.isnan(y_pred), column_idx] = pred\n        return imputed_X\n\n    def impute(self, X: torch.tensor, t: float = 0.000000001) -&gt; torch.tensor:\n        \"\"\"Impute missing values in the input data.\n\n        Parameters:\n            X : torch.Tensor of shape (n_samples, n_features)\n                Input data with missing values encoded as np.nan.\n\n            t : float, default=0.000000001\n                Temperature for sampling from the imputation distribution. Lower values result in\n                more deterministic imputations.\n\n        Returns:\n            torch.Tensor of shape (n_samples, n_features)\n                Imputed data with missing values replaced.\n        \"\"\"\n        train_X = self.X_\n        imputed_X = copy.deepcopy(X)\n        from tqdm import tqdm\n\n        categorical_features = self.tabpfn_clf.infer_categorical_features(self.X_)\n\n        for column_idx in tqdm(range(0, X.shape[1])):\n            mask = torch.zeros_like(train_X).bool()\n            mask[:, column_idx] = True\n            X_, y_ = train_X[~mask].reshape(train_X.shape[0], -1), train_X[mask]\n\n            mask = torch.zeros_like(X).bool()\n            mask[:, column_idx] = True\n            X_pred, y_pred = (\n                imputed_X[~mask].reshape(imputed_X.shape[0], -1),\n                imputed_X[mask],\n            )\n\n            if torch.isnan(y_pred).sum() == 0:\n                continue\n\n            cat_column = column_idx in categorical_features\n            model = self.tabpfn_clf if cat_column else self.tabpfn_reg\n\n            X_where_y_is_nan = X_pred[torch.isnan(y_pred)]\n\n            # TODO: If the model can't do semisupervised, need to remove missing ys\n            model.fit(X_, y_)\n\n            if not cat_column:\n                pred = model.predict_full(X_where_y_is_nan)\n                pred = pred[\"criterion\"].sample(torch.tensor(pred[\"logits\"]), t=t)\n            else:\n                pred = model.predict_proba(X_where_y_is_nan)\n                # sample from the predicted distribution\n                pred = (\n                    torch.distributions.Categorical(probs=torch.tensor(pred))\n                    .sample()\n                    .float()\n                )\n            imputed_X[torch.isnan(y_pred), column_idx] = pred\n        return imputed_X\n\n    def get_embeddings(self, X: torch.tensor) -&gt; torch.tensor:\n        \"\"\"\n        Get the transformer embeddings for the test data X.\n\n        :param X:\n        :return:\n        \"\"\"\n        y = (self.y\n            if self.y is not None\n            else (torch.zeros_like(self.X_[:, 0])))\n        model = self.tabpfn_reg if self.y is None or len(np.unique(y)) &gt; 10 else self.tabpfn_clf\n        model.fit(\n            self.X_,\n            y,  # Must contain more than one class\n        )  # Fit the data for random labels\n        embs = model.get_embeddings(X, additional_y=None)\n        return embs.reshape(X.shape[0], -1)\n\n    def get_embeddings_per_column(self, X: torch.tensor) -&gt; torch.tensor:\n        \"\"\"\n        Alternative implementation for get_embeddings, where we get the embeddings for each column as a label\n         separately and concatenate the results. This alternative way needs more passes but might be more accurate\n        \"\"\"\n        embs = []\n        for column_idx in range(0, X.shape[1]):\n            mask = torch.zeros_like(self.X_).bool()\n            mask[:, column_idx] = True\n            X_train, y_train = (\n                self.X_[~(mask)].reshape(self.X_.shape[0], -1),\n                self.X_[mask],\n            )\n\n            X_pred, y_pred = X[~(mask)].reshape(X.shape[0], -1), X[mask]\n\n            model = (\n                self.tabpfn_clf\n                if column_idx in self.categorical_features\n                else self.tabpfn_reg\n            )\n            model.fit(X_train, y_train)\n            embs += [model.get_embeddings(X_pred, additional_y=None)]\n\n        return torch.cat(embs, 1).reshape(embs[0].shape[0], -1)\n\n    def init_model_and_get_model_config(self):\n        for estimator in self.estimators:\n            estimator.init_model_and_get_model_config()\n\n    def outliers(self, X: torch.tensor) -&gt; torch.tensor:\n        \"\"\"\n        Preferred implementation for outliers, where we calculate the sample probability for each sample in X by\n        multiplying the probabilities of each feature according to chain rule of probability. The first feature is\n        estimated by using a zero feature as input.\n\n        :param X: Samples to calculate the sample probability for, shape (n_samples, n_features)\n        :return: Sample probability for each sample in X, shape (n_samples,)\n        \"\"\"\n        print('NEW')\n\n        self.init_model_and_get_model_config()\n\n        p = torch.ones_like(X[:, 0])  # Start with a probability of 1\n        for column_idx in range(0, X.shape[1]):\n            mask = torch.zeros_like(X).bool()\n            if column_idx &gt; 0:\n                # If not the first feature, use all previous features\n                mask[:, :column_idx] = True\n                X_train, y_train = X[(mask)], X[:, column_idx]\n                X_train = X_train.reshape(X.shape[0], -1)\n            else:\n                # If the first feature, use a zero feature as input\n                # Because of preprocessing, we can't use a zero feature, so we use a random feature\n                X_train, y_train = torch.rand_like(X[:, 0:1]), X[:, 0]\n            X_train, y_train = X_train.numpy(), y_train.numpy()\n\n            model = (\n                self.tabpfn_clf\n                if column_idx in self.categorical_features\n                and len(np.unique(y_train)) &lt; self.tabpfn_clf.max_num_classes_\n                else self.tabpfn_reg\n            )\n\n            model.fit(X_train, y_train)\n\n            pred = model.predict_y_proba(X_train, y_train)\n\n            p = p * pred\n\n        return p\n\n    def outliers_leave_one_out(self, X: torch.tensor) -&gt; torch.tensor:\n        \"\"\"\n        Alternative implementation for outliers, where we calculate the sample probability for each sample in X by\n        leaving one feature out at a time and multiplying the probabilities.\n\n        :param X: Samples to calculate the sample probability for, shape (n_samples, n_features)\n        :return: Sample probability for each sample in X, shape (n_samples,)\n        \"\"\"\n        p = torch.ones_like(X[:, 0])\n        for column_idx in range(0, X.shape[1]):\n            mask = torch.zeros_like(X).bool()\n            mask[:, column_idx] = True\n            X_train, y_train = X[~(mask)], X[mask]\n            X_train = X_train.reshape(X.shape[0], -1)\n\n            model = (\n                self.tabpfn_clf\n                if column_idx in self.categorical_features\n                else self.tabpfn_reg\n            )\n\n            model.fit(X_train, y_train)\n\n            pred = model.predict_y_proba(X_train, y_train)\n\n            p = p * pred\n\n        return p\n\n    def generate_synthetic_data(self, n_samples=100, t=1.0):\n        \"\"\"\n        Generate synthetic data using the trained models. Uses imputation method to generate synthetic data, passed with\n        a matrix of nans. Samples are generated feature by feature in one pass, so samples are not dependent on each\n        other per feature.\n\n        Parameters:\n            n_samples : int, default=100\n                Number of synthetic samples to generate.\n\n            t : float, default=1.0\n                Temperature for sampling from the imputation distribution. Lower values result in\n                more deterministic samples.\n\n        Returns:\n            torch.Tensor of shape (n_samples, n_features)\n                Generated synthetic data.\n\n        Raises:\n            AssertionError\n                If the model is not fitted.\n        \"\"\"\n        # TODO: Test what happens if we generate one feature at a time, with train data only for that featur\n        #  and previous ones, like outliers\n        assert hasattr(\n            self, \"X_\"\n        ), \"You need to fit the model before generating synthetic data\"\n\n        X = torch.zeros(n_samples, self.X_.shape[1]) * np.nan\n        return self.impute_for_gen(X, t=t)\n</code></pre>"},{"location":"api/tabpfn_unsupervised/#scripts.estimator.TabPFNUnsupervisedModel.__init__","title":"__init__","text":"<pre><code>__init__(tabpfn_clf: Optional[TabPFNClassifier] = None, tabpfn_reg: Optional[TabPFNRegressor] = None) -&gt; None\n</code></pre> <p>Initialize the TabPFNUnsupervisedModel.</p> <p>Parameters:</p> Name Type Description Default <code>tabpfn_clf</code> <p>TabPFNClassifier, optional TabPFNClassifier instance for handling categorical features. If not provided, the model assumes that there are no categorical features in the data.</p> <code>None</code> <code>tabpfn_reg</code> <p>TabPFNRegressor, optional TabPFNRegressor instance for handling numerical features. If not provided, the model assumes that there are no numerical features in the data.</p> <code>None</code> Source code in <code>scripts/estimator/unsupervised.py</code> <pre><code>def __init__(\n    self,\n    tabpfn_clf: Optional[TabPFNClassifier] = None,\n    tabpfn_reg: Optional[TabPFNRegressor] = None,\n) -&gt; None:\n    \"\"\"Initialize the TabPFNUnsupervisedModel.\n\n    Parameters:\n        tabpfn_clf : TabPFNClassifier, optional\n            TabPFNClassifier instance for handling categorical features. If not provided, the model\n            assumes that there are no categorical features in the data.\n\n        tabpfn_reg : TabPFNRegressor, optional\n            TabPFNRegressor instance for handling numerical features. If not provided, the model\n            assumes that there are no numerical features in the data.\n\n    Raises:\n        AssertionError\n            If both tabpfn_clf and tabpfn_reg are None.\n    \"\"\"\n    assert (\n        tabpfn_clf is not None or tabpfn_reg is not None\n    ), \"You cannot set both `tabpfn_clf` and `tabpfn_reg` to None. You can set one to None, if your table exclusively consists of categoricals/numericals.\"\n\n    self.tabpfn_clf = tabpfn_clf\n    self.tabpfn_reg = tabpfn_reg\n    self.estimators = [self.tabpfn_clf, self.tabpfn_reg]\n\n    self.categorical_features = []\n</code></pre>"},{"location":"api/tabpfn_unsupervised/#scripts.estimator.TabPFNUnsupervisedModel.fit","title":"fit","text":"<pre><code>fit(X: ndarray, y: Optional[ndarray] = None) -&gt; None\n</code></pre> <p>Fit the model to the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Input data to fit the model.</p> required <code>y</code> <p>array-like of shape (n_samples,), optional Target values.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>None</code> <p>TabPFNUnsupervisedModel Fitted model.</p> Source code in <code>scripts/estimator/unsupervised.py</code> <pre><code>def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; None:\n    \"\"\"Fit the model to the input data.\n\n    Parameters:\n        X : array-like of shape (n_samples, n_features)\n            Input data to fit the model.\n\n        y : array-like of shape (n_samples,), optional\n            Target values.\n\n    Returns:\n        self : TabPFNUnsupervisedModel\n            Fitted model.\n    \"\"\"\n    self.X_ = copy.deepcopy(X)\n    self.y = copy.deepcopy(y)\n    self.tabpfn_clf.infer_categorical_features(X)\n    self.categorical_features = self.tabpfn_clf.categorical_features\n</code></pre>"},{"location":"api/tabpfn_unsupervised/#scripts.estimator.TabPFNUnsupervisedModel.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(X: tensor) -&gt; tensor\n</code></pre> <p>Get the transformer embeddings for the test data X.</p> <p>:param X: :return:</p> Source code in <code>scripts/estimator/unsupervised.py</code> <pre><code>def get_embeddings(self, X: torch.tensor) -&gt; torch.tensor:\n    \"\"\"\n    Get the transformer embeddings for the test data X.\n\n    :param X:\n    :return:\n    \"\"\"\n    y = (self.y\n        if self.y is not None\n        else (torch.zeros_like(self.X_[:, 0])))\n    model = self.tabpfn_reg if self.y is None or len(np.unique(y)) &gt; 10 else self.tabpfn_clf\n    model.fit(\n        self.X_,\n        y,  # Must contain more than one class\n    )  # Fit the data for random labels\n    embs = model.get_embeddings(X, additional_y=None)\n    return embs.reshape(X.shape[0], -1)\n</code></pre>"},{"location":"api/tabpfn_unsupervised/#scripts.estimator.TabPFNUnsupervisedModel.outliers","title":"outliers","text":"<pre><code>outliers(X: tensor) -&gt; tensor\n</code></pre> <p>Preferred implementation for outliers, where we calculate the sample probability for each sample in X by multiplying the probabilities of each feature according to chain rule of probability. The first feature is estimated by using a zero feature as input.</p> <p>:param X: Samples to calculate the sample probability for, shape (n_samples, n_features) :return: Sample probability for each sample in X, shape (n_samples,)</p> Source code in <code>scripts/estimator/unsupervised.py</code> <pre><code>def outliers(self, X: torch.tensor) -&gt; torch.tensor:\n    \"\"\"\n    Preferred implementation for outliers, where we calculate the sample probability for each sample in X by\n    multiplying the probabilities of each feature according to chain rule of probability. The first feature is\n    estimated by using a zero feature as input.\n\n    :param X: Samples to calculate the sample probability for, shape (n_samples, n_features)\n    :return: Sample probability for each sample in X, shape (n_samples,)\n    \"\"\"\n    print('NEW')\n\n    self.init_model_and_get_model_config()\n\n    p = torch.ones_like(X[:, 0])  # Start with a probability of 1\n    for column_idx in range(0, X.shape[1]):\n        mask = torch.zeros_like(X).bool()\n        if column_idx &gt; 0:\n            # If not the first feature, use all previous features\n            mask[:, :column_idx] = True\n            X_train, y_train = X[(mask)], X[:, column_idx]\n            X_train = X_train.reshape(X.shape[0], -1)\n        else:\n            # If the first feature, use a zero feature as input\n            # Because of preprocessing, we can't use a zero feature, so we use a random feature\n            X_train, y_train = torch.rand_like(X[:, 0:1]), X[:, 0]\n        X_train, y_train = X_train.numpy(), y_train.numpy()\n\n        model = (\n            self.tabpfn_clf\n            if column_idx in self.categorical_features\n            and len(np.unique(y_train)) &lt; self.tabpfn_clf.max_num_classes_\n            else self.tabpfn_reg\n        )\n\n        model.fit(X_train, y_train)\n\n        pred = model.predict_y_proba(X_train, y_train)\n\n        p = p * pred\n\n    return p\n</code></pre>"},{"location":"api/tabpfn_unsupervised/#scripts.estimator.TabPFNUnsupervisedModel.generate_synthetic_data","title":"generate_synthetic_data","text":"<pre><code>generate_synthetic_data(n_samples=100, t=1.0)\n</code></pre> <p>Generate synthetic data using the trained models. Uses imputation method to generate synthetic data, passed with a matrix of nans. Samples are generated feature by feature in one pass, so samples are not dependent on each other per feature.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <p>int, default=100 Number of synthetic samples to generate.</p> <code>100</code> <code>t</code> <p>float, default=1.0 Temperature for sampling from the imputation distribution. Lower values result in more deterministic samples.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>torch.Tensor of shape (n_samples, n_features) Generated synthetic data.</p> Source code in <code>scripts/estimator/unsupervised.py</code> <pre><code>def generate_synthetic_data(self, n_samples=100, t=1.0):\n    \"\"\"\n    Generate synthetic data using the trained models. Uses imputation method to generate synthetic data, passed with\n    a matrix of nans. Samples are generated feature by feature in one pass, so samples are not dependent on each\n    other per feature.\n\n    Parameters:\n        n_samples : int, default=100\n            Number of synthetic samples to generate.\n\n        t : float, default=1.0\n            Temperature for sampling from the imputation distribution. Lower values result in\n            more deterministic samples.\n\n    Returns:\n        torch.Tensor of shape (n_samples, n_features)\n            Generated synthetic data.\n\n    Raises:\n        AssertionError\n            If the model is not fitted.\n    \"\"\"\n    # TODO: Test what happens if we generate one feature at a time, with train data only for that featur\n    #  and previous ones, like outliers\n    assert hasattr(\n        self, \"X_\"\n    ), \"You need to fit the model before generating synthetic data\"\n\n    X = torch.zeros(n_samples, self.X_.shape[1]) * np.nan\n    return self.impute_for_gen(X, t=t)\n</code></pre>"},{"location":"api/tabpfn_unsupervised/#scripts.estimator.TabPFNUnsupervisedModel.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features)\n</code></pre> Source code in <code>scripts/estimator/unsupervised.py</code> <pre><code>def set_categorical_features(self, categorical_features):\n    self.categorical_features = categorical_features\n    for estimator in self.estimators:\n        estimator.set_categorical_features(categorical_features)\n</code></pre>"}]}