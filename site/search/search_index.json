{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>OVERWRITE!</p>"},{"location":"api-usage-limit/","title":"Increase My API Usage Limit","text":"<p>To request an increase in your API usage limit, please fill out the following form:  </p> <p>\u27a1\ufe0f Request API Limit Increase </p> <p>We will be in touch shortly!</p>"},{"location":"aup/","title":"ACCEPTABLE USE POLICY","text":"<p>Effective Date: January 8<sup>th</sup> 2025</p> <p>This Acceptable Use Policy (\"AUP\") applies to the use of PriorLabs' Services. Where this AUP uses terms that are defined in the General Terms and Conditions (\"GTC\"), those terms shall have the meaning ascribed to them in the GTC.</p> <p>PriorLabs reserves the right to change this AUP in accordance with the GTC at https://www.priorlabs.ai/aup.</p>"},{"location":"aup/#1-what-type-of-activity-is-prohibited","title":"1. What type of activity is prohibited?","text":"<p>Customer shall not use, and encourage or allow any other person or entity to use the Services in prohibited manners, including but not limited to the following:</p> <ul> <li> <p>Customer may not upload any personal data within the meaning of the GDPR to the Contract Software or the Services.</p> </li> <li> <p>Customer may not upload any material to the Contract Software or the Services that infringes the intellectual property rights or other rights of third parties, including but not limited to trademarks, copyrights, trade secrets, rights of publicity, or otherwise violating, infringing or misappropriating the rights of any third party.</p> </li> <li> <p>Customer may not misappropriate, reverse-engineer, copy, disassemble, decompile, extract source code, trade secrets, or know-how, including PriorLabs' models, algorithms or artificial intelligence systems, or otherwise misuse or manipulate the Contract Software or Services or any part thereof.</p> </li> <li> <p>Customer may not use the Services or the Contract Software in a way that imposes an unreasonable or disproportionately large load on PriorLabs' infrastructure, which adversely impacting the availability, reliability or stability of PriorLabs' Services.</p> </li> <li> <p>Customer may not upload any viruses, spam, trojan horses, worms or any other malicious, harmful, or deleterious programs or code, including prompt-based manipulation or scraping behaviors, to the Contract Software or the Services.</p> </li> <li> <p>Customer may not attempt to use the Services and Contract Software in a manner that compromises, circumvents, or tests the vulnerability of any of PriorLabs' technical safeguards or other security measures.</p> </li> <li> <p>Customer may not use PriorLabs' Services or the Contract Software in any manner that may subject PriorLabs or any third party to liability, damages or danger.</p> </li> <li> <p>Customer shall not use the Contract Software improperly or allow it to be used improperly, and in particular shall not use or upload to the Contract Software any content that is illegal or immoral and/or such content that serves to incite hatred, hate speech, illicit deep fakes, or fake news, or incites criminal acts or glorifies or trivializes violence, is sexually offensive or pornographic, is capable of seriously endangering children or young people morally or impairing their well-being or may damage the reputation of PriorLabs, and shall not refer to such content.</p> </li> </ul> <p>This list of prohibited uses is provided by way of example and should not be considered exhaustive.</p>"},{"location":"aup/#2-who-is-prohibited-from-using-the-services","title":"2. Who is prohibited from using the Services?","text":"<p>Consumers within the meaning of Section 13 German Civil Code may not use PriorLabs' Services.</p>"},{"location":"cla/","title":"Contributor Agreement","text":""},{"location":"cla/#individual-contributor-exclusive-license-agreement","title":"Individual Contributor Exclusive License Agreement","text":""},{"location":"cla/#including-the-traditional-patent-license-option","title":"(including the Traditional Patent License OPTION)","text":"<p>Thank you for your interest in contributing to PriorLabs's TabPFN (\"We\" or \"Us\").</p> <p>The purpose of this contributor agreement (\"Agreement\") is to clarify and document the rights granted by contributors to Us. To make this document effective, please follow the instructions at https://www.priorlabs.ai/sign-cla.</p>"},{"location":"cla/#how-to-use-this-contributor-agreement","title":"How to use this Contributor Agreement","text":"<p>If You are an employee and have created the Contribution as part of your employment, You need to have Your employer approve this Agreement or sign the Entity version of this document. If You do not own the Copyright in the entire work of authorship, any other author of the Contribution should also sign this \u2013 in any event, please contact Us at noah.homa@gmail.com</p>"},{"location":"cla/#1-definitions","title":"1. Definitions","text":"<p>\"You\" means the individual Copyright owner who Submits a Contribution to Us.</p> <p>\"Contribution\" means any original work of authorship, including any original modifications or additions to an existing work of authorship, Submitted by You to Us, in which You own the Copyright.</p> <p>\"Copyright\" means all rights protecting works of authorship, including copyright, moral and neighboring rights, as appropriate, for the full term of their existence.</p> <p>\"Material\" means the software or documentation made available by Us to third parties. When this Agreement covers more than one software project, the Material means the software or documentation to which the Contribution was Submitted. After You Submit the Contribution, it may be included in the Material.</p> <p>\"Submit\" means any act by which a Contribution is transferred to Us by You by means of tangible or intangible media, including but not limited to electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Us, but excluding any transfer that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\"</p> <p>\"Documentation\" means any non-software portion of a Contribution.</p>"},{"location":"cla/#2-license-grant","title":"2. License grant","text":""},{"location":"cla/#21-copyright-license-to-us","title":"2.1 Copyright license to Us","text":"<p>Subject to the terms and conditions of this Agreement, You hereby grant to Us a worldwide, royalty-free, Exclusive, perpetual and irrevocable (except as stated in Section 8.2) license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul>"},{"location":"cla/#22-moral-rights","title":"2.2 Moral rights","text":"<p>Moral Rights remain unaffected to the extent they are recognized and not waivable by applicable law. Notwithstanding, You may add your name to the attribution mechanism customary used in the Materials you Contribute to, such as the header of the source code files of Your Contribution, and We will respect this attribution when using Your Contribution.</p>"},{"location":"cla/#23-copyright-license-back-to-you","title":"2.3 Copyright license back to You","text":"<p>Upon such grant of rights to Us, We immediately grant to You a worldwide, royalty-free, non-exclusive, perpetual and irrevocable license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul> <p>This license back is limited to the Contribution and does not provide any rights to the Material.</p>"},{"location":"cla/#3-patents","title":"3. Patents","text":""},{"location":"cla/#31-patent-license","title":"3.1 Patent license","text":"<p>Subject to the terms and conditions of this Agreement You hereby grant to Us and to recipients of Materials distributed by Us a worldwide, royalty-free, non-exclusive, perpetual and irrevocable (except as stated in Section 3.2) patent license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, to make, have made, use, sell, offer for sale, import and otherwise transfer the Contribution and the Contribution in combination with any Material (and portions of such combination). This license applies to all patents owned or controlled by You, whether already acquired or hereafter acquired, that would be infringed by making, having made, using, selling, offering for sale, importing or otherwise transferring of Your Contribution(s) alone or by combination of Your Contribution(s) with any Material.</p>"},{"location":"cla/#32-revocation-of-patent-license","title":"3.2 Revocation of patent license","text":"<p>You reserve the right to revoke the patent license stated in section 3.1 if We make any infringement claim that is targeted at your Contribution and not asserted for a Defensive Purpose. An assertion of claims of the Patents shall be considered for a \"Defensive Purpose\" if the claims are asserted against an entity that has filed, maintained, threatened, or voluntarily participated in a patent infringement lawsuit against Us or any of Our licensees.</p>"},{"location":"cla/#4-license-obligations-by-us","title":"4. License obligations by Us","text":"<p>We agree to license the Contribution only under the terms of the license or licenses that We are using on the Submission Date for the Material (including any rights to adopt any future version of a license).</p> <p>In addition, We may use the following licenses for Documentation in the Contribution: CC-BY-4.0, CC-BY-ND-4.0, CC-BY-NC-4.0, CC-BY-NC-ND-4.0, CC-BY-NC-SA-4.0, CC-BY-SA-4.0, CC0-1.0, MIT License, Apache License, GNU General Public License (GPL) v2.0, GNU General Public License (GPL) v3.0, GNU Affero General Public License v3.0, GNU Lesser General Public License (LGPL) v2.1, GNU Lesser General Public License (LGPL) v3.0, Mozilla Public License 2.0, Eclipse Public License 2.0, Microsoft Public License (Ms-PL), Microsoft Reciprocal License (Ms-RL), BSD 2-Clause \"Simplified\" or \"FreeBSD\" license, BSD 3-Clause \"New\" or \"Revised\" license (including any right to adopt any future version of a license).</p> <p>We agree to license patents owned or controlled by You only to the extent necessary to (sub)license Your Contribution(s) and the combination of Your Contribution(s) with the Material under the terms of the license or licenses that We are using on the Submission Date.</p>"},{"location":"cla/#5-disclaimer","title":"5. Disclaimer","text":"<p>THE CONTRIBUTION IS PROVIDED \"AS IS\". MORE PARTICULARLY, ALL EXPRESS OR IMPLIED WARRANTIES INCLUDING, WITHOUT LIMITATION, ANY IMPLIED WARRANTY OF SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT ARE EXPRESSLY DISCLAIMED BY YOU TO US AND BY US TO YOU. TO THE EXTENT THAT ANY SUCH WARRANTIES CANNOT BE DISCLAIMED, SUCH WARRANTY IS LIMITED IN DURATION AND EXTENT TO THE MINIMUM PERIOD AND EXTENT PERMITTED BY LAW.</p>"},{"location":"cla/#6-consequential-damage-waiver","title":"6. Consequential damage waiver","text":"<p>TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL YOU OR WE BE LIABLE FOR ANY LOSS OF PROFITS, LOSS OF ANTICIPATED SAVINGS, LOSS OF DATA, INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL AND EXEMPLARY DAMAGES ARISING OUT OF THIS AGREEMENT REGARDLESS OF THE LEGAL OR EQUITABLE THEORY (CONTRACT, TORT OR OTHERWISE) UPON WHICH THE CLAIM IS BASED.</p>"},{"location":"cla/#7-approximation-of-disclaimer-and-damage-waiver","title":"7. Approximation of disclaimer and damage waiver","text":"<p>IF THE DISCLAIMER AND DAMAGE WAIVER MENTIONED IN SECTION 5. AND SECTION 6. CANNOT BE GIVEN LEGAL EFFECT UNDER APPLICABLE LOCAL LAW, REVIEWING COURTS SHALL APPLY LOCAL LAW THAT MOST CLOSELY APPROXIMATES AN ABSOLUTE WAIVER OF ALL CIVIL OR CONTRACTUAL LIABILITY IN CONNECTION WITH THE CONTRIBUTION.</p>"},{"location":"cla/#8-term","title":"8. Term","text":"<p>8.1 This Agreement shall come into effect upon Your acceptance of the terms and conditions.</p> <p>8.2 This Agreement shall apply for the term of the copyright and patents licensed here. However, You shall have the right to terminate the Agreement if We do not fulfill the obligations as set forth in Section 4. Such termination must be made in writing.</p> <p>8.3 In the event of a termination of this Agreement Sections 5, 6, 7, 8 and 9 shall survive such termination and shall remain in full force thereafter. For the avoidance of doubt, Free and Open Source Software (sub)licenses that have already been granted for Contributions at the date of the termination shall remain in full force after the termination of this Agreement.</p>"},{"location":"cla/#9-miscellaneous","title":"9. Miscellaneous","text":"<p>9.1 This Agreement and all disputes, claims, actions, suits or other proceedings arising out of this agreement or relating in any way to it shall be governed by the laws of Germany excluding its private international law provisions.</p> <p>9.2 This Agreement sets out the entire agreement between You and Us for Your Contributions to Us and overrides all other agreements or understandings.</p> <p>9.3 In case of Your death, this agreement shall continue with Your heirs. In case of more than one heir, all heirs must exercise their rights through a commonly authorized person.</p> <p>9.4 If any provision of this Agreement is found void and unenforceable, such provision will be replaced to the extent possible with a provision that comes closest to the meaning of the original provision and that is enforceable. The terms and conditions set forth in this Agreement shall apply notwithstanding any failure of essential purpose of this Agreement or any limited remedy to the maximum extent possible under law.</p> <p>9.5 You agree to notify Us of any facts or circumstances of which you become aware that would make this Agreement inaccurate in any respect.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>Put out project that people could contribute to and provide instructions for contributing</p>"},{"location":"docs/","title":"","text":"<p>PriorLabs is building breakthrough foundation models that understand spreadsheets and databases. While foundation models have transformed text and images, tabular data has remained largely untouched. We're tackling this opportunity with technology that could revolutionize how we approach scientific discovery, medical research, financial modeling, and business intelligence.</p>"},{"location":"docs/#tabpfn-integrations","title":"TabPFN Integrations","text":"<ul> <li> <p> API Client</p> <p>The fastest way to get started with TabPFN. Access our models through the cloud without requiring local GPU resources.</p> <p> TabPFN Client</p> </li> <li> <p> User Interface</p> <p>Visual interface for no-code interaction with TabPFN. Perfect for quick experimentation and visualization.</p> <p> Access GUI</p> </li> <li> <p> Python Package</p> <p>Local installation for research and privacy sesitive use cases with GPU support and scikit-learn compatible interface.</p> <p> TabPFN Local</p> </li> <li> <p> R Integration</p> <p>Bringing TabPFN's capabilities to the R ecosystem for data scientists and researchers. We have an experimental R package and an alternative tutorial on usage in R. Contributions welcome!</p> </li> </ul>"},{"location":"docs/#why-tabpfn","title":"Why TabPFN","text":"<ul> <li> <p> Rapid Training</p> <p>TabPFN significantly reduces training time, outperforming traditional models tuned for hours in just a few seconds. For instance, it surpasses an ensemble of the strongest baselines in 2.8 seconds compared to 4 hours of tuning.</p> </li> <li> <p> Superior Accuracy</p> <p>TabPFN consistently outperforms state-of-the-art methods like gradient-boosted decision trees (GBDTs) on datasets with up to 10,000 samples. It achieves higher accuracy and better performance metrics across a range of classification and regression tasks.</p> </li> <li> <p> Robustness</p> <p>The model demonstrates robustness to various dataset characteristics, including uninformative features, outliers, and missing values, maintaining high performance where other methods struggle.</p> </li> <li> <p> Generative Capabilities</p> <p>As a generative transformer-based model, TabPFN can be fine-tuned for specific tasks, generate synthetic data, estimate densities, and learn reusable embeddings. This makes it versatile for various applications beyond standard prediction tasks.</p> </li> <li> <p> Sklearn Interface</p> <p>TabPFN follows the interfaces provided by scikit-learn, making it easy to integrate into existing workflows and utilize familiar functions for fitting, predicting, and evaluating models.</p> </li> <li> <p> Minimal Preprocessing</p> <p>The model handles various types of raw data, including missing values and categorical variables, with minimal preprocessing. This reduces the burden on users to perform extensive data preparation.</p> </li> </ul>"},{"location":"enterprise/","title":"TabPFN Business","text":"<p>Unlock the hidden value in your company's databases and spreadsheets using TabPFN. Our state-of-the-art tabular foundation model is faster and more accurate in 96% of the use-cases and requires 50% less data as previous methods.</p> <p>Save your data science team hours &amp; days of work and enable them to focus on mission-critical business problems, even when data availability is limited.</p>"},{"location":"enterprise/#why-tabpfn-business","title":"Why TabPFN Business?","text":""},{"location":"enterprise/#access-to-enterprise-grade-features","title":"Access to Enterprise-Grade Features","text":"<ul> <li>Priority Access: Get early access to new features and model improvements with support for larger datasets</li> <li>Custom Features: Direct access to features designed specifically for business users such as fine-tuning, tech stack integrations and data sharing across your organization</li> <li>Dedicated Support: Premium SLA support from our scientists and engineers on implemenation, training and critical issues</li> <li>Compliance: Enterprise-level GDPR compliance including audit logs and role based access control</li> </ul> Get In Touch Email Address * First Name * Last Name * Company size * 1-49 50-249 250-999 1000+ Company Website * What are you interested in? *<ul> <li>I want to use Prior Labs products for my business</li> <li>I have a press/event request</li> <li>Other</li> </ul> Marketing Permissions <p>Please select all the ways you would like to hear from PriorLabs:</p> Email <p>You can unsubscribe at any time by clicking the link in the footer of our emails.</p> <p>We use Mailchimp as our marketing platform. By clicking below to subscribe, you acknowledge that your information will be transferred to Mailchimp for processing. Learn more about Mailchimp's privacy practices.</p>"},{"location":"newsletter/","title":"Stay Updated with TabPFN","text":"<p>Join our newsletter to get the latest updates on TabPFN's development, best practices, and breakthrough research in tabular machine learning.</p>"},{"location":"newsletter/#what-youll-get","title":"What You'll Get","text":"<ul> <li>Early Access: Be the first to know about new releases and features</li> <li>Technical Insights: Deep dives into TabPFN's architecture and capabilities</li> <li>Best Practices: Tips and tricks for getting the most out of TabPFN</li> <li>Research Updates: Latest developments in tabular foundation models</li> <li>Community Highlights: Featured projects and use cases from our community</li> </ul> Email Address * Marketing Permissions <p>Please select all the ways you would like to hear from PriorLabs:</p> Email <p>You can unsubscribe at any time by clicking the link in the footer of our emails.</p> <p>We use Mailchimp as our marketing platform. By clicking below to subscribe, you acknowledge that your information will be transferred to Mailchimp for processing. Learn more about Mailchimp's privacy practices.</p>"},{"location":"privacy_policy/","title":"Privacy Policy","text":"<p>Last updated: May 12<sup>th</sup>, 2025</p>"},{"location":"privacy_policy/#1-general-information","title":"1. General information","text":"<p>Prior Labs GmbH, Kaiser-Joseph-Str. 254, 79098 Freiburg (hereinafter \u201cPriorLabs\u201d, \u201cwe\u201d or \u201cus\u201d) takes the protection of personal data very seriously. We treat personal data confidentially and always in accordance with the applicable data protection laws, in particular Regulation (EU) 2016/679 (hereinafter \u201cGeneral Data Protection Regulation\u201d or \u201cGDPR\u201d), the German Federal Data Protection Act (hereinafter \u201cBDSG\u201d), and in accordance with the provisions of this privacy policy.</p> <p>The aim of this privacy policy is to inform you (hereinafter \u201cdata subject\u201d or \u201cyou\u201d) in accordance with Art. 12 et seq. GDPR about how we process your personal data and for what purposes we process your personal data when using our website https://priorlabs.ai/ (hereinafter \u201cWebsite\u201d), our services or contacting us.</p> <p>Unless otherwise stated in this privacy policy, the terms used here have the meaning as defined in the GDPR.</p>"},{"location":"privacy_policy/#2-data-controller","title":"2. Data controller","text":"<p>PriorLabs acts as a controller within the meaning of the GDPR in relation to your personal data processed in connection with the use of our Website, Service or a contact made to or by PriorLabs.</p> <p>If you have any questions about this privacy policy or the processing of your personal data, you can contact us at the following contact details:</p> <p>Prior Labs GmbH Kaiser-Joseph-Str. 254 79098 Freiburg E-mail: dataprotection@priorlabs.ai</p>"},{"location":"privacy_policy/#categories-purposes-and-legal-bases-of-the-personal-data-processed","title":"Categories, purposes and legal bases of the personal data processed","text":"<p>We process different categories of your personal data for different purposes. Below you can see which data we process in which contexts, for which purposes and on which legal basis we base the respective processing.</p>"},{"location":"privacy_policy/#21-visiting-our-website","title":"2.1. Visiting our Website","text":"<p>When visiting our Website for informational purposes (i.e., mere viewing and without you providing us with any other information), certain personal data is automatically collected and stored in so-called server log files:</p> <ul> <li>Browser type and version  </li> <li>Operating system used  </li> <li>Host name of the accessing computer  </li> <li>Date and time of access  </li> <li>IP address of the requesting computer  </li> </ul> <p>Such data is not merged with other data sources and is not evaluated for marketing purposes.</p> <p>Legal basis: Art. 6 para. 1 sent. 1 lit. f GDPR \u2013 our legitimate interest in providing a technically functional, user-friendly Website and ensuring system security.</p> <p>Duration of storage: Personal data in log files is deleted after 7 days unless legal retention obligations require longer storage.</p>"},{"location":"privacy_policy/#22-use-of-our-services","title":"2.2. Use of our Services","text":"<p>We provide a software to access TabPFN foundation models for analysis of tabular business data (\u201cServices\u201d). Our Acceptable Use Policy strictly prohibits the upload of personal data to use our Services.</p> <p>Although uploading personal data is not permitted, we do process some personal data when you access our Services via our API.</p>"},{"location":"privacy_policy/#221-user-account","title":"2.2.1. User account","text":"<p>We process the following data upon registration:</p> <ul> <li>First and last name  </li> <li>E-mail address  </li> <li>Password  </li> </ul> <p>Legal basis: Art. 6 para. 1 sent. 1 lit. b GDPR \u2013 performance of or steps prior to entering a contract.</p> <p>Duration of storage: You can request account deletion via dataprotection@priorlabs.ai. Inactive accounts are deleted after 3 years.</p>"},{"location":"privacy_policy/#222-usage-data","title":"2.2.2. Usage data","text":"<p>We process the following log file data:</p> <ul> <li>IP address  </li> <li>Browser type and version  </li> <li>Operating system used  </li> <li>Date and time of access  </li> <li>Host name of the accessing computer  </li> </ul> <p>This data ensures technical functionality, usability, and security.</p> <p>Legal basis: Art. 6 para. 1 sent. 1 lit. f GDPR \u2013 our legitimate interest in providing and securing our services.</p> <p>Duration of storage: Deleted after 7 days unless legally required otherwise.</p>"},{"location":"privacy_policy/#23-contact","title":"2.3. Contact","text":"<p>If you contact us via e-mail, we process:</p> <ul> <li>Name  </li> <li>E-mail address  </li> <li>Other voluntarily provided data (\u201cContact Data\u201d)  </li> </ul> <p>Legal basis: Art. 6 para. 1 sent. 1 lit. b GDPR \u2013 if related to a (pre-)contractual relationship. Otherwise, Art. 6 para. 1 sent. 1 lit. f GDPR \u2013 legitimate interest in appropriate customer communication.</p> <p>Duration of storage: Deleted once the inquiry is resolved, unless legal obligations require retention.</p>"},{"location":"privacy_policy/#24-newsletter","title":"2.4. Newsletter","text":"<p>With your consent, we process:</p> <ul> <li>E-mail address  </li> <li>Date and time of registration  </li> <li>IP address and browser type  </li> </ul> <p>Newsletters may include tracking links to analyze user engagement. We process:</p> <ul> <li>Newsletter opening (date/time)  </li> <li>Clicked links  </li> <li>IP address, browser type, device type, operating system (\u201cTracking Data\u201d)  </li> </ul> <p>Legal basis: Art. 6 para. 1 sent. 1 lit. a GDPR \u2013 based on your explicit consent.</p> <p>Duration of storage: Stored while subscription is active. You can revoke consent anytime via unsubscribe link.</p>"},{"location":"privacy_policy/#25-social-media-and-professional-networks","title":"2.5. Social media and professional networks","text":"<p>We maintain company profiles on LinkedIn, Github, X, and Discord. Clicking on icons on our Website redirects you to these platforms in a new browser window. No personal data is transferred before you click.</p>"},{"location":"privacy_policy/#251-visiting-our-pages","title":"2.5.1. Visiting our pages","text":"<p>Each platform is primarily responsible for processing data when you visit our page there.</p>"},{"location":"privacy_policy/#252-communication-via-platforms","title":"2.5.2. Communication via platforms","text":"<p>We may process information such as:</p> <ul> <li>User name  </li> <li>E-mail address  </li> <li>Contact details and communication content  </li> <li>Job title, company, education, photo, etc.  </li> </ul> <p>Legal basis: Art. 6 para. 1 sent. 1 lit. b GDPR \u2013 for contractual/pre-contractual communication. Otherwise, Art. 6 para. 1 sent. 1 lit. f GDPR \u2013 legitimate interest in customer communication.</p> <p>Duration of storage: Deleted once no longer needed unless retention is legally required.</p>"},{"location":"privacy_policy/#3-data-receivers","title":"3. Data receivers","text":"<p>We may share personal data with:</p> <ul> <li>Legal/tax consultants (acting as independent controllers)  </li> <li>Advisors or potential buyers in corporate transactions  </li> <li>Data Processors (under Art. 28 GDPR or EU SCCs)  </li> </ul> <p>Current Data Processors:</p> Data Processor Purpose OpenAI Processing text inputs to our model API Mailchimp Newsletter signup Google Analytics Usage analytics Google Cloud Cloud infrastructure, model processing/inference"},{"location":"privacy_policy/#4-data-transfers-to-third-countries","title":"4. Data transfers to third countries","text":"<p>We primarily process data within the EEA. Data may be transferred to third countries (e.g., USA) only under appropriate safeguards:</p> <ul> <li>EU Standard Contractual Clauses (Art. 46 para. 2 lit. c GDPR)  </li> <li>Adequacy decisions if available  </li> </ul> <p>Documentation available on request.</p>"},{"location":"privacy_policy/#5-your-rights","title":"5. Your rights","text":"<p>Under GDPR, you have the following rights:</p>"},{"location":"privacy_policy/#51-right-of-revocation","title":"5.1. Right of revocation","text":"<p>Art. 7 para. 3 GDPR \u2013 revoke your consent at any time (future effect only).</p>"},{"location":"privacy_policy/#52-right-of-access","title":"5.2. Right of access","text":"<p>Art. 15 GDPR \u2013 obtain confirmation and details about your processed personal data.</p>"},{"location":"privacy_policy/#53-right-to-rectification","title":"5.3. Right to rectification","text":"<p>Art. 16 GDPR \u2013 request correction of inaccurate/incomplete data.</p>"},{"location":"privacy_policy/#54-right-to-erasure","title":"5.4. Right to erasure","text":"<p>Art. 17 GDPR \u2013 request deletion of your data.</p>"},{"location":"privacy_policy/#55-right-to-restrict-processing","title":"5.5. Right to restrict processing","text":"<p>Art. 18 GDPR \u2013 request restriction of data processing.</p>"},{"location":"privacy_policy/#56-right-to-data-portability","title":"5.6. Right to data portability","text":"<p>Art. 20 GDPR \u2013 receive your data in a structured, machine-readable format.</p>"},{"location":"privacy_policy/#57-right-to-object","title":"5.7. Right to object","text":"<p>Art. 21 GDPR \u2013 object to processing based on legitimate interest.</p>"},{"location":"privacy_policy/#58-right-to-complain-to-a-supervisory-authority","title":"5.8. Right to complain to a supervisory authority","text":"<p>Art. 77 GDPR \u2013 complain to a supervisory authority. Responsible authority for PriorLabs: State Commissioner for Data Protection and Freedom of Information for Baden-W\u00fcrttemberg. List: [Link to German supervisory authorities]</p>"},{"location":"privacy_policy/#6-obligation-to-provide-data","title":"6. Obligation to provide data","text":"<p>Some data must be provided to use the Website as described above. If not provided, we may be unable to respond to inquiries or provide services.</p>"},{"location":"privacy_policy/#7-automated-decisions-profiling","title":"7. Automated decisions / profiling","text":"<p>We do not engage in automated decision-making within the meaning of Art. 22 para. 1 GDPR.</p>"},{"location":"privacy_policy/#8-changes-to-this-privacy-policy","title":"8. Changes to this privacy policy","text":"<p>We may update this policy. The \u201cLast updated\u201d date will be adjusted accordingly. Latest version always available at: https://priorlabs.ai/privacy_policy</p>"},{"location":"privacy_policy/#9-link-to-general-terms","title":"9. Link to General Terms","text":"<p>For information on the use of our services, including contractual obligations, limitations of liability, and user responsibilities, please refer to our General Terms.</p>"},{"location":"tabpfn-license/","title":"TabPFN License","text":"<pre><code>                              Prior Labs License\n                           Version 1.0, January 2025\n                       http://priorlabs.ai/tabpfn-license\n\n   This license is a derivative of the Apache 2.0 license\n   (http://www.apache.org/licenses/) with a single modification:\n   The added Paragraph 10 introduces an enhanced attribution requirement\n   inspired by the Llama 3 license.\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   ---------------------- ADDITIONAL PROVISION --------------------------\n\n    10. Additional attribution.\n    If You distribute or make available the Work or any Derivative\n    Work thereof relating to any part of the source or model weights,\n    or a product or service (including another AI model) that contains\n    any source or model weights, You shall (A) provide a copy of this\n    License with any such materials; and (B) prominently display\n    \u201cBuilt with TabPFN\u201d on each related website, user interface, blogpost,\n    about page, or product documentation. If You use the source or model\n    weights or model outputs to create, train, fine tune, distil, or\n    otherwise improve an AI model, which is distributed or made available,\n    you shall also include \u201cTabPFN\u201d at the beginning of any such AI model name.\n    To clarify, internal benchmarking and testing without external\n    communication shall not qualify as distribution or making available\n    pursuant to this Section 10 and no attribution under this Section 10\n    shall be required.\n\n\n   END OF TERMS AND CONDITIONS\n</code></pre>"},{"location":"tabpfn-nature/","title":"Accurate predictions on small data with a tabular foundation model","text":"<ul> <li> <p> API Client</p> <p>The fastest way to get started with TabPFN. Access our models through the cloud without requiring local GPU resources.</p> <p> TabPFN Client</p> </li> <li> <p> User Interface</p> <p>Visual interface for no-code interaction with TabPFN. Perfect for quick experimentation and visualization.</p> <p> Access GUI</p> </li> <li> <p> Python Package</p> <p>Local installation for research and privacy sesitive use cases with GPU support and scikit-learn compatible interface.</p> <p> TabPFN Local</p> </li> <li> <p> R Integration</p> <p>Currently in development. Bringing TabPFN's capabilities to the R ecosystem for data scientists and researchers. Contact us for more information, or to get involved!</p> </li> </ul>"},{"location":"terms-eu-en/","title":"Terms eu en","text":"<pre><code>GENERAL TERMS AND CONDITIONS\n1.  Scope of Application\n1.1.    These general terms and conditions (\"GTC\") govern the provision of access to the TabPFN foundation models as available at https://www.priorlabs.ai (\u201cServices\u201d) provided by Prior Labs GmbH, Georges-K\u00f6hler-Allee 302, 79110 Freiburg im Breisgau (\u201cPriorLabs\").\n1.2.    The Services of PriorLabs are directed exclusively at business customers (Unternehmer) within the meaning of Sec. 14 German Civil Code (B\u00fcrgerliches Gesetzbuch, BGB) (\u201cCustomer\u201d). PriorLabs may require the Customer to provide sufficient proof of its status as business customer prior to the conclusion of the contract. \n1.3.    Conflicting or additional contractual conditions of the Customer shall only apply if PriorLabs expressly confirms them in writing. \n2.  Conclusion of Contract\n2.1.    The contract is concluded with the inclusion of these GTC (\u201cContract\u201d) at the earliest of (i) when the Customer registers and sets up an account via the Services (\u201cPriorLabs Account\u201d). \n2.2.    Upon conclusion of the Contract, the Customer shall provide PriorLabs with all information that PriorLabs reasonably requires in order to provide the Services correctly and completely. The Customer is obliged to inform PriorLabs immediately of any relevant changes. \n3.  Registration and PriorLabs Account\n3.1.    In order to fully use the Services, the registration and setting up of a PriorLabs Account is required. By registering or using a PriorLabs Account, [the Customer agrees and represents that they created their PriorLabs Account, and they will use their PriorLabs Account only for themselves. Each Customer shall register only one PriorLabs Account. A PriorLabs Account is not transferable.\n3.2.    If and to the extent, PriorLabs stores Customer\u2019s data, PriorLabs disclaims any liability for the storage, accessibility, or integrity of such data.\n3.3.    The Customer is obliged (i) to provide complete and correct information about its person or entity at the time of registration and (ii) in case of respective changes to correct without undue delay this information insofar such information is mandatory for the performance of the Contract. \n3.4.    If PriorLabs receives a notice or otherwise has reason to believe that the information or documents provided by the Customer are wholly or partially incorrect, incomplete or not up to date, PriorLabs is entitled to request the Customer to remedy the situation immediately. If the Customer fails to correct or complete the information or document within the set deadline, PriorLabs is entitled to restrict access to the Services and block the Customer until the Customer has fully complied with the request.\n3.5.    The Customer must keep their log-in information secret and carefully secure access to their PriorLabs Account. The Customer shall take reasonable precautions to prevent unauthorized access to the PriorLabs Account, and to protect the Services from unauthorized use. The Customer is obliged to inform PriorLabs immediately if there are indications that a PriorLabs Account has been misused by a third party. The Customer\u2019s liability for any activity of or interaction with a corrupted account is subject to statutory rules.\n4.  Contract Software\n4.1.    PriorLabs has developed the TabPFN foundation models that allows the analysis, processing and evaluation of tabular data (\u201cContract Software\u201d).\n4.2.    PriorLabs may, to the extent available, provide the Customer with Customer documentation for the Contract Software in digital form (e.g. as a pdf file).\n4.3.    PriorLabs provides the Contract Software \"as is\" with the functionality, scope and performance and in a condition suitable for the contractual use.. PriorLabs disclaims any liability of the availability, accuracy, or correctness of the use of the Contract Software and does not warrant the integration in the Customer\u2019s IT systems. \n4.4.    The functionality, scope and performance of the Contract Software may change during the Contract Term (as defined below). PriorLabs reserves the right to add, remove, change or substitute elements of the Contract Software as deemed necessary at any time, in particular for the purpose of increasing efficiency, improvements, additional features, and/or safety or due to changes in the legal situation, technical developments or for reasons of IT security, or cease providing the Services altogether. \n5.  PriorLabs Intellectual Property\n5.1.    PriorLabs remains the sole owner of all right, title, and interest in the Contract Software, including but not limited to any models, algorithms, and neural networks. To the extent PriorLabs provides any Services or access to the Contract Software free of charge, PriorLabs does not waive any rights in such Services or the Contract Software. \n5.2.    Except as stated in these GTC, PriorLabs does not grant the Customer any rights to patents, copyrights, trade secrets, trademarks, or any other rights in respect to the Contract Software. \n5.3.    By using the Contract Software or using any Services, the Customer does not acquire ownership of any rights in the Contract Software, Services, documentation, and/or any related intellectual property other than stated in these GTC.\n6.  API Access \n6.1.    PriorLabs allows registered Customers, as and to the extent available from time to time, access to the Contract Software via an application programming interface (\u201cAPI\u201d), non-exclusively, non-transferable and non-sublicensable to use it exclusively as provided on the PriorLabs website or as described in the Customer documentation for the API (\u201cAPI Access\u201d). \n6.2.    The Customer\u2019s access to and use of the Services must at all times be in accordance with applicable laws and regulations. The Customer is solely responsible for knowing and complying with the applicable laws and regulations. Permitted conditions of use and scope of use of the Services are further set out in the Acceptable Use Policy available under https://www.priorlabs.ai/aup (\u201cAUP\u201d). The Customer acknowledges that the provisions set out in the AUP shall be deemed material obligations under this Contract.\n7.  Customer Content; Licenses\n7.1.    The Customer must own or hold valid rights of sufficient scope to any material, documents, data or other content uploaded into the Services and to be processed by the Contract Software (\u201cCustomer Content\u201d). The Customer Content consists exclusively of non-personal data within the meaning of the General Data Protection Regulation (\u201cGDPR\u201d), as set out in the AUP. \n7.2.    PriorLabs shall take appropriate physical, technical, and organizational security measures with regard to the Contract Software and any Customer Content. \n7.3.    The Customer grants PriorLabs the non-exclusive, worldwide, sublicensable right (i) to use Customer Content for the performance of PriorLabs\u2019 obligations under this Contract and, in particular, to reproduce such data on the server under PriorLabs\u2019 name itself or through a subcontractor for the purpose of providing the Service, and (ii) to use Customer Content as so-called training data in order to develop, test, and improve the Contract Software, in particular the underlying artificial intelligence systems and/or foundation models.\n7.4.    The Customer is fully responsible for all Customer Content uploaded to the Services, in particular the Customer ensures that Customer Content is fit for PriorLabs\u2019 use in accordance with this Contract (including any necessary licenses pursuant to Section 7.3) and does not violate any applicable law or other rights of third parties, in particular copyright, trade secrets, or rights under the GDPR.\n8.  Service Results\n8.1.    The Contract Software may be used to generate certain analyses, content, documents, reports, or other results (\u201cService Results\u201d) based on Customer Content.\n8.2.    The Customer may freely use the Service Results. PriorLabs provides the Service Results \"as is\". The Customer is responsible for reviewing any Service Results of its use of the Contract Software. PriorLabs does not warrant the accuracy, correctness, completeness, usability, or fitness for a certain purpose of the Service Results and does not assume any liability for Customer\u2019s use of Service Results. In particular, PriorLabs disclaims all warranty if the Customer modifies, adapts or combines Service Results with third-party material or products.\n8.3.    PriorLabs may use the Service Results to develop, test and improve the Contract Software, in particular the underlying artificial intelligence systems and/or foundation models.\n9.  Obligations of the Customer\n9.1.    The Customer shall create their own backup copies of Customer Data in case of loss of data. PriorLabs provides a corresponding function for creating backup copies.\n9.2.    The Customer shall inform PriorLabs without undue delay as soon as they become aware of the infringement of an intellectual property right or copyright in the Contract Software.\n9.3.    The Customer shall ensure that all of its employees authorized to use the Contract Software have (i) received sufficient training on the safe use of the Contract Software, (ii) exercise the necessary care when using it, and (iii) are compliant with these GTC including the AUP .\n9.4.    The Customer shall subject any end-users of the Contract Software and the Services to obligations reflecting the stipulations of this Contract, in particular the AUP. \n10. Blocking of Accesses\n10.1.   PriorLabs is entitled to block access to the Contract Software and the Services temporarily or permanently if there are reliable indications that the Customer or, where applicable, one of its employees is violating or has violated material obligations under this GTC, including the Acceptable Use Policy, and/or applicable intellectual property, data protection of other statutory laws or if PriorLabs has another justified interest in the blocking, such as IT-security concerns. \n10.2.   When deciding on a blocking, PriorLabs shall give due consideration to the legitimate interests of the Customer. PriorLabs shall inform the Customer of the blocking within a reasonable timeframe before the blocking comes into effect, provided that the information does not conflict with the purpose of the blocking. The blocking shall continue until the contractual or legal violation has been remedied in an appropriate manner.\n11. Limitation of Liability \n11.1.   The Services are provided free of charge. Therefore, PriorLabs\u2019 liability is in any cases limited to acts of intent or gross negligence.\n11.2.   The strict liability for damages for defects of the Services already existing at the beginning of the Contract Term (as defined below) in terms of Section 536a German Civil Code is excluded. The Services are provided on an \u201cas is\u201d basis, which, in accordance with Section 4 of these GTC, refers in particular to the marketability, availability, and security aspects of the Contract Software.\n12. Indemnity\nThe Customer shall indemnify PriorLabs from any and all claims of end-users or third parties who assert claims against PriorLabs on account of the use of the Services by the Customer or the Customer\u2019s end-users, in particular concerning any Customer Content used in combination with the Contract Software. The provisions of this Section shall apply mutatis mutandis to any liquidated damages (Vertragsstrafen) as well as to any administrative fines (Bu\u00dfgeld) or penalties imposed by the authorities or by the courts, to the extent that the Customer is responsible for such.\n13. Term; Termination of the Contract\n13.1.   If not agreed otherwise, the Contract is concluded for an indefinite period of time until terminated by either Party (\"Contract Term\"). \n13.2.   The Customer may terminate the Contract at any time by deleting its PriorLabs Account. \n13.3.   PriorLabs reserves the right to terminate the Contract at any time but will consider the Customer\u2019s legitimate interests to the extent possible, e.g., by sending the notice of termination in due time to the email address provided by the Customer upon registration of the PriorLabs Account.\n13.4.   The right of PriorLabs and the Customer to extraordinary termination without notice for cause shall remain unaffected.\n14. Changes to this Contract\n14.1.   PriorLabs may change this Contract during the Contract Term in compliance with the following procedure, provided that the amendment is reasonable for the Customer, i.e. without significant legal or economic disadvantages, taking into account the interests of the Customer and that there is a valid reason for the amendment. Such a reason exists, in particular, in cases of new technical developments or changes in the regulatory environment.\n14.2.   PriorLabs shall inform the Customer of any changes to this Contract at least 30 calendar days before the planned entry into force of the changes. The Customer may object to the changes within 30 calendar days from receipt of the notification. If no objection is made and the Customer continues to use the Services after expiry of the objection period, the changes shall be deemed to have been effectively agreed for all Services to be provided from the end of the objection period. In the notification, PriorLabs will inform the Customer of all relevant changes to the Contract, the objection period and the legal consequences of the expiry of the objection period without exercise of the right of objection. If the Customer objects to the changes, PriorLabs may terminate the Contract pursuant to Section 13.\n15. Final Provisions\n15.1.   Should individual provisions of the Contract be or become invalid in whole or in part, this shall not affect the validity of the remaining provisions. Invalid provisions shall be replaced first and foremost by provisions that most closely correspond to the invalid provisions in a legally effective manner. The same applies to any loopholes.\n15.2.   The law of the Federal Republic of Germany shall apply with the exception of its provisions on the choice of law which would lead to the application of another legal system. The validity of the CISG (\"UN Sales Convention\") is excluded. \n15.3.   For Customers who are merchants (Kaufleute) within the meaning of the German Commercial Code (Handelsgesetzbuch), a special fund (Sonderverm\u00f6gen) under public law or a legal entity under public law, Berlin, Germany, shall be the exclusive place of jurisdiction for all disputes arising from the contractual relationship.\n\nStatus: January 2025\n***\n</code></pre>"},{"location":"terms/","title":"General Terms and Conditions","text":"<p>Status: May 2025</p>"},{"location":"terms/#1-scope-of-application","title":"1. Scope of Application","text":"<p>1.1. These general terms and conditions (\"GTC\") govern the provision of access to the TabPFN foundation models available at https://www.priorlabs.ai (\u201cServices\u201d) provided by Prior Labs GmbH, Kaiser-Joseph-Str. 254, 79098 Freiburg (\u201cPriorLabs\u201d).</p> <p>1.2. The Services of PriorLabs are directed exclusively at business customers (Unternehmer) within the meaning of Sec. 14 German Civil Code (B\u00fcrgerliches Gesetzbuch, BGB) (\u201cCustomer\u201d). PriorLabs may require proof of business customer status.</p> <p>1.3. Conflicting or additional contractual conditions of the Customer apply only if PriorLabs expressly confirms them in writing.</p>"},{"location":"terms/#2-conclusion-of-contract","title":"2. Conclusion of Contract","text":"<p>2.1. The contract is concluded with these GTC (\u201cContract\u201d) upon Customer registration and setup of a PriorLabs Account.</p> <p>2.2. The Customer shall provide all necessary and accurate information and inform PriorLabs of any changes.</p>"},{"location":"terms/#3-registration-and-priorlabs-account","title":"3. Registration and PriorLabs Account","text":"<p>3.1. Registration and a PriorLabs Account are required. Each Customer may only register one account and must use it only for themselves.</p> <p>3.2. PriorLabs disclaims liability for any stored Customer data.</p> <p>3.3. Customer must provide complete, accurate, and updated registration information.</p> <p>3.4. PriorLabs may request corrections to inaccurate or outdated information and restrict access if not remedied.</p> <p>3.5. Customer must secure login information and notify PriorLabs of suspected misuse.</p>"},{"location":"terms/#4-contract-software","title":"4. Contract Software","text":"<p>4.1. PriorLabs has developed TabPFN foundation models for analysis and processing of tabular data (\u201cContract Software\u201d).</p> <p>4.2. Customer documentation may be provided digitally.</p> <p>4.3. The Contract Software is provided \u201cas is\u201d and without warranty for accuracy, availability, or integration.</p> <p>4.4. PriorLabs may update, modify, or discontinue elements of the Contract Software as necessary.</p>"},{"location":"terms/#5-priorlabs-intellectual-property","title":"5. PriorLabs Intellectual Property","text":"<p>5.1. PriorLabs retains full rights to the Contract Software, including models and algorithms.</p> <p>5.2. No rights to intellectual property are granted beyond these GTC.</p> <p>5.3. Use of Services does not confer ownership of any rights beyond what is stated herein.</p>"},{"location":"terms/#6-api-access","title":"6. API Access","text":"<p>6.1. Registered Customers may access the Contract Software via API under a non-exclusive, non-transferable, non-sublicensable license.</p> <p>6.2. Use must comply with applicable law and the Acceptable Use Policy (https://www.priorlabs.ai/aup).</p>"},{"location":"terms/#7-customer-content-licenses","title":"7. Customer Content; Licenses","text":"<p>7.1. Customer must hold valid rights to all uploaded content (\u201cCustomer Content\u201d), which must be non-personal data per the GDPR.</p> <p>7.2. PriorLabs shall implement appropriate security measures.</p> <p>7.3. Customer grants PriorLabs rights to use Customer Content to perform services and improve the Contract Software, including training AI systems.</p> <p>7.4. Customer is responsible for ensuring lawful use of Customer Content.</p>"},{"location":"terms/#8-service-results","title":"8. Service Results","text":"<p>8.1. The Contract Software may generate outputs (\u201cService Results\u201d) based on Customer Content.</p> <p>8.2. Customer may use Service Results freely but bears responsibility for their use. PriorLabs disclaims any warranties.</p> <p>8.3. PriorLabs may use Service Results to improve the Contract Software.</p>"},{"location":"terms/#9-obligations-of-the-customer","title":"9. Obligations of the Customer","text":"<p>9.1. Customer shall back up all data independently.</p> <p>9.2. Customer shall promptly report any IP or copyright violations.</p> <p>9.3. Customer shall train and supervise its authorized users.</p> <p>9.4. Customer shall bind end-users to the same obligations under this Contract.</p>"},{"location":"terms/#10-privacy-and-data-protection","title":"10. Privacy and Data Protection","text":"<p>10.1. Data processing by PriorLabs is governed by our Privacy Policy. By using our Services, the Customer consents to the data processing described therein.</p>"},{"location":"terms/#11-blocking-of-accesses","title":"11. Blocking of Accesses","text":"<p>11.1. PriorLabs may block access in cases of violations, legal risks, or justified interest.</p> <p>11.2. Blocking is proportionate and preceded by notice unless it conflicts with the blocking purpose.</p>"},{"location":"terms/#12-limitation-of-liability","title":"12. Limitation of Liability","text":"<p>12.1. Services are provided free of charge. Liability is limited to intent or gross negligence.</p> <p>12.2. Strict liability under Sec. 536a BGB is excluded. Services are \u201cas is.\u201d</p>"},{"location":"terms/#13-indemnity","title":"13. Indemnity","text":"<p>Customer shall indemnify PriorLabs from any third-party claims, penalties, or damages related to use of the Services or Customer Content.</p>"},{"location":"terms/#14-term-termination","title":"14. Term; Termination","text":"<p>14.1. Contract is indefinite unless otherwise agreed.</p> <p>14.2. Customer may terminate by deleting its PriorLabs Account.</p> <p>14.3. PriorLabs may terminate at any time with reasonable consideration of Customer\u2019s interests.</p> <p>14.4. Both parties retain the right to extraordinary termination.</p>"},{"location":"terms/#15-changes-to-this-contract","title":"15. Changes to this Contract","text":"<p>15.1. PriorLabs may amend these GTC with 30 days' notice if reasonable and justified (e.g., legal or technical changes).</p> <p>15.2. Customer may object within 30 days. Continued use after this period implies acceptance. PriorLabs may terminate if the Customer objects.</p>"},{"location":"terms/#16-final-provisions","title":"16. Final Provisions","text":"<p>16.1. Invalid provisions shall be replaced with valid ones closest in meaning. Same applies to gaps.</p> <p>16.2. German law applies. CISG is excluded.</p> <p>16.3. Exclusive jurisdiction is Berlin for business Customers.</p>"},{"location":"getting_started/api/","title":"TabPFN API Guide","text":""},{"location":"getting_started/api/#authentication","title":"Authentication","text":""},{"location":"getting_started/api/#interactive-login","title":"Interactive Login","text":"<p>The first time you use TabPFN, you'll be guided through an interactive login process:</p> <pre><code>from tabpfn_client import init\ninit()\n</code></pre>"},{"location":"getting_started/api/#managing-access-tokens","title":"Managing Access Tokens","text":"<p>You can save your token for use on other machines:</p> <pre><code>import tabpfn_client\n# Get your token\ntoken = tabpfn_client.get_access_token()\n\n# Use token on another machine\ntabpfn_client.set_access_token(token)\n</code></pre>"},{"location":"getting_started/api/#rate-limits","title":"Rate Limits","text":"<p>Our API implements a fair usage system that resets daily at 00:00:00 UTC.</p>"},{"location":"getting_started/api/#usage-cost-calculation","title":"Usage Cost Calculation","text":"<p>The cost for each API request is calculated as: <pre><code>api_cost = (num_train_rows + num_test_rows) * num_cols * n_estimators\n</code></pre></p> <p>Where <code>n_estimators</code> is by default 4 for classification tasks and 8 for regression tasks.</p>"},{"location":"getting_started/api/#monitoring-usage","title":"Monitoring Usage","text":"<p>Track your API usage through response headers:</p> Header Description <code>X-RateLimit-Limit</code> Your total allowed usage <code>X-RateLimit-Remaining</code> Remaining usage <code>X-RateLimit-Reset</code> Reset timestamp (UTC)"},{"location":"getting_started/api/#current-limitations","title":"Current Limitations","text":"<p>Important Data Guidelines</p> <ul> <li>Do NOT upload any Personally Identifiable Information (PII)</li> <li>Do NOT upload any sensitive or confidential data</li> <li>Do NOT upload any data you don't have permission to share</li> <li>Consider anonymizing or pseudonymizing your data</li> <li>Review your organization's data sharing policies</li> </ul>"},{"location":"getting_started/api/#size-limitations","title":"Size Limitations","text":"<ol> <li> <p>Maximum total cells per request must be below 100,000: <pre><code>(num_train_rows + num_test_rows) * num_cols &lt; 100,000\n</code></pre></p> </li> <li> <p>For regression with full output turned on (<code>return_full_output=True</code>), the number of test samples must be below 500.</p> </li> </ol> <p>These limits will be relaxed in future releases.</p>"},{"location":"getting_started/api/#managing-user-data","title":"Managing User Data","text":"<p>You can access and manage your personal information:</p> <pre><code>from tabpfn_client import UserDataClient\nprint(UserDataClient.get_data_summary())\n</code></pre>"},{"location":"getting_started/api/#error-handling","title":"Error Handling","text":"<p>The API uses standard HTTP status codes:</p> Code Meaning 200 Success 400 Invalid request 429 Rate limit exceeded <p>Example response, when limit reached: <pre><code>{\n    \"error\": \"API_LIMIT_REACHED\",\n    \"message\": \"Usage limit exceeded\",\n    \"next_available_at\": \"2024-01-07 00:00:00\"\n}\n</code></pre></p>"},{"location":"getting_started/install/","title":"Installation","text":"<p>Client Local </p> <p>You can access our models through our API (https://github.com/automl/tabpfn-client), via our user interface built on top of the API (https://www.ux.priorlabs.ai/) or locally.</p> Python API Client (No GPU, Online)Python Local (GPU)Web InterfaceR <pre><code>pip install tabpfn-client\n\n# TabPFN Extensions installs optional functionalities around the TabPFN model\n# These include post-hoc ensembles, interpretability tools, and more\ngit clone https://github.com/PriorLabs/tabpfn-extensions\npip install -e tabpfn-extensions\n</code></pre> <pre><code># TabPFN Extensions installs optional functionalities around the TabPFN model\n# These include post-hoc ensembles, interpretability tools, and more\npip install tabpfn\n</code></pre> <p>You can access our models through our Interface here.</p> <p>Warning</p> <p>R support is currently under development. You can find a work in progress at TabPFN R. Looking for contributors!</p>"},{"location":"getting_started/intended_use/","title":"Usage tips","text":"<p>Note</p> <p>For a simple example getting started with classification see classification tutorial.</p> <p>We provide two comprehensive demo notebooks that guides through installation and functionalities. One colab tutorial using the cloud and one colab tutorial using the local GPU.</p>"},{"location":"getting_started/intended_use/#when-to-use-tabpfn","title":"When to use TabPFN","text":"<p>TabPFN excels in handling small to medium-sized datasets with up to 10,000 samples and 500 features. For larger datasets, methods such as CatBoost, XGBoost, or AutoGluon are likely to outperform TabPFN.</p>"},{"location":"getting_started/intended_use/#intended-use-of-tabpfn","title":"Intended Use of TabPFN","text":"<p>TabPFN is intended as a powerful drop-in replacement for traditional tabular data prediction tools, where top performance and fast training matter. It still requires data scientists to prepare the data using their domain knowledge. Data scientists will see benefits in performing feature engineering, data cleaning, and problem framing to get the most out of TabPFN.</p>"},{"location":"getting_started/intended_use/#limitations-of-tabpfn","title":"Limitations of TabPFN","text":"<ol> <li>TabPFN's inference speed may be slower than highly optimized approaches like CatBoost.</li> <li>TabPFN's memory usage scales linearly with dataset size, which can be prohibitive for very large datasets.</li> <li>Our evaluation focused on datasets with up to 10,000 samples and 500 features; scalability to larger datasets requires further study.</li> </ol>"},{"location":"getting_started/intended_use/#computational-and-time-requirements","title":"Computational and Time Requirements","text":"<p>TabPFN is computationally efficient and can run inference on consumer hardware for most datasets. Training on a new dataset is recommended to run on a GPU as this speeds it up significantly. TabPFN is not optimized for real-time inference tasks, but V2 can perform much faster predictions than V1 of TabPFN.</p>"},{"location":"getting_started/intended_use/#data-preparation","title":"Data Preparation","text":"<p>TabPFN can handle raw data with minimal preprocessing. Provide the data in a tabular format, and TabPFN will automatically handle missing values, encode categorical variables, and normalize features. While TabPFN works well out-of-the-box, performance can further be improved using dataset-specific preprocessings.</p>"},{"location":"getting_started/intended_use/#interpreting-results","title":"Interpreting Results","text":"<p>TabPFN's predictions come with uncertainty estimates, allowing you to assess the reliability of the results. You can use SHAP to interpret TabPFN's predictions and identify the most important features driving the model's decisions.</p>"},{"location":"getting_started/intended_use/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>TabPFN provides strong performance out-of-the-box without extensive hyperparameter tuning. If you have additional computational resources, you can automatically tune its hyperparameters using post-hoc ensembling or random tuning.</p>"},{"location":"reference/tabpfn/base/","title":"Base","text":""},{"location":"reference/tabpfn/base/#tabpfn.base","title":"base","text":"<p>Common logic for TabPFN models.</p>"},{"location":"reference/tabpfn/base/#tabpfn.base.check_cpu_warning","title":"check_cpu_warning","text":"<pre><code>check_cpu_warning(\n    device: str | device, X: ndarray | Tensor | DataFrame\n) -&gt; None\n</code></pre> <p>Check if using CPU with large datasets and warn or error appropriately.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str | device</code> <p>The torch device being used</p> required <code>X</code> <code>ndarray | Tensor | DataFrame</code> <p>The input data (NumPy array, Pandas DataFrame, or Torch Tensor)</p> required"},{"location":"reference/tabpfn/base/#tabpfn.base.create_inference_engine","title":"create_inference_engine","text":"<pre><code>create_inference_engine(\n    *,\n    X_train: ndarray,\n    y_train: ndarray,\n    model: PerFeatureTransformer,\n    ensemble_configs: Any,\n    cat_ix: list[int],\n    fit_mode: Literal[\n        \"low_memory\", \"fit_preprocessors\", \"fit_with_cache\"\n    ],\n    device_: device,\n    rng: Generator,\n    n_jobs: int,\n    byte_size: int,\n    forced_inference_dtype_: dtype | None,\n    memory_saving_mode: (\n        bool | Literal[\"auto\"] | float | int\n    ),\n    use_autocast_: bool\n) -&gt; InferenceEngine\n</code></pre> <p>Creates the appropriate TabPFN inference engine based on <code>fit_mode</code>.</p> <p>Each execution mode will perform slightly different operations based on the mode specified by the user. In the case where preprocessors will be fit after <code>prepare</code>, we will use them to further transform the associated borders with each ensemble config member.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Training features</p> required <code>y_train</code> <code>ndarray</code> <p>Training target</p> required <code>model</code> <code>PerFeatureTransformer</code> <p>The loaded TabPFN model.</p> required <code>ensemble_configs</code> <code>Any</code> <p>The ensemble configurations to create multiple \"prompts\".</p> required <code>cat_ix</code> <code>list[int]</code> <p>Indices of inferred categorical features.</p> required <code>fit_mode</code> <code>Literal['low_memory', 'fit_preprocessors', 'fit_with_cache']</code> <p>Determines how we prepare inference (pre-cache or not).</p> required <code>device_</code> <code>device</code> <p>The device for inference.</p> required <code>rng</code> <code>Generator</code> <p>Numpy random generator.</p> required <code>n_jobs</code> <code>int</code> <p>Number of parallel CPU workers.</p> required <code>byte_size</code> <code>int</code> <p>Byte size for the chosen inference precision.</p> required <code>forced_inference_dtype_</code> <code>dtype | None</code> <p>If not None, the forced dtype for inference.</p> required <code>memory_saving_mode</code> <code>bool | Literal['auto'] | float | int</code> <p>GPU/CPU memory saving settings.</p> required <code>use_autocast_</code> <code>bool</code> <p>Whether we use torch.autocast for inference.</p> required"},{"location":"reference/tabpfn/base/#tabpfn.base.determine_precision","title":"determine_precision","text":"<pre><code>determine_precision(\n    inference_precision: (\n        dtype | Literal[\"autocast\", \"auto\"]\n    ),\n    device_: device,\n) -&gt; tuple[bool, dtype | None, int]\n</code></pre> <p>Decide whether to use autocast or a forced precision dtype.</p> <p>Parameters:</p> Name Type Description Default <code>inference_precision</code> <code>dtype | Literal['autocast', 'auto']</code> <ul> <li>If <code>\"auto\"</code>, decide automatically based on the device.</li> <li>If <code>\"autocast\"</code>, explicitly use PyTorch autocast (mixed precision).</li> <li>If a <code>torch.dtype</code>, force that precision.</li> </ul> required <code>device_</code> <code>device</code> <p>The device on which inference is run.</p> required <p>Returns:</p> Name Type Description <code>use_autocast_</code> <code>bool</code> <p>True if mixed-precision autocast will be used.</p> <code>forced_inference_dtype_</code> <code>dtype | None</code> <p>If not None, the forced precision dtype for the model.</p> <code>byte_size</code> <code>int</code> <p>The byte size per element for the chosen precision.</p>"},{"location":"reference/tabpfn/base/#tabpfn.base.initialize_tabpfn_model","title":"initialize_tabpfn_model","text":"<pre><code>initialize_tabpfn_model(\n    model_path: str | Path | Literal[\"auto\"],\n    which: Literal[\"classifier\", \"regressor\"],\n    fit_mode: Literal[\n        \"low_memory\", \"fit_preprocessors\", \"fit_with_cache\"\n    ],\n    static_seed: int,\n) -&gt; tuple[\n    PerFeatureTransformer,\n    InferenceConfig,\n    FullSupportBarDistribution | None,\n]\n</code></pre> <p>Common logic to load the TabPFN model, set up the random state, and optionally download the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str | Path | Literal['auto']</code> <p>Path or directive (\"auto\") to load the pre-trained model from.</p> required <code>which</code> <code>Literal['classifier', 'regressor']</code> <p>Which TabPFN model to load.</p> required <code>fit_mode</code> <code>Literal['low_memory', 'fit_preprocessors', 'fit_with_cache']</code> <p>Determines caching behavior.</p> required <code>static_seed</code> <code>int</code> <p>Random seed for reproducibility logic.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>PerFeatureTransformer</code> <p>The loaded TabPFN model.</p> <code>config</code> <code>InferenceConfig</code> <p>The configuration object associated with the loaded model.</p> <code>bar_distribution</code> <code>FullSupportBarDistribution | None</code> <p>The BarDistribution for regression (<code>None</code> if classifier).</p>"},{"location":"reference/tabpfn/classifier/","title":"Classifier","text":""},{"location":"reference/tabpfn/classifier/#tabpfn.classifier","title":"classifier","text":"<p>TabPFNClassifier class.</p> <p>Example</p> <pre><code>import sklearn.datasets\nfrom tabpfn import TabPFNClassifier\n\nmodel = TabPFNClassifier()\n\nX, y = sklearn.datasets.load_iris(return_X_y=True)\n\nmodel.fit(X, y)\npredictions = model.predict(X)\n</code></pre>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier","title":"TabPFNClassifier","text":"<p>             Bases: <code>ClassifierMixin</code>, <code>BaseEstimator</code></p> <p>TabPFNClassifier class.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.class_counts_","title":"class_counts_  <code>instance-attribute</code>","text":"<pre><code>class_counts_: NDArray[Any]\n</code></pre> <p>The number of classes per class found in the target data during <code>fit()</code>.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.classes_","title":"classes_  <code>instance-attribute</code>","text":"<pre><code>classes_: NDArray[Any]\n</code></pre> <p>The unique classes found in the target data during <code>fit()</code>.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.config_","title":"config_  <code>instance-attribute</code>","text":"<pre><code>config_: InferenceConfig\n</code></pre> <p>The configuration of the loaded model to be used for inference.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.device_","title":"device_  <code>instance-attribute</code>","text":"<pre><code>device_: device\n</code></pre> <p>The device determined to be used.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.executor_","title":"executor_  <code>instance-attribute</code>","text":"<pre><code>executor_: InferenceEngine\n</code></pre> <p>The inference engine used to make predictions.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.feature_names_in_","title":"feature_names_in_  <code>instance-attribute</code>","text":"<pre><code>feature_names_in_: NDArray[Any]\n</code></pre> <p>The feature names of the input data.</p> <p>May not be set if the input data does not have feature names, such as with a numpy array.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.forced_inference_dtype_","title":"forced_inference_dtype_  <code>instance-attribute</code>","text":"<pre><code>forced_inference_dtype_: _dtype | None\n</code></pre> <p>The forced inference dtype for the model based on <code>inference_precision</code>.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.inferred_categorical_indices_","title":"inferred_categorical_indices_  <code>instance-attribute</code>","text":"<pre><code>inferred_categorical_indices_: list[int]\n</code></pre> <p>The indices of the columns that were inferred to be categorical, as a product of any features deemed categorical by the user and what would work best for the model.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.interface_config_","title":"interface_config_  <code>instance-attribute</code>","text":"<pre><code>interface_config_: ModelInterfaceConfig\n</code></pre> <p>Additional configuration of the interface for expert users.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.label_encoder_","title":"label_encoder_  <code>instance-attribute</code>","text":"<pre><code>label_encoder_: LabelEncoder\n</code></pre> <p>The label encoder used to encode the target variable.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.n_classes_","title":"n_classes_  <code>instance-attribute</code>","text":"<pre><code>n_classes_: int\n</code></pre> <p>The number of classes found in the target data during <code>fit()</code>.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.n_features_in_","title":"n_features_in_  <code>instance-attribute</code>","text":"<pre><code>n_features_in_: int\n</code></pre> <p>The number of features in the input data used during <code>fit()</code>.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.n_outputs_","title":"n_outputs_  <code>instance-attribute</code>","text":"<pre><code>n_outputs_: Literal[1]\n</code></pre> <p>The number of outputs the model has. Only 1 for now</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.preprocessor_","title":"preprocessor_  <code>instance-attribute</code>","text":"<pre><code>preprocessor_: ColumnTransformer\n</code></pre> <p>The column transformer used to preprocess the input data to be numeric.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.use_autocast_","title":"use_autocast_  <code>instance-attribute</code>","text":"<pre><code>use_autocast_: bool\n</code></pre> <p>Whether torch's autocast should be used.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.fit","title":"fit","text":"<pre><code>fit(X: XType, y: YType) -&gt; Self\n</code></pre> <p>Fit the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input data.</p> required <code>y</code> <code>YType</code> <p>The target variable.</p> required"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    X: XType, data_source: Literal[\"train\", \"test\"] = \"test\"\n) -&gt; ndarray\n</code></pre> <p>Get the embeddings for the input data <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input data.</p> required <code>data_source</code> <code>str</code> <p>Extract either the train or test embeddings</p> <code>'test'</code>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X: XType) -&gt; ndarray\n</code></pre> <p>Predict the class labels for the provided input samples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input samples.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The predicted class labels.</p>"},{"location":"reference/tabpfn/classifier/#tabpfn.classifier.TabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X: XType) -&gt; ndarray\n</code></pre> <p>Predict the probabilities of the classes for the provided input samples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input data.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The predicted probabilities of the classes.</p>"},{"location":"reference/tabpfn/config/","title":"Config","text":""},{"location":"reference/tabpfn/config/#tabpfn.config","title":"config","text":"<p>Configuration for the model interfaces.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig","title":"ModelInterfaceConfig  <code>dataclass</code>","text":"<p>Constants used as default HPs in the model interfaces.</p> <p>These constants are not exposed to the models' init on purpose to reduce the complexity for users. Furthermore, most of these should not be optimized over by the (standard) user.</p> <p>Several of the preprocessing options are supported by our code for efficiency reasons (to avoid loading TabPFN multiple times). However, these can also be applied outside of the model interface.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.CLASS_SHIFT_METHOD","title":"CLASS_SHIFT_METHOD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLASS_SHIFT_METHOD: Literal[\"rotate\", \"shuffle\"] | None = (\n    \"shuffle\"\n)\n</code></pre> <p>The method used to shift classes during preprocessing for ensembling to emulate the effect of invariance to class order. Without ensembling, TabPFN is not invariant to class order due to using a transformer. Shifting classes can have a positive effect on the model's performance. The options are:     - If \"shuffle\", the classes are shuffled.     - If \"rotate\", the classes are rotated (think of a ring).     - If None, no class shifting is done.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.FEATURE_SHIFT_METHOD","title":"FEATURE_SHIFT_METHOD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FEATURE_SHIFT_METHOD: (\n    Literal[\"shuffle\", \"rotate\"] | None\n) = \"shuffle\"\n</code></pre> <p>The method used to shift features during preprocessing for ensembling to emulate the effect of invariance to feature position. Without ensembling, TabPFN is not invariant to feature position due to using a transformer. Moreover, shifting features can have a positive effect on the model's performance. The options are:    - If \"shuffle\", the features are shuffled.    - If \"rotate\", the features are rotated (think of a ring).    - If None, no feature shifting is done.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.FINGERPRINT_FEATURE","title":"FINGERPRINT_FEATURE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FINGERPRINT_FEATURE: bool = True\n</code></pre> <p>Whether to add a fingerprint feature to the data. The added feature is a hash of the row, counting up for duplicates. This helps TabPFN to distinguish between duplicated data points in the input data. Otherwise, duplicates would be less obvious during attention. This is expected to improve prediction performance and help with stability if the data has many sample duplicates.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.FIX_NAN_BORDERS_AFTER_TARGET_TRANSFORM","title":"FIX_NAN_BORDERS_AFTER_TARGET_TRANSFORM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FIX_NAN_BORDERS_AFTER_TARGET_TRANSFORM: bool = True\n</code></pre> <p>Whether to repair any borders of the bar distribution in regression that are NaN after the transformation. This can happen due to multiple reasons and should in general always be done.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.MAX_NUMBER_OF_CLASSES","title":"MAX_NUMBER_OF_CLASSES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_NUMBER_OF_CLASSES: int = 10\n</code></pre> <p>The number of classes seen during pretraining for classification. If the number of classes is larger than this number, TabPFN requires an additional step to predict for more than classes.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.MAX_NUMBER_OF_FEATURES","title":"MAX_NUMBER_OF_FEATURES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_NUMBER_OF_FEATURES: int = 500\n</code></pre> <p>The number of features that the pretraining was intended for. If the number of features is larger than this number, you may see degraded performance. Note, this is not the number of features seen by the model during pretraining but also accounts for expected generalization (i.e., length extrapolation).</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.MAX_NUMBER_OF_SAMPLES","title":"MAX_NUMBER_OF_SAMPLES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_NUMBER_OF_SAMPLES: int = 10000\n</code></pre> <p>The number of samples that the pretraining was intended for. If the number of samples is larger than this number, you may see degraded performance. Note, this is not the number of samples seen by the model during pretraining but also accounts for expected generalization (i.e., length extrapolation).</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.MAX_UNIQUE_FOR_CATEGORICAL_FEATURES","title":"MAX_UNIQUE_FOR_CATEGORICAL_FEATURES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_UNIQUE_FOR_CATEGORICAL_FEATURES: int = 30\n</code></pre> <p>The maximum number of unique values for a feature to be considered categorical. Otherwise, it is considered numerical.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.MIN_NUMBER_SAMPLES_FOR_CATEGORICAL_INFERENCE","title":"MIN_NUMBER_SAMPLES_FOR_CATEGORICAL_INFERENCE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MIN_NUMBER_SAMPLES_FOR_CATEGORICAL_INFERENCE: int = 100\n</code></pre> <p>The minimum number of samples in the data to run our infer which features might be categorical.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.MIN_UNIQUE_FOR_NUMERICAL_FEATURES","title":"MIN_UNIQUE_FOR_NUMERICAL_FEATURES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MIN_UNIQUE_FOR_NUMERICAL_FEATURES: int = 4\n</code></pre> <p>The minimum number of unique values for a feature to be considered numerical. Otherwise, it is considered categorical.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.OUTLIER_REMOVAL_STD","title":"OUTLIER_REMOVAL_STD  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OUTLIER_REMOVAL_STD: float | None | Literal[\"auto\"] = \"auto\"\n</code></pre> <p>The number of standard deviations from the mean to consider a sample an outlier. - If None, no outliers are removed. - If float, the number of standard deviations from the mean to consider a sample     an outlier. - If \"auto\", the OUTLIER_REMOVAL_STD is automatically determined.     -&gt; 12.0 for classification and None for regression.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.POLYNOMIAL_FEATURES","title":"POLYNOMIAL_FEATURES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>POLYNOMIAL_FEATURES: Literal['no', 'all'] | int = 'no'\n</code></pre> <p>The number of 2 factor polynomial features to generate and add to the original data before passing the data to TabPFN. The polynomial features are generated by multiplying the original features together, e.g., this might add a feature <code>x1*x2</code> to the features, if <code>x1</code> and <code>x2</code> are features. In  total, this can add up O(n^2) many features. Adding polynomial features can  improve predictive performance by exploiting simple feature engineering.     - If \"no\", no polynomial features are added.     - If \"all\", all possible polynomial features are added.     - If an int, determines the maximal number of polynomial features to add to the      original data.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.PREPROCESS_TRANSFORMS","title":"PREPROCESS_TRANSFORMS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PREPROCESS_TRANSFORMS: (\n    list[PreprocessorConfig | dict] | None\n) = None\n</code></pre> <p>The preprocessing applied to the data before passing it to TabPFN. See <code>PreprocessorConfig</code> for options and more details. If a list of <code>PreprocessorConfig</code> is provided, the preprocessors are (repeatedly) applied across different estimators.</p> <p>By default, for classification, two preprocessors are applied:     1. Uses the original input data, all features transformed with a quantile         scaler, and the first n-many components of SVD transformer (whereby         n is a fract of on the number of features or samples). Categorical features         are ordinal encoded but all categories with less than 10 features are         ignored.     2. Uses the original input data, with categorical features as ordinal encoded.</p> <p>By default, for regression, two preprocessor are applied:     1. The same as for classification, with a minimal different quantile scaler.     2. The original input data power transformed and categories onehot encoded.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.REGRESSION_Y_PREPROCESS_TRANSFORMS","title":"REGRESSION_Y_PREPROCESS_TRANSFORMS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REGRESSION_Y_PREPROCESS_TRANSFORMS: tuple[\n    Literal[\"safepower\", \"power\", \"quantile_norm\", None],\n    ...,\n] = (None, \"safepower\")\n</code></pre> <p>The preprocessing applied to the target variable before passing it to TabPFN for regression. This can be understood as scaling the target variable to better predict it. The preprocessors should be passed as a tuple/list and are then (repeatedly) used by the estimators in the ensembles.</p> <p>By default, we use no preprocessing and a power transformation (if we have more than one estimator).</p> The options are <ul> <li>If None, no preprocessing is done.</li> <li>If \"power\", a power transformation is applied.</li> <li>If \"safepower\", a power transformation is applied with a safety factor to     avoid numerical issues.</li> <li>If \"quantile_norm\", a quantile normalization is applied.</li> </ul>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.SUBSAMPLE_SAMPLES","title":"SUBSAMPLE_SAMPLES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SUBSAMPLE_SAMPLES: int | float | None = None\n</code></pre> <p>Subsample the input data sample/row-wise before performing any preprocessing and the TabPFN forward pass.     - If None, no subsampling is done.     - If an int, the number of samples to subsample (or oversample if         <code>SUBSAMPLE_SAMPLES</code> is larger than the number of samples).     - If a float, the percentage of samples to subsample.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.USE_SKLEARN_16_DECIMAL_PRECISION","title":"USE_SKLEARN_16_DECIMAL_PRECISION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>USE_SKLEARN_16_DECIMAL_PRECISION: bool = False\n</code></pre> <p>Whether to round the probabilities to float 16 to match the precision of scikit-learn. This can help with reproducibility and compatibility with scikit-learn but is not recommended for general use. This is not exposed to the user or as a hyperparameter. To improve reproducibility,set <code>._sklearn_16_decimal_precision = True</code> before calling <code>.predict()</code> or <code>.predict_proba()</code>.</p>"},{"location":"reference/tabpfn/config/#tabpfn.config.ModelInterfaceConfig.from_user_input","title":"from_user_input  <code>staticmethod</code>","text":"<pre><code>from_user_input(\n    *, inference_config: dict | ModelInterfaceConfig | None\n) -&gt; ModelInterfaceConfig\n</code></pre> <p>Converts the user input to a <code>ModelInterfaceConfig</code> object.</p> <p>The input inference_config can be a dictionary, a <code>ModelInterfaceConfig</code> object, or None. If a dictionary is passed, the keys must match the attributes of <code>ModelInterfaceConfig</code>. If a <code>ModelInterfaceConfig</code> object is passed, it is returned as is. If None is passed, a new <code>ModelInterfaceConfig</code> object is created with default values.</p>"},{"location":"reference/tabpfn/constants/","title":"Constants","text":""},{"location":"reference/tabpfn/constants/#tabpfn.constants","title":"constants","text":"<p>Various constants used throughout the library.</p>"},{"location":"reference/tabpfn/inference/","title":"Inference","text":""},{"location":"reference/tabpfn/inference/#tabpfn.inference","title":"inference","text":"<p>Module that defines different ways to run inference with TabPFN.</p>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngine","title":"InferenceEngine  <code>dataclass</code>","text":"<p>             Bases: <code>ABC</code></p> <p>These define how tabpfn inference can be run.</p> <p>As there are many things that can be cached, with multiple ways to parallelize, <code>Executor</code> defines three primary things:</p> <p>Most will define a method <code>prepare()</code> which is specific to that inference engine. These do not share a common interface.</p> <ol> <li> <p>What to cache:</p> <p>As we can prepare a lot of the transformers context, there is a tradeoff in terms of how much memory to be spent in caching. This memory is used when <code>prepare()</code> is called, usually in <code>fit()</code>.</p> </li> <li> <p>Using the cached data for inference:</p> <p>Based on what has been prepared for the transformer context, <code>iter_outputs()</code> will use this cached information to make predictions.</p> </li> <li> <p>Controlling parallelism:</p> <p>As we have trivially parallel parts for inference, we can parallelize them. However as the GPU is typically a bottle-neck in most systems, we can define, where and how we would like to parallelize the inference.</p> </li> </ol>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngine.iter_outputs","title":"iter_outputs  <code>abstractmethod</code>","text":"<pre><code>iter_outputs(\n    X: ndarray, *, device: device, autocast: bool\n) -&gt; Iterator[tuple[Tensor, EnsembleConfig]]\n</code></pre> <p>Iterate over the outputs of the model.</p> <p>One for each ensemble configuration that was used to initialize the executor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data to make predictions on.</p> required <code>device</code> <code>device</code> <p>The device to run the model on.</p> required <code>autocast</code> <code>bool</code> <p>Whether to use torch.autocast during inference.</p> required"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineCacheKV","title":"InferenceEngineCacheKV  <code>dataclass</code>","text":"<p>             Bases: <code>InferenceEngine</code></p> <p>Inference engine that caches the actual KV cache calculated from the context of the processed training data.</p> <p>This is by far the most memory intensive inference engine, as for each ensemble member we store the full KV cache of that model. For now this is held in CPU RAM (TODO(eddiebergman): verify)</p>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineCacheKV.prepare","title":"prepare  <code>classmethod</code>","text":"<pre><code>prepare(\n    X_train: ndarray,\n    y_train: ndarray,\n    *,\n    cat_ix: list[int],\n    ensemble_configs: Sequence[EnsembleConfig],\n    n_workers: int,\n    model: PerFeatureTransformer,\n    device: device,\n    rng: Generator,\n    dtype_byte_size: int,\n    force_inference_dtype: dtype | None,\n    save_peak_mem: bool | Literal[\"auto\"] | float | int,\n    autocast: bool,\n    only_return_standard_out: bool = True\n) -&gt; InferenceEngineCacheKV\n</code></pre> <p>Prepare the inference engine.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>The training data.</p> required <code>y_train</code> <code>ndarray</code> <p>The training target.</p> required <code>cat_ix</code> <code>list[int]</code> <p>The categorical indices.</p> required <code>ensemble_configs</code> <code>Sequence[EnsembleConfig]</code> <p>The ensemble configurations to use.</p> required <code>n_workers</code> <code>int</code> <p>The number of workers to use.</p> required <code>model</code> <code>PerFeatureTransformer</code> <p>The model to use.</p> required <code>device</code> <code>device</code> <p>The device to run the model on.</p> required <code>rng</code> <code>Generator</code> <p>The random number generator.</p> required <code>dtype_byte_size</code> <code>int</code> <p>Size of the dtype in bytes.</p> required <code>force_inference_dtype</code> <code>dtype | None</code> <p>The dtype to force inference to.</p> required <code>save_peak_mem</code> <code>bool | Literal['auto'] | float | int</code> <p>Whether to save peak memory usage.</p> required <code>autocast</code> <code>bool</code> <p>Whether to use torch.autocast during inference.</p> required <code>only_return_standard_out</code> <code>bool</code> <p>Whether to only return the standard output</p> <code>True</code>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineCachePreprocessing","title":"InferenceEngineCachePreprocessing  <code>dataclass</code>","text":"<p>             Bases: <code>InferenceEngine</code></p> <p>Inference engine that caches the preprocessing for feeding as model context on predict.</p> <p>This will fit the preprocessors on the training data, as well as cache the transformed training data on RAM (not GPU RAM).</p> <p>This saves some time on each predict call, at the cost of increasing the amount of memory in RAM. The main functionality performed at <code>predict()</code> time is to forward pass through the model which is currently done sequentially.</p>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineCachePreprocessing.prepare","title":"prepare  <code>classmethod</code>","text":"<pre><code>prepare(\n    X_train: ndarray,\n    y_train: ndarray,\n    *,\n    cat_ix: list[int],\n    model: PerFeatureTransformer,\n    ensemble_configs: Sequence[EnsembleConfig],\n    n_workers: int,\n    rng: Generator,\n    dtype_byte_size: int,\n    force_inference_dtype: dtype | None,\n    save_peak_mem: bool | Literal[\"auto\"] | float | int\n) -&gt; InferenceEngineCachePreprocessing\n</code></pre> <p>Prepare the inference engine.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>The training data.</p> required <code>y_train</code> <code>ndarray</code> <p>The training target.</p> required <code>cat_ix</code> <code>list[int]</code> <p>The categorical indices.</p> required <code>model</code> <code>PerFeatureTransformer</code> <p>The model to use.</p> required <code>ensemble_configs</code> <code>Sequence[EnsembleConfig]</code> <p>The ensemble configurations to use.</p> required <code>n_workers</code> <code>int</code> <p>The number of workers to use.</p> required <code>rng</code> <code>Generator</code> <p>The random number generator.</p> required <code>dtype_byte_size</code> <code>int</code> <p>The byte size of the dtype.</p> required <code>force_inference_dtype</code> <code>dtype | None</code> <p>The dtype to force inference to.</p> required <code>save_peak_mem</code> <code>bool | Literal['auto'] | float | int</code> <p>Whether to save peak memory usage.</p> required <p>Returns:</p> Type Description <code>InferenceEngineCachePreprocessing</code> <p>The prepared inference engine.</p>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineOnDemand","title":"InferenceEngineOnDemand  <code>dataclass</code>","text":"<p>             Bases: <code>InferenceEngine</code></p> <p>Inference engine that does not cache anything, computes everything as needed.</p> <p>This is one of the slowest ways to run inference, as computation that could be cached is recomputed on every call. However the memory demand is lowest and can be more trivially parallelized across GPUs with some work.</p>"},{"location":"reference/tabpfn/inference/#tabpfn.inference.InferenceEngineOnDemand.prepare","title":"prepare  <code>classmethod</code>","text":"<pre><code>prepare(\n    X_train: ndarray,\n    y_train: ndarray,\n    *,\n    cat_ix: list[int],\n    model: PerFeatureTransformer,\n    ensemble_configs: Sequence[EnsembleConfig],\n    rng: Generator,\n    n_workers: int,\n    dtype_byte_size: int,\n    force_inference_dtype: dtype | None,\n    save_peak_mem: bool | Literal[\"auto\"] | float | int\n) -&gt; InferenceEngineOnDemand\n</code></pre> <p>Prepare the inference engine.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>The training data.</p> required <code>y_train</code> <code>ndarray</code> <p>The training target.</p> required <code>cat_ix</code> <code>list[int]</code> <p>The categorical indices.</p> required <code>model</code> <code>PerFeatureTransformer</code> <p>The model to use.</p> required <code>ensemble_configs</code> <code>Sequence[EnsembleConfig]</code> <p>The ensemble configurations to use.</p> required <code>rng</code> <code>Generator</code> <p>The random number generator.</p> required <code>n_workers</code> <code>int</code> <p>The number of workers to use.</p> required <code>dtype_byte_size</code> <code>int</code> <p>The byte size of the dtype.</p> required <code>force_inference_dtype</code> <code>dtype | None</code> <p>The dtype to force inference to.</p> required <code>save_peak_mem</code> <code>bool | Literal['auto'] | float | int</code> <p>Whether to save peak memory usage.</p> required"},{"location":"reference/tabpfn/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing","title":"preprocessing","text":"<p>Defines the preprocessing configurations that define the ensembling of different members.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.ClassifierEnsembleConfig","title":"ClassifierEnsembleConfig  <code>dataclass</code>","text":"<p>             Bases: <code>EnsembleConfig</code></p> <p>Configuration for a classifier ensemble member.</p> <p>See EnsembleConfig for more details.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.ClassifierEnsembleConfig.generate_for_classification","title":"generate_for_classification  <code>classmethod</code>","text":"<pre><code>generate_for_classification(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    class_shift_method: Literal[\"rotate\", \"shuffle\"] | None,\n    n_classes: int,\n    random_state: int | Generator | None\n) -&gt; list[ClassifierEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for classification.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>class_shift_method</code> <code>Literal['rotate', 'shuffle'] | None</code> <p>How to shift classes for classpermutation.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[ClassifierEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.ClassifierEnsembleConfig.generate_for_regression","title":"generate_for_regression  <code>classmethod</code>","text":"<pre><code>generate_for_regression(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    target_transforms: Sequence[\n        TransformerMixin | Pipeline | None\n    ],\n    random_state: int | Generator | None\n) -&gt; list[RegressorEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for regression.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>target_transforms</code> <code>Sequence[TransformerMixin | Pipeline | None]</code> <p>Target transformations to apply.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[RegressorEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.ClassifierEnsembleConfig.to_pipeline","title":"to_pipeline","text":"<pre><code>to_pipeline(\n    *, random_state: int | Generator | None\n) -&gt; SequentialFeatureTransformer\n</code></pre> <p>Convert the ensemble configuration to a preprocessing pipeline.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.EnsembleConfig","title":"EnsembleConfig  <code>dataclass</code>","text":"<p>Configuration for an ensemble member.</p> <p>Attributes:</p> Name Type Description <code>feature_shift_count</code> <code>int</code> <p>How much to shift the features columns.</p> <code>class_permutation</code> <code>int</code> <p>Permutation to apply to classes</p> <code>preprocess_config</code> <code>PreprocessorConfig</code> <p>Preprocessor configuration to use.</p> <code>subsample_ix</code> <code>NDArray[int64] | None</code> <p>Indices of samples to use for this ensemble member. If <code>None</code>, no subsampling is done.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.EnsembleConfig.generate_for_classification","title":"generate_for_classification  <code>classmethod</code>","text":"<pre><code>generate_for_classification(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    class_shift_method: Literal[\"rotate\", \"shuffle\"] | None,\n    n_classes: int,\n    random_state: int | Generator | None\n) -&gt; list[ClassifierEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for classification.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>class_shift_method</code> <code>Literal['rotate', 'shuffle'] | None</code> <p>How to shift classes for classpermutation.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[ClassifierEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.EnsembleConfig.generate_for_regression","title":"generate_for_regression  <code>classmethod</code>","text":"<pre><code>generate_for_regression(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    target_transforms: Sequence[\n        TransformerMixin | Pipeline | None\n    ],\n    random_state: int | Generator | None\n) -&gt; list[RegressorEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for regression.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>target_transforms</code> <code>Sequence[TransformerMixin | Pipeline | None]</code> <p>Target transformations to apply.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[RegressorEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.EnsembleConfig.to_pipeline","title":"to_pipeline","text":"<pre><code>to_pipeline(\n    *, random_state: int | Generator | None\n) -&gt; SequentialFeatureTransformer\n</code></pre> <p>Convert the ensemble configuration to a preprocessing pipeline.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.PreprocessorConfig","title":"PreprocessorConfig  <code>dataclass</code>","text":"<p>Configuration for data preprocessors.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['per_feature', 'power', 'safepower', 'power_box', 'safepower_box', 'quantile_uni_coarse', 'quantile_norm_coarse', 'quantile_uni', 'quantile_norm', 'quantile_uni_fine', 'quantile_norm_fine', 'robust', 'kdi', 'none', 'kdi_random_alpha', 'kdi_uni', 'kdi_random_alpha_uni', 'adaptive', 'norm_and_kdi', 'kdi_alpha_0.3_uni', 'kdi_alpha_0.5_uni', 'kdi_alpha_0.8_uni', 'kdi_alpha_1.0_uni', 'kdi_alpha_1.2_uni', 'kdi_alpha_1.5_uni', 'kdi_alpha_2.0_uni', 'kdi_alpha_3.0_uni', 'kdi_alpha_5.0_uni', 'kdi_alpha_0.3', 'kdi_alpha_0.5', 'kdi_alpha_0.8', 'kdi_alpha_1.0', 'kdi_alpha_1.2', 'kdi_alpha_1.5', 'kdi_alpha_2.0', 'kdi_alpha_3.0', 'kdi_alpha_5.0']</code> <p>Name of the preprocessor.</p> <code>categorical_name</code> <code>Literal['none', 'numeric', 'onehot', 'ordinal', 'ordinal_shuffled', 'ordinal_very_common_categories_shuffled']</code> <p>Name of the categorical encoding method. Options: \"none\", \"numeric\", \"onehot\", \"ordinal\", \"ordinal_shuffled\", \"none\".</p> <code>append_original</code> <code>bool</code> <p>Whether to append original features to the transformed features</p> <code>subsample_features</code> <code>float</code> <p>Fraction of features to subsample. -1 means no subsampling.</p> <code>global_transformer_name</code> <code>str | None</code> <p>Name of the global transformer to use.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.PreprocessorConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict: dict) -&gt; PreprocessorConfig\n</code></pre> <p>Create a config from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>Dictionary containing the config parameters.</p> required <p>Returns:</p> Type Description <code>PreprocessorConfig</code> <p>PreprocessorConfig instance.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.PreprocessorConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert the config to a dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary representation of the config.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.RegressorEnsembleConfig","title":"RegressorEnsembleConfig  <code>dataclass</code>","text":"<p>             Bases: <code>EnsembleConfig</code></p> <p>Configuration for a regression ensemble member.</p> <p>See EnsembleConfig for more details.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.RegressorEnsembleConfig.generate_for_classification","title":"generate_for_classification  <code>classmethod</code>","text":"<pre><code>generate_for_classification(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    class_shift_method: Literal[\"rotate\", \"shuffle\"] | None,\n    n_classes: int,\n    random_state: int | Generator | None\n) -&gt; list[ClassifierEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for classification.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>class_shift_method</code> <code>Literal['rotate', 'shuffle'] | None</code> <p>How to shift classes for classpermutation.</p> required <code>n_classes</code> <code>int</code> <p>Number of classes.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[ClassifierEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.RegressorEnsembleConfig.generate_for_regression","title":"generate_for_regression  <code>classmethod</code>","text":"<pre><code>generate_for_regression(\n    *,\n    n: int,\n    subsample_size: int | float | None,\n    max_index: int,\n    add_fingerprint_feature: bool,\n    polynomial_features: Literal[\"no\", \"all\"] | int,\n    feature_shift_decoder: (\n        Literal[\"shuffle\", \"rotate\"] | None\n    ),\n    preprocessor_configs: Sequence[PreprocessorConfig],\n    target_transforms: Sequence[\n        TransformerMixin | Pipeline | None\n    ],\n    random_state: int | Generator | None\n) -&gt; list[RegressorEnsembleConfig]\n</code></pre> <p>Generate ensemble configurations for regression.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of ensemble configurations to generate.</p> required <code>subsample_size</code> <code>int | float | None</code> <p>Number of samples to subsample. If int, subsample that many samples. If float, subsample that fraction of samples. If <code>None</code>, no subsampling is done.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate for.</p> required <code>add_fingerprint_feature</code> <code>bool</code> <p>Whether to add fingerprint features.</p> required <code>polynomial_features</code> <code>Literal['no', 'all'] | int</code> <p>Maximum number of polynomial features to add, if any.</p> required <code>feature_shift_decoder</code> <code>Literal['shuffle', 'rotate'] | None</code> <p>How shift features</p> required <code>preprocessor_configs</code> <code>Sequence[PreprocessorConfig]</code> <p>Preprocessor configurations to use on the data.</p> required <code>target_transforms</code> <code>Sequence[TransformerMixin | Pipeline | None]</code> <p>Target transformations to apply.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[RegressorEnsembleConfig]</code> <p>List of ensemble configurations.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.RegressorEnsembleConfig.to_pipeline","title":"to_pipeline","text":"<pre><code>to_pipeline(\n    *, random_state: int | Generator | None\n) -&gt; SequentialFeatureTransformer\n</code></pre> <p>Convert the ensemble configuration to a preprocessing pipeline.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.balance","title":"balance","text":"<pre><code>balance(x: Iterable[T], n: int) -&gt; list[T]\n</code></pre> <p>Take a list of elements and make a new list where each appears <code>n</code> times.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.default_classifier_preprocessor_configs","title":"default_classifier_preprocessor_configs","text":"<pre><code>default_classifier_preprocessor_configs() -&gt; (\n    list[PreprocessorConfig]\n)\n</code></pre> <p>Default preprocessor configurations for classification.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.default_regressor_preprocessor_configs","title":"default_regressor_preprocessor_configs","text":"<pre><code>default_regressor_preprocessor_configs() -&gt; (\n    list[PreprocessorConfig]\n)\n</code></pre> <p>Default preprocessor configurations for regression.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.fit_preprocessing","title":"fit_preprocessing","text":"<pre><code>fit_preprocessing(\n    configs: Sequence[EnsembleConfig],\n    X_train: ndarray,\n    y_train: ndarray,\n    *,\n    random_state: int | Generator | None,\n    cat_ix: list[int],\n    n_workers: int,\n    parallel_mode: Literal[\"block\", \"as-ready\", \"in-order\"]\n) -&gt; Iterator[\n    tuple[\n        EnsembleConfig,\n        SequentialFeatureTransformer,\n        ndarray,\n        ndarray,\n        list[int],\n    ]\n]\n</code></pre> <p>Fit preprocessing pipelines in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>configs</code> <code>Sequence[EnsembleConfig]</code> <p>List of ensemble configurations.</p> required <code>X_train</code> <code>ndarray</code> <p>Training data.</p> required <code>y_train</code> <code>ndarray</code> <p>Training target.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <code>cat_ix</code> <code>list[int]</code> <p>Indices of categorical features.</p> required <code>n_workers</code> <code>int</code> <p>Number of workers to use.</p> required <code>parallel_mode</code> <code>Literal['block', 'as-ready', 'in-order']</code> <p>Parallel mode to use.</p> <ul> <li><code>\"block\"</code>: Blocks until all workers are done. Returns in order.</li> <li><code>\"as-ready\"</code>: Returns results as they are ready. Any order.</li> <li><code>\"in-order\"</code>: Returns results in order, blocking only in the order that     needs to be returned in.</li> </ul> required <p>Returns:</p> Type Description <code>EnsembleConfig</code> <p>Iterator of tuples containing the ensemble configuration, the fitted</p> <code>SequentialFeatureTransformer</code> <p>preprocessing pipeline, the transformed training data, the transformed target,</p> <code>ndarray</code> <p>and the indices of categorical features.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.fit_preprocessing_one","title":"fit_preprocessing_one","text":"<pre><code>fit_preprocessing_one(\n    config: EnsembleConfig,\n    X_train: ndarray,\n    y_train: ndarray,\n    random_state: int | Generator | None = None,\n    *,\n    cat_ix: list[int]\n) -&gt; tuple[\n    EnsembleConfig,\n    SequentialFeatureTransformer,\n    ndarray,\n    ndarray,\n    list[int],\n]\n</code></pre> <p>Fit preprocessing pipeline for a single ensemble configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EnsembleConfig</code> <p>Ensemble configuration.</p> required <code>X_train</code> <code>ndarray</code> <p>Training data.</p> required <code>y_train</code> <code>ndarray</code> <p>Training target.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random seed.</p> <code>None</code> <code>cat_ix</code> <code>list[int]</code> <p>Indices of categorical features.</p> required <p>Returns:</p> Type Description <code>EnsembleConfig</code> <p>Tuple containing the ensemble configuration, the fitted preprocessing pipeline,</p> <code>SequentialFeatureTransformer</code> <p>the transformed training data, the transformed target, and the indices of</p> <code>ndarray</code> <p>categorical features.</p>"},{"location":"reference/tabpfn/preprocessing/#tabpfn.preprocessing.generate_index_permutations","title":"generate_index_permutations","text":"<pre><code>generate_index_permutations(\n    n: int,\n    *,\n    max_index: int,\n    subsample: int | float,\n    random_state: int | Generator | None\n) -&gt; list[NDArray[int64]]\n</code></pre> <p>Generate indices for subsampling from the data.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of indices to generate.</p> required <code>max_index</code> <code>int</code> <p>Maximum index to generate.</p> required <code>subsample</code> <code>int | float</code> <p>Number of indices to subsample. If <code>int</code>, subsample that many indices. If float, subsample that fraction of indices. random_state: Random number generator.</p> required <code>random_state</code> <code>int | Generator | None</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>list[NDArray[int64]]</code> <p>List of indices to subsample.</p>"},{"location":"reference/tabpfn/regressor/","title":"Regressor","text":""},{"location":"reference/tabpfn/regressor/#tabpfn.regressor","title":"regressor","text":"<p>TabPFNRegressor class.</p> <p>Example</p> <pre><code>import sklearn.datasets\nfrom tabpfn import TabPFNRegressor\n\nmodel = TabPFNRegressor()\nX, y = sklearn.datasets.make_regression(n_samples=50, n_features=10)\n\nmodel.fit(X, y)\npredictions = model.predict(X)\n</code></pre>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.FullOutputDict","title":"FullOutputDict","text":"<p>             Bases: <code>MainOutputDict</code></p> <p>Dictionary containing all outputs from the TabPFN regressor.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.MainOutputDict","title":"MainOutputDict","text":"<p>             Bases: <code>TypedDict</code></p> <p>Dictionary containing the main output types from the TabPFN regressor.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor","title":"TabPFNRegressor","text":"<p>             Bases: <code>RegressorMixin</code>, <code>BaseEstimator</code></p> <p>TabPFNRegressor class.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.bardist_","title":"bardist_  <code>instance-attribute</code>","text":"<pre><code>bardist_: FullSupportBarDistribution\n</code></pre> <p>The bar distribution of the target variable, used by the model.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.config_","title":"config_  <code>instance-attribute</code>","text":"<pre><code>config_: InferenceConfig\n</code></pre> <p>The configuration of the loaded model to be used for inference.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.device_","title":"device_  <code>instance-attribute</code>","text":"<pre><code>device_: device\n</code></pre> <p>The device determined to be used.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.executor_","title":"executor_  <code>instance-attribute</code>","text":"<pre><code>executor_: InferenceEngine\n</code></pre> <p>The inference engine used to make predictions.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.feature_names_in_","title":"feature_names_in_  <code>instance-attribute</code>","text":"<pre><code>feature_names_in_: NDArray[Any]\n</code></pre> <p>The feature names of the input data.</p> <p>May not be set if the input data does not have feature names, such as with a numpy array.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.forced_inference_dtype_","title":"forced_inference_dtype_  <code>instance-attribute</code>","text":"<pre><code>forced_inference_dtype_: _dtype | None\n</code></pre> <p>The forced inference dtype for the model based on <code>inference_precision</code>.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.inferred_categorical_indices_","title":"inferred_categorical_indices_  <code>instance-attribute</code>","text":"<pre><code>inferred_categorical_indices_: list[int]\n</code></pre> <p>The indices of the columns that were inferred to be categorical, as a product of any features deemed categorical by the user and what would work best for the model.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.interface_config_","title":"interface_config_  <code>instance-attribute</code>","text":"<pre><code>interface_config_: ModelInterfaceConfig\n</code></pre> <p>Additional configuration of the interface for expert users.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.n_features_in_","title":"n_features_in_  <code>instance-attribute</code>","text":"<pre><code>n_features_in_: int\n</code></pre> <p>The number of features in the input data used during <code>fit()</code>.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.n_outputs_","title":"n_outputs_  <code>instance-attribute</code>","text":"<pre><code>n_outputs_: Literal[1]\n</code></pre> <p>The number of outputs the model supports. Only 1 for now</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.preprocessor_","title":"preprocessor_  <code>instance-attribute</code>","text":"<pre><code>preprocessor_: ColumnTransformer\n</code></pre> <p>The column transformer used to preprocess the input data to be numeric.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.renormalized_criterion_","title":"renormalized_criterion_  <code>instance-attribute</code>","text":"<pre><code>renormalized_criterion_: FullSupportBarDistribution\n</code></pre> <p>The normalized bar distribution used for computing the predictions.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.use_autocast_","title":"use_autocast_  <code>instance-attribute</code>","text":"<pre><code>use_autocast_: bool\n</code></pre> <p>Whether torch's autocast should be used.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.y_train_mean_","title":"y_train_mean_  <code>instance-attribute</code>","text":"<pre><code>y_train_mean_: float\n</code></pre> <p>The mean of the target variable during training.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.y_train_std","title":"y_train_std  <code>instance-attribute</code>","text":"<pre><code>y_train_std: float\n</code></pre> <p>The standard deviation of the target variable during training.</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.fit","title":"fit","text":"<pre><code>fit(X: XType, y: YType) -&gt; Self\n</code></pre> <p>Fit the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input data.</p> required <code>y</code> <code>YType</code> <p>The target variable.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>self</p>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    X: XType, data_source: Literal[\"train\", \"test\"] = \"test\"\n) -&gt; ndarray\n</code></pre> <p>Get the embeddings for the input data <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input data.</p> required <code>data_source</code> <code>str</code> <p>Extract either the train or test embeddings</p> <code>'test'</code>"},{"location":"reference/tabpfn/regressor/#tabpfn.regressor.TabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(\n    X: XType,\n    *,\n    output_type: Literal[\n        \"mean\",\n        \"median\",\n        \"mode\",\n        \"quantiles\",\n        \"full\",\n        \"main\",\n    ] = \"mean\",\n    quantiles: list[float] | None = None\n) -&gt; (\n    ndarray\n    | list[ndarray]\n    | MainOutputDict\n    | FullOutputDict\n)\n</code></pre> <p>Predict the target variable.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>XType</code> <p>The input data.</p> required <code>output_type</code> <code>Literal['mean', 'median', 'mode', 'quantiles', 'full', 'main']</code> <p>Determines the type of output to return.</p> <ul> <li>If <code>\"mean\"</code>, we return the mean over the predicted distribution.</li> <li>If <code>\"median\"</code>, we return the median over the predicted distribution.</li> <li>If <code>\"mode\"</code>, we return the mode over the predicted distribution.</li> <li>If <code>\"quantiles\"</code>, we return the quantiles of the predicted     distribution. The parameter <code>output_quantiles</code> determines which     quantiles are returned.</li> <li>If <code>\"main\"</code>, we return the all output types above in a dict.</li> <li>If <code>\"full\"</code>, we return the full output of the model, including the   logits and the criterion, and all the output types from \"main\".</li> </ul> <code>'mean'</code> <code>quantiles</code> <code>list[float] | None</code> <p>The quantiles to return if <code>output=\"quantiles\"</code>.</p> <p>By default, the <code>[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]</code> quantiles are returned. The predictions per quantile match the input order.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray | list[ndarray] | MainOutputDict | FullOutputDict</code> <p>The predicted target variable or a list of predictions per quantile.</p>"},{"location":"reference/tabpfn/utils/","title":"Utils","text":""},{"location":"reference/tabpfn/utils/#tabpfn.utils","title":"utils","text":"<p>A collection of random utilities for the TabPFN models.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.get_total_memory_windows","title":"get_total_memory_windows","text":"<pre><code>get_total_memory_windows() -&gt; float\n</code></pre> <p>Get the total memory of the system for windows OS, using windows API.</p> <p>Returns:</p> Type Description <code>float</code> <p>The total memory of the system in GB.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.infer_categorical_features","title":"infer_categorical_features","text":"<pre><code>infer_categorical_features(\n    X: ndarray,\n    *,\n    provided: Sequence[int] | None,\n    min_samples_for_inference: int,\n    max_unique_for_category: int,\n    min_unique_for_numerical: int\n) -&gt; list[int]\n</code></pre> <p>Infer the categorical features from the given data.</p> <p>Note</p> <p>This function may infer particular columns to not be categorical as defined by what suits the model predictions and it's pre-training.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The data to infer the categorical features from.</p> required <code>provided</code> <code>Sequence[int] | None</code> <p>Any user provided indices of what is considered categorical.</p> required <code>min_samples_for_inference</code> <code>int</code> <p>The minimum number of samples required for automatic inference of features which were not provided as categorical.</p> required <code>max_unique_for_category</code> <code>int</code> <p>The maximum number of unique values for a feature to be considered categorical.</p> required <code>min_unique_for_numerical</code> <code>int</code> <p>The minimum number of unique values for a feature to be considered numerical.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The indices of inferred categorical features.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.infer_device_and_type","title":"infer_device_and_type","text":"<pre><code>infer_device_and_type(\n    device: str | device | None,\n) -&gt; device\n</code></pre> <p>Infer the device and data type from the given device string.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str | device | None</code> <p>The device to infer the type from.</p> required <p>Returns:</p> Type Description <code>device</code> <p>The inferred device</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.infer_fp16_inference_mode","title":"infer_fp16_inference_mode","text":"<pre><code>infer_fp16_inference_mode(\n    device: device, *, enable: bool | None\n) -&gt; bool\n</code></pre> <p>Infer whether fp16 inference should be enabled.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>The device to validate against.</p> required <code>enable</code> <code>bool | None</code> <p>Whether it should be enabled, <code>True</code> or <code>False</code>, otherwise if <code>None</code>, detect if it's possible and use it if so.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether to use fp16 inference or not.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If fp16 inference was enabled and device type does not support it.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.infer_random_state","title":"infer_random_state","text":"<pre><code>infer_random_state(\n    random_state: int | RandomState | Generator | None,\n) -&gt; tuple[int, Generator]\n</code></pre> <p>Infer the random state from the given input.</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>int | RandomState | Generator | None</code> <p>The random state to infer.</p> required <p>Returns:</p> Type Description <code>tuple[int, Generator]</code> <p>A static integer seed and a random number generator.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.is_autocast_available","title":"is_autocast_available","text":"<pre><code>is_autocast_available(device_type: str) -&gt; bool\n</code></pre> <p>Infer whether autocast is available for the given device type.</p> <p>Parameters:</p> Name Type Description Default <code>device_type</code> <code>str</code> <p>The device type to check for autocast availability.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether autocast is available for the given device type.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.translate_probs_across_borders","title":"translate_probs_across_borders","text":"<pre><code>translate_probs_across_borders(\n    logits: Tensor, *, frm: Tensor, to: Tensor\n) -&gt; Tensor\n</code></pre> <p>Translate the probabilities across the borders.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The logits defining the distribution to translate.</p> required <code>frm</code> <code>Tensor</code> <p>The borders to translate from.</p> required <code>to</code> <code>Tensor</code> <p>The borders to translate to.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The translated probabilities.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.update_encoder_outlier_params","title":"update_encoder_outlier_params","text":"<pre><code>update_encoder_outlier_params(\n    model: Module,\n    remove_outliers_std: float | None,\n    seed: int | None,\n    *,\n    inplace: Literal[True]\n) -&gt; None\n</code></pre> <p>Update the encoder to handle outliers in the model.</p> <p>Warning</p> <p>This only happens inplace.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to update.</p> required <code>remove_outliers_std</code> <code>float | None</code> <p>The standard deviation to remove outliers.</p> required <code>seed</code> <code>int | None</code> <p>The seed to use, if any.</p> required <code>inplace</code> <code>Literal[True]</code> <p>Whether to do the operation inplace.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>inplace</code> is not <code>True</code>.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.validate_X_predict","title":"validate_X_predict","text":"<pre><code>validate_X_predict(\n    X: XType, estimator: TabPFNRegressor | TabPFNClassifier\n) -&gt; ndarray\n</code></pre> <p>Validate the input data for prediction.</p>"},{"location":"reference/tabpfn/utils/#tabpfn.utils.validate_Xy_fit","title":"validate_Xy_fit","text":"<pre><code>validate_Xy_fit(\n    X: XType,\n    y: YType,\n    estimator: TabPFNRegressor | TabPFNClassifier,\n    *,\n    max_num_features: int,\n    max_num_samples: int,\n    ensure_y_numeric: bool = False,\n    ignore_pretraining_limits: bool = False\n) -&gt; tuple[ndarray, ndarray, NDArray[Any] | None, int]\n</code></pre> <p>Validate the input data for fitting.</p>"},{"location":"reference/tabpfn/model/bar_distribution/","title":"Bar distribution","text":""},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution","title":"bar_distribution","text":""},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution","title":"BarDistribution","text":"<p>             Bases: <code>Module</code></p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.average_bar_distributions_into_this","title":"average_bar_distributions_into_this","text":"<pre><code>average_bar_distributions_into_this(\n    list_of_bar_distributions: Sequence[BarDistribution],\n    list_of_logits: Sequence[Tensor],\n    *,\n    average_logits: bool = False\n) -&gt; Tensor\n</code></pre> <p>:param list_of_bar_distributions: :param list_of_logits: :param average_logits: :return:</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.cdf","title":"cdf","text":"<pre><code>cdf(logits: Tensor, ys: Tensor) -&gt; Tensor\n</code></pre> <p>Calculates the cdf of the distribution described by the logits. The cdf is scaled by the width of the bars.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>tensor of shape (batch_size, ..., num_bars) with the logits describing the distribution</p> required <code>ys</code> <code>Tensor</code> <p>tensor of shape (batch_size, ..., n_ys to eval) or (n_ys to eval) with the targets.</p> required"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.cdf_temporary","title":"cdf_temporary","text":"<pre><code>cdf_temporary(logits: Tensor) -&gt; Tensor\n</code></pre> <p>Cumulative distribution function.</p> <p>TODO: this already exists here, make sure to merge, at the moment still used.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.get_probs_for_different_borders","title":"get_probs_for_different_borders","text":"<pre><code>get_probs_for_different_borders(\n    logits: Tensor, new_borders: Tensor\n) -&gt; Tensor\n</code></pre> <p>The logits describe the density of the distribution over the current self.borders.</p> <p>This function returns the logits if the self.borders were changed to new_borders. This is useful to average the logits of different models.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.icdf","title":"icdf","text":"<pre><code>icdf(logits: Tensor, left_prob: float) -&gt; Tensor\n</code></pre> <p>Implementation of the quantile function :param logits: Tensor of any shape, with the last dimension being logits :param left_prob: float: The probability mass to the left of the result. :return: Position with <code>left_prob</code> probability weight to the left.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.mean_of_square","title":"mean_of_square","text":"<pre><code>mean_of_square(logits: Tensor) -&gt; Tensor\n</code></pre> <p>Computes E[x^2].</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Output of the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>mean of square</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.pi","title":"pi","text":"<pre><code>pi(\n    logits: Tensor,\n    best_f: float | Tensor,\n    *,\n    maximize: bool = True\n) -&gt; Tensor\n</code></pre> <p>Acquisition Function: Probability of Improvement.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>as returned by Transformer</p> required <code>best_f</code> <code>float | Tensor</code> <p>best evaluation so far (the incumbent)</p> required <code>maximize</code> <code>bool</code> <p>whether to maximize</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>probability of improvement</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.plot","title":"plot","text":"<pre><code>plot(\n    logits: Tensor,\n    ax: Axes | None = None,\n    zoom_to_quantile: float | None = None,\n    **kwargs: Any\n) -&gt; Axes\n</code></pre> <p>Plots the distribution.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.BarDistribution.ucb","title":"ucb","text":"<pre><code>ucb(\n    logits: Tensor,\n    best_f: float,\n    rest_prob: float = 1 - 0.682 / 2,\n    *,\n    maximize: bool = True\n) -&gt; Tensor\n</code></pre> <p>UCB utility. Rest Prob is the amount of utility above (below) the confidence interval that is ignored.</p> <p>Higher rest_prob is equivalent to lower beta in the standard GP-UCB formulation.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Logits, as returned by the Transformer.</p> required <code>rest_prob</code> <code>float</code> <p>The amount of utility above (below) the confidence interval that is ignored.</p> <p>The default is equivalent to using GP-UCB with <code>beta=1</code>. To get the corresponding <code>beta</code>, where <code>beta</code> is from the standard GP definition of UCB <code>ucb_utility = mean + beta * std</code>, you can use this computation:</p> <p><code>beta = math.sqrt(2)*torch.erfinv(torch.tensor(2*(1-rest_prob)-1))</code></p> <code>1 - 0.682 / 2</code> <code>best_f</code> <code>float</code> <p>Unused</p> required <code>maximize</code> <code>bool</code> <p>Whether to maximize.</p> <code>True</code>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution","title":"FullSupportBarDistribution","text":"<p>             Bases: <code>BarDistribution</code></p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.average_bar_distributions_into_this","title":"average_bar_distributions_into_this","text":"<pre><code>average_bar_distributions_into_this(\n    list_of_bar_distributions: Sequence[BarDistribution],\n    list_of_logits: Sequence[Tensor],\n    *,\n    average_logits: bool = False\n) -&gt; Tensor\n</code></pre> <p>:param list_of_bar_distributions: :param list_of_logits: :param average_logits: :return:</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.cdf","title":"cdf","text":"<pre><code>cdf(logits: Tensor, ys: Tensor) -&gt; Tensor\n</code></pre> <p>Calculates the cdf of the distribution described by the logits. The cdf is scaled by the width of the bars.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>tensor of shape (batch_size, ..., num_bars) with the logits describing the distribution</p> required <code>ys</code> <code>Tensor</code> <p>tensor of shape (batch_size, ..., n_ys to eval) or (n_ys to eval) with the targets.</p> required"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.cdf_temporary","title":"cdf_temporary","text":"<pre><code>cdf_temporary(logits: Tensor) -&gt; Tensor\n</code></pre> <p>Cumulative distribution function.</p> <p>TODO: this already exists here, make sure to merge, at the moment still used.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.ei_for_halfnormal","title":"ei_for_halfnormal","text":"<pre><code>ei_for_halfnormal(\n    scale: float,\n    best_f: Tensor | float,\n    *,\n    maximize: bool = True\n) -&gt; Tensor\n</code></pre> <p>EI for a standard normal distribution with mean 0 and variance <code>scale</code> times 2.</p> <p>Which is the same as the half normal EI. Tested this with MC approximation:</p> <pre><code>ei_for_halfnormal = lambda scale, best_f: (torch.distributions.HalfNormal(torch.tensor(scale)).sample((10_000_000,))- best_f ).clamp(min=0.).mean()\nprint([(ei_for_halfnormal(scale,best_f), FullSupportBarDistribution().ei_for_halfnormal(scale,best_f)) for scale in [0.1,1.,10.] for best_f in [.1,10.,4.]])\n</code></pre>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.forward","title":"forward","text":"<pre><code>forward(\n    logits: Tensor,\n    y: Tensor,\n    mean_prediction_logits: Tensor | None = None,\n) -&gt; Tensor\n</code></pre> <p>Returns the negative log density (the loss).</p> <p>y: T x B, logits: T x B x self.num_bars.</p> <p>:param logits: Tensor of shape T x B x self.num_bars :param y: Tensor of shape T x B :param mean_prediction_logits: :return:</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.get_probs_for_different_borders","title":"get_probs_for_different_borders","text":"<pre><code>get_probs_for_different_borders(\n    logits: Tensor, new_borders: Tensor\n) -&gt; Tensor\n</code></pre> <p>The logits describe the density of the distribution over the current self.borders.</p> <p>This function returns the logits if the self.borders were changed to new_borders. This is useful to average the logits of different models.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.icdf","title":"icdf","text":"<pre><code>icdf(logits: Tensor, left_prob: float) -&gt; Tensor\n</code></pre> <p>Implementation of the quantile function :param logits: Tensor of any shape, with the last dimension being logits :param left_prob: float: The probability mass to the left of the result. :return: Position with <code>left_prob</code> probability weight to the left.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.mean_of_square","title":"mean_of_square","text":"<pre><code>mean_of_square(logits: Tensor) -&gt; Tensor\n</code></pre> <p>Computes E[x^2].</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Output of the model.</p> required"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.pdf","title":"pdf","text":"<pre><code>pdf(logits: Tensor, y: Tensor) -&gt; Tensor\n</code></pre> <p>Probability density function at y.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.pi","title":"pi","text":"<pre><code>pi(\n    logits: Tensor,\n    best_f: Tensor | float,\n    *,\n    maximize: bool = True\n) -&gt; Tensor\n</code></pre> <p>Acquisition Function: Probability of Improvement.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>as returned by Transformer (evaluation_points x batch x feature_dim)</p> required <code>best_f</code> <code>Tensor | float</code> <p>best evaluation so far (the incumbent)</p> required <code>maximize</code> <code>bool</code> <p>whether to maximize</p> <code>True</code>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.plot","title":"plot","text":"<pre><code>plot(\n    logits: Tensor,\n    ax: Axes | None = None,\n    zoom_to_quantile: float | None = None,\n    **kwargs: Any\n) -&gt; Axes\n</code></pre> <p>Plots the distribution.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.sample","title":"sample","text":"<pre><code>sample(logits: Tensor, t: float = 1.0) -&gt; Tensor\n</code></pre> <p>Samples values from the distribution.</p> <p>Temperature t.</p>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.FullSupportBarDistribution.ucb","title":"ucb","text":"<pre><code>ucb(\n    logits: Tensor,\n    best_f: float,\n    rest_prob: float = 1 - 0.682 / 2,\n    *,\n    maximize: bool = True\n) -&gt; Tensor\n</code></pre> <p>UCB utility. Rest Prob is the amount of utility above (below) the confidence interval that is ignored.</p> <p>Higher rest_prob is equivalent to lower beta in the standard GP-UCB formulation.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Logits, as returned by the Transformer.</p> required <code>rest_prob</code> <code>float</code> <p>The amount of utility above (below) the confidence interval that is ignored.</p> <p>The default is equivalent to using GP-UCB with <code>beta=1</code>. To get the corresponding <code>beta</code>, where <code>beta</code> is from the standard GP definition of UCB <code>ucb_utility = mean + beta * std</code>, you can use this computation:</p> <p><code>beta = math.sqrt(2)*torch.erfinv(torch.tensor(2*(1-rest_prob)-1))</code></p> <code>1 - 0.682 / 2</code> <code>best_f</code> <code>float</code> <p>Unused</p> required <code>maximize</code> <code>bool</code> <p>Whether to maximize.</p> <code>True</code>"},{"location":"reference/tabpfn/model/bar_distribution/#tabpfn.model.bar_distribution.get_bucket_limits","title":"get_bucket_limits","text":"<pre><code>get_bucket_limits(\n    num_outputs: int,\n    full_range: tuple | None = None,\n    ys: Tensor | None = None,\n    *,\n    verbose: bool = False,\n    widen_bucket_limits_factor: float | None = None\n) -&gt; Tensor\n</code></pre> <p>Decide for a set of bucket limits based on a distritbution of ys.</p> <p>Parameters:</p> Name Type Description Default <code>num_outputs</code> <code>int</code> <p>This is only tested for num_outputs=1, but should work for larger num_outputs as well.</p> required <code>full_range</code> <code>tuple | None</code> <p>If ys is not passed, this is the range of the ys that should be used to estimate the bucket limits.</p> <code>None</code> <code>ys</code> <code>Tensor | None</code> <p>If ys is passed, this is the ys that should be used to estimate the bucket limits. Do not pass full_range in this case.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Unused</p> <code>False</code> <code>widen_bucket_limits_factor</code> <code>float | None</code> <p>If set, the bucket limits are widened by this factor. This allows to have a slightly larger range than the actual data.</p> <code>None</code>"},{"location":"reference/tabpfn/model/config/","title":"Config","text":""},{"location":"reference/tabpfn/model/config/#tabpfn.model.config","title":"config","text":""},{"location":"reference/tabpfn/model/config/#tabpfn.model.config.InferenceConfig","title":"InferenceConfig  <code>dataclass</code>","text":"<p>Configuration for the TabPFN model.</p>"},{"location":"reference/tabpfn/model/config/#tabpfn.model.config.InferenceConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config: dict) -&gt; InferenceConfig\n</code></pre> <p>Create a Config object from a dictionary.</p> <p>This method also does some sanity checking initially.</p>"},{"location":"reference/tabpfn/model/encoders/","title":"Encoders","text":""},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders","title":"encoders","text":""},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.CategoricalInputEncoderPerFeatureEncoderStep","title":"CategoricalInputEncoderPerFeatureEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Expects input of size 1.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.CategoricalInputEncoderPerFeatureEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.FrequencyFeatureEncoderStep","title":"FrequencyFeatureEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to add frequency-based features to the input.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.FrequencyFeatureEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.InputEncoder","title":"InputEncoder","text":"<p>             Bases: <code>Module</code></p> <p>Base class for input encoders.</p> <p>All input encoders should subclass this class and implement the <code>forward</code> method.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.InputEncoder.forward","title":"forward","text":"<pre><code>forward(x: Tensor, single_eval_pos: int) -&gt; Tensor\n</code></pre> <p>Encode the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to encode.</p> required <code>single_eval_pos</code> <code>int</code> <p>The position to use for single evaluation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded tensor.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.InputNormalizationEncoderStep","title":"InputNormalizationEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to normalize the input in different ways.</p> <p>Can be used to normalize the input to a ranking, remove outliers, or normalize the input to have unit variance.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.InputNormalizationEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.InputNormalizationEncoderStep.reset_seed","title":"reset_seed","text":"<pre><code>reset_seed() -&gt; None\n</code></pre> <p>Reset the random seed.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.LinearInputEncoder","title":"LinearInputEncoder","text":"<p>             Bases: <code>Module</code></p> <p>A simple linear input encoder.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.LinearInputEncoder.forward","title":"forward","text":"<pre><code>forward(*x: Tensor, **kwargs: Any) -&gt; tuple[Tensor]\n</code></pre> <p>Apply the linear transformation to the input.</p> <p>Parameters:</p> Name Type Description Default <code>*x</code> <code>Tensor</code> <p>The input tensors to concatenate and transform.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Unused keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>A tuple containing the transformed tensor.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.LinearInputEncoderStep","title":"LinearInputEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>A simple linear input encoder step.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.LinearInputEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.NanHandlingEncoderStep","title":"NanHandlingEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to handle NaN and infinite values in the input.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.NanHandlingEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.RemoveDuplicateFeaturesEncoderStep","title":"RemoveDuplicateFeaturesEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to remove duplicate features.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.RemoveDuplicateFeaturesEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.RemoveEmptyFeaturesEncoderStep","title":"RemoveEmptyFeaturesEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to remove empty (constant) features. Was changed to NOT DO ANYTHING, the removal of empty features now done elsewhere, but the saved model still needs this encoder step. TODO: REMOVE.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.RemoveEmptyFeaturesEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.SeqEncStep","title":"SeqEncStep","text":"<p>             Bases: <code>Module</code></p> <p>Abstract base class for sequential encoder steps.</p> <p>SeqEncStep is a wrapper around a module that defines the expected input keys and the produced output keys. The outputs are assigned to the output keys in the order specified by <code>out_keys</code>.</p> <p>Subclasses should either implement <code>_forward</code> or <code>_fit</code> and <code>_transform</code>. Subclasses that transform <code>x</code> should always use <code>_fit</code> and <code>_transform</code>, creating any state that depends on the train set in <code>_fit</code> and using it in <code>_transform</code>. This allows fitting on data first and doing inference later without refitting. Subclasses that work with <code>y</code> can alternatively use <code>_forward</code> instead.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.SeqEncStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.SequentialEncoder","title":"SequentialEncoder","text":"<p>             Bases: <code>Sequential</code>, <code>InputEncoder</code></p> <p>An encoder that applies a sequence of encoder steps.</p> <p>SequentialEncoder allows building an encoder from a sequence of EncoderSteps. The input is passed through each step in the provided order.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.SequentialEncoder.forward","title":"forward","text":"<pre><code>forward(input: dict, **kwargs: Any) -&gt; Tensor\n</code></pre> <p>Apply the sequence of encoder steps to the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>dict</code> <p>The input state dictionary. If the input is not a dict and the first layer expects one input key, the input tensor is mapped to the key expected by the first layer.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to each encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output of the final encoder step.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.VariableNumFeaturesEncoderStep","title":"VariableNumFeaturesEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to handle variable number of features.</p> <p>Transforms the input to a fixed number of features by appending zeros. Also normalizes the input by the number of used features to keep the variance of the input constant, even when zeros are appended.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.VariableNumFeaturesEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs: Any\n) -&gt; dict\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation. Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.normalize_data","title":"normalize_data","text":"<pre><code>normalize_data(\n    data: Tensor,\n    *,\n    normalize_positions: int = -1,\n    return_scaling: bool = False,\n    clip: bool = True,\n    std_only: bool = False,\n    mean: Tensor | None = None,\n    std: Tensor | None = None\n) -&gt; Tensor | tuple[Tensor, tuple[Tensor, Tensor]]\n</code></pre> <p>Normalize data to mean 0 and std 1.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The data to normalize. (T, B, H)</p> required <code>normalize_positions</code> <code>int</code> <p>If &gt; 0, only use the first <code>normalize_positions</code> positions for normalization.</p> <code>-1</code> <code>return_scaling</code> <code>bool</code> <p>If True, return the scaling parameters as well (mean, std).</p> <code>False</code> <code>std_only</code> <code>bool</code> <p>If True, only divide by std.</p> <code>False</code> <code>clip</code> <code>bool</code> <p>If True, clip the data to [-100, 100].</p> <code>True</code> <code>mean</code> <code>Tensor | None</code> <p>If given, use this value instead of computing it.</p> <code>None</code> <code>std</code> <code>Tensor | None</code> <p>If given, use this value instead of computing it.</p> <code>None</code>"},{"location":"reference/tabpfn/model/encoders/#tabpfn.model.encoders.select_features","title":"select_features","text":"<pre><code>select_features(x: Tensor, sel: Tensor) -&gt; Tensor\n</code></pre> <p>Select features from the input tensor based on the selection mask, and arrange them contiguously in the last dimension. If batch size is bigger than 1, we pad the features with zeros to make the number of features fixed.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape (sequence_length, batch_size, total_features)</p> required <code>sel</code> <code>Tensor</code> <p>The boolean selection mask indicating which features to keep of shape (batch_size, total_features)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensor with selected features.</p> <code>Tensor</code> <p>The shape is (sequence_length, batch_size, number_of_selected_features) if batch_size is 1.</p> <code>Tensor</code> <p>The shape is (sequence_length, batch_size, total_features) if batch_size is greater than 1.</p>"},{"location":"reference/tabpfn/model/layer/","title":"Layer","text":""},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer","title":"layer","text":""},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer.LayerNorm","title":"LayerNorm","text":"<p>             Bases: <code>LayerNorm</code></p> <p>Custom LayerNorm module that supports saving peak memory factor.</p> <p>This module extends the PyTorch LayerNorm implementation to handle FP16 inputs efficiently and support saving peak memory factor.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments passed to the base LayerNorm class.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to the base LayerNorm class.</p> <code>{}</code>"},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer.LayerNorm.forward","title":"forward","text":"<pre><code>forward(\n    input: Tensor,\n    *,\n    allow_inplace: bool = False,\n    save_peak_mem_factor: int | None = None\n) -&gt; Tensor\n</code></pre> <p>Perform layer normalization on the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>allow_inplace</code> <code>bool</code> <p>Whether to allow in-place operations. Default is False.</p> <code>False</code> <code>save_peak_mem_factor</code> <code>int | None</code> <p>The factor to save peak memory. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The layer normalized tensor.</p>"},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer.PerFeatureEncoderLayer","title":"PerFeatureEncoderLayer","text":"<p>             Bases: <code>Module</code></p> <p>Transformer encoder layer that processes each feature block separately.</p> <p>This layer consists of multi-head attention between features, multi-head attention between items, and feedforward neural networks (MLPs).</p> <p>It supports various configurations and optimization options.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The dimensionality of the input and output embeddings.</p> required <code>nhead</code> <code>int</code> <p>The number of attention heads.</p> required <code>dim_feedforward</code> <code>int | None</code> <p>The dimensionality of the feedforward network. Default is None (2 * d_model).</p> <code>None</code> <code>activation</code> <code>str</code> <p>The activation function to use in the MLPs.</p> <code>'relu'</code> <code>layer_norm_eps</code> <code>float</code> <p>The epsilon value for layer normalization.</p> <code>1e-05</code> <code>pre_norm</code> <code>bool</code> <p>Whether to apply layer normalization before or after the attention and MLPs.</p> <code>False</code> <code>device</code> <code>device | None</code> <p>The device to use for the layer parameters.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>The data type to use for the layer parameters.</p> <code>None</code> <code>recompute_attn</code> <code>bool</code> <p>Whether to recompute attention during backpropagation.</p> <code>False</code> <code>second_mlp</code> <code>bool</code> <p>Whether to include a second MLP in the layer.</p> <code>False</code> <code>layer_norm_with_elementwise_affine</code> <code>bool</code> <p>Whether to use elementwise affine parameters in layer normalization.</p> <code>False</code> <code>zero_init</code> <code>bool</code> <p>Whether to initialize the output of the MLPs to zero.</p> <code>False</code> <code>save_peak_mem_factor</code> <code>int | None</code> <p>The factor to save peak memory, only effective with post-norm.</p> <code>None</code> <code>attention_between_features</code> <code>bool</code> <p>Whether to apply attention between feature blocks.</p> <code>True</code> <code>multiquery_item_attention</code> <code>bool</code> <p>Whether to use multiquery attention for items.</p> <code>False</code> <code>multiquery_item_attention_for_test_set</code> <code>bool</code> <p>Whether to use multiquery attention for the test set.</p> <code>False</code> <code>attention_init_gain</code> <code>float</code> <p>The gain value for initializing attention parameters.</p> <code>1.0</code> <code>d_k</code> <code>int | None</code> <p>The dimensionality of the query and key vectors. Default is (d_model // nhead).</p> <code>None</code> <code>d_v</code> <code>int | None</code> <p>The dimensionality of the value vectors. Default is (d_model // nhead).</p> <code>None</code> <code>precomputed_kv</code> <code>None | Tensor | tuple[Tensor, Tensor]</code> <p>Precomputed key-value pairs for attention.</p> <code>None</code>"},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer.PerFeatureEncoderLayer.empty_trainset_representation_cache","title":"empty_trainset_representation_cache","text":"<pre><code>empty_trainset_representation_cache() -&gt; None\n</code></pre> <p>Empty the trainset representation cache.</p>"},{"location":"reference/tabpfn/model/layer/#tabpfn.model.layer.PerFeatureEncoderLayer.forward","title":"forward","text":"<pre><code>forward(\n    state: Tensor,\n    single_eval_pos: int | None = None,\n    *,\n    cache_trainset_representation: bool = False,\n    att_src: Tensor | None = None\n) -&gt; Tensor\n</code></pre> <p>Pass the input through the encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Tensor</code> <p>The transformer state passed as input to the layer of shape (batch_size, num_items, num_feature_blocks, d_model).</p> required <code>single_eval_pos</code> <code>int | None</code> <p>The position from which on everything is treated as test set.</p> <code>None</code> <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the trainset representation. If single_eval_pos is set (&gt; 0 and not None), create a cache of the trainset KV. This may require a lot of memory. Otherwise, use cached KV representations for inference.</p> <code>False</code> <code>att_src</code> <code>Tensor | None</code> <p>The tensor to attend to from the final layer of the encoder. It has a shape of (batch_size, num_train_items, num_feature_blocks, d_model). This does not work with multiquery_item_attention_for_test_set and cache_trainset_representation at this point.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The transformer state passed through the encoder layer.</p>"},{"location":"reference/tabpfn/model/loading/","title":"Loading","text":""},{"location":"reference/tabpfn/model/loading/#tabpfn.model.loading","title":"loading","text":""},{"location":"reference/tabpfn/model/loading/#tabpfn.model.loading.download_all_models","title":"download_all_models","text":"<pre><code>download_all_models(to: Path) -&gt; None\n</code></pre> <p>Download all v2 classifier and regressor models into a local directory.</p>"},{"location":"reference/tabpfn/model/loading/#tabpfn.model.loading.download_model","title":"download_model","text":"<pre><code>download_model(\n    to: Path,\n    *,\n    version: Literal[\"v2\"],\n    which: Literal[\"classifier\", \"regressor\"],\n    model_name: str | None = None\n) -&gt; Literal[\"ok\"] | list[Exception]\n</code></pre> <p>Download a TabPFN model, trying all available sources.</p> <p>Parameters:</p> Name Type Description Default <code>to</code> <code>Path</code> <p>The directory to download the model to.</p> required <code>version</code> <code>Literal['v2']</code> <p>The version of the model to download.</p> required <code>which</code> <code>Literal['classifier', 'regressor']</code> <p>The type of model to download.</p> required <code>model_name</code> <code>str | None</code> <p>Optional specific model name to download.</p> <code>None</code> <p>Returns:</p> Type Description <code>Literal['ok'] | list[Exception]</code> <p>\"ok\" if the model was downloaded successfully, otherwise a list of</p> <code>Literal['ok'] | list[Exception]</code> <p>exceptions that occurred that can be handled as desired.</p>"},{"location":"reference/tabpfn/model/loading/#tabpfn.model.loading.load_model","title":"load_model","text":"<pre><code>load_model(*, path: Path, model_seed: int) -&gt; tuple[\n    PerFeatureTransformer,\n    BCEWithLogitsLoss\n    | CrossEntropyLoss\n    | FullSupportBarDistribution,\n    InferenceConfig,\n]\n</code></pre> <p>Loads a model from a given path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the checkpoint</p> required <code>model_seed</code> <code>int</code> <p>The seed to use for the model</p> required"},{"location":"reference/tabpfn/model/loading/#tabpfn.model.loading.load_model_criterion_config","title":"load_model_criterion_config","text":"<pre><code>load_model_criterion_config(\n    model_path: None | str | Path,\n    *,\n    check_bar_distribution_criterion: bool,\n    cache_trainset_representation: bool,\n    which: Literal[\"regressor\", \"classifier\"],\n    version: Literal[\"v2\"] = \"v2\",\n    download: bool,\n    model_seed: int\n) -&gt; tuple[\n    PerFeatureTransformer,\n    BCEWithLogitsLoss\n    | CrossEntropyLoss\n    | FullSupportBarDistribution,\n    InferenceConfig,\n]\n</code></pre> <p>Load the model, criterion, and config from the given path.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>None | str | Path</code> <p>The path to the model.</p> required <code>check_bar_distribution_criterion</code> <code>bool</code> <p>Whether to check if the criterion is a FullSupportBarDistribution, which is the expected criterion for models trained for regression.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether the model should know to cache the trainset representation.</p> required <code>which</code> <code>Literal['regressor', 'classifier']</code> <p>Whether the model is a regressor or classifier.</p> required <code>version</code> <code>Literal['v2']</code> <p>The version of the model.</p> <code>'v2'</code> <code>download</code> <code>bool</code> <p>Whether to download the model if it doesn't exist.</p> required <code>model_seed</code> <code>int</code> <p>The seed of the model.</p> required <p>Returns:</p> Type Description <code>tuple[PerFeatureTransformer, BCEWithLogitsLoss | CrossEntropyLoss | FullSupportBarDistribution, InferenceConfig]</code> <p>The model, criterion, and config.</p>"},{"location":"reference/tabpfn/model/memory/","title":"Memory","text":""},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory","title":"memory","text":""},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator","title":"MemoryUsageEstimator","text":""},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.convert_bytes_to_unit","title":"convert_bytes_to_unit  <code>classmethod</code>","text":"<pre><code>convert_bytes_to_unit(\n    value: float, unit: Literal[\"b\", \"mb\", \"gb\"]\n) -&gt; float\n</code></pre> <p>Convenience method to convert bytes to a different unit.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>The number of bytes.</p> required <code>unit</code> <code>Literal['b', 'mb', 'gb']</code> <p>The unit to convert to.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The number of bytes in the new unit.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.convert_units","title":"convert_units  <code>classmethod</code>","text":"<pre><code>convert_units(\n    value: float,\n    from_unit: Literal[\"b\", \"mb\", \"gb\"],\n    to_unit: Literal[\"b\", \"mb\", \"gb\"],\n) -&gt; float\n</code></pre> <p>Convert a value from one unit to another.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.estimate_memory_of_one_batch","title":"estimate_memory_of_one_batch  <code>classmethod</code>","text":"<pre><code>estimate_memory_of_one_batch(\n    X: Tensor,\n    model: Module,\n    *,\n    cache_kv: bool,\n    dtype_byte_size: int,\n    unit: Literal[\"b\", \"mb\", \"gb\"] = \"gb\",\n    n_train_samples: int | None = None\n) -&gt; float\n</code></pre> <p>Estimate the memory usage of a single batch.</p> <p>The calculation is done based on the assumption that save_peak_mem_factor is not used (since this estimation is used to determine whether to use it).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>The input tensor.</p> required <code>model</code> <code>Module</code> <p>The model to estimate the memory usage of.</p> required <code>cache_kv</code> <code>bool</code> <p>Whether key and value tensors are cached.</p> required <code>dtype_byte_size</code> <code>int</code> <p>The size of the data type in bytes.</p> required <code>unit</code> <code>Literal['b', 'mb', 'gb']</code> <p>The unit to convert the memory usage to.</p> <code>'gb'</code> <code>n_train_samples</code> <code>int | None</code> <p>The number of training samples (only for cache_kv mode)</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The estimated memory usage of a single batch.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.estimate_memory_remainder_after_batch","title":"estimate_memory_remainder_after_batch  <code>classmethod</code>","text":"<pre><code>estimate_memory_remainder_after_batch(\n    X: Tensor,\n    model: Module,\n    *,\n    cache_kv: bool,\n    device: device,\n    dtype_byte_size: int,\n    safety_factor: float,\n    n_train_samples: int | None = None,\n    max_free_mem: float | int | None = None\n) -&gt; float\n</code></pre> <p>Whether to save peak memory or not.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>The input tensor.</p> required <code>model</code> <code>Module</code> <p>The model to estimate the memory usage of.</p> required <code>cache_kv</code> <code>bool</code> <p>Whether key and value tensors are cached.</p> required <code>device</code> <code>device</code> <p>The device to use.</p> required <code>dtype_byte_size</code> <code>int</code> <p>The size of the data type in bytes.</p> required <code>safety_factor</code> <code>float</code> <p>The safety factor to apply.</p> required <code>n_train_samples</code> <code>int | None</code> <p>The number of training samples (only for cache_kv mode)</p> <code>None</code> <code>max_free_mem</code> <code>float | int | None</code> <p>The amount of free memory available.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The amount of free memory available after a batch is computed.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.get_max_free_memory","title":"get_max_free_memory  <code>classmethod</code>","text":"<pre><code>get_max_free_memory(\n    device: device,\n    *,\n    unit: Literal[\"b\", \"mb\", \"gb\"] = \"gb\",\n    default_gb_cpu_if_failed_to_calculate: float\n) -&gt; float\n</code></pre> <p>How much memory to use at most in GB, the memory usage will be calculated based on an estimation of the systems free memory.</p> <p>For CUDA will use the free memory of the GPU. For CPU will default to 32 GB.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.get_max_free_memory--returns","title":"Returns:","text":"<p>The maximum memory usage in GB.</p>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.MemoryUsageEstimator.reset_peak_memory_if_required","title":"reset_peak_memory_if_required  <code>classmethod</code>","text":"<pre><code>reset_peak_memory_if_required(\n    save_peak_mem: bool | Literal[\"auto\"] | float | int,\n    model: Module,\n    X: Tensor,\n    *,\n    cache_kv: bool,\n    device: device,\n    dtype_byte_size: int,\n    safety_factor: float = 5.0,\n    n_train_samples: int | None = None\n) -&gt; None\n</code></pre> <p>Reset the peak memory if required.</p> <p>Parameters:</p> Name Type Description Default <code>save_peak_mem</code> <code>bool | 'auto' | float | int</code> <p>If bool, specifies whether to save peak memory or not. If \"auto\", the amount of free memory is estimated and the option is enabled or disabled based on the estimated usage. If float or int, it is considered as the amount of memory available (in GB) explicitly specified by the user. In this case, this value is used to estimate whether or not to save peak memory.</p> required <code>model</code> <code>Module</code> <p>The model to reset the peak memory of.</p> required <code>X</code> <code>Tensor</code> <p>The input tensor.</p> required <code>cache_kv</code> <code>bool</code> <p>Whether key and value tensors are cached.</p> required <code>device</code> <code>device</code> <p>The device to use.</p> required <code>dtype_byte_size</code> <code>int</code> <p>The size of the data type in bytes.</p> required <code>safety_factor</code> <code>float</code> <p>The safety factor to apply.</p> <code>5.0</code> <code>n_train_samples</code> <code>int</code> <p>The number of training samples (to be used only for cache_kv mode)</p> <code>None</code>"},{"location":"reference/tabpfn/model/memory/#tabpfn.model.memory.support_save_peak_mem_factor","title":"support_save_peak_mem_factor","text":"<pre><code>support_save_peak_mem_factor(\n    method: MethodType,\n) -&gt; Callable\n</code></pre> <p>Can be applied to a method acting on a tensor 'x' whose first dimension is a flat batch dimension (i.e. the operation is trivially parallel over the first dimension).</p> <p>For additional tensor arguments, it is assumed that the first dimension is again the batch dimension, and that non-tensor arguments can be passed as-is to splits when parallelizing over the batch dimension.</p> <p>The decorator adds options 'add_input' to add the principal input 'x' to the result of the method and 'allow_inplace'. By setting 'allow_inplace', the caller indicates that 'x' is not used after the call and its buffer can be reused for the output.</p> <p>Setting 'allow_inplace' does not ensure that the operation will be inplace, and the return value should be used for clarity and simplicity.</p> <p>Moreover, it adds an optional int parameter 'save_peak_mem_factor' that is only supported in combination with 'allow_inplace' during inference and subdivides the operation into the specified number of chunks to reduce peak memory consumption.</p>"},{"location":"reference/tabpfn/model/mlp/","title":"Mlp","text":""},{"location":"reference/tabpfn/model/mlp/#tabpfn.model.mlp","title":"mlp","text":""},{"location":"reference/tabpfn/model/mlp/#tabpfn.model.mlp.Activation","title":"Activation","text":"<p>             Bases: <code>Enum</code></p> <p>Enum for activation functions.</p>"},{"location":"reference/tabpfn/model/mlp/#tabpfn.model.mlp.MLP","title":"MLP","text":"<p>             Bases: <code>Module</code></p> <p>Multi-Layer Perceptron (MLP) module.</p> <p>This module consists of two linear layers with an activation function in between. It supports various configurations such as the hidden size, activation function, initializing the output to zero, and recomputing the forward pass during backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The input and output size of the MLP.</p> required <code>hidden_size</code> <code>int</code> <p>The size of the hidden layer.</p> required <code>activation</code> <code>Activation | str</code> <p>The activation function to use. Can be either an Activation enum or a string representing the activation name.</p> required <code>device</code> <code>device | None</code> <p>The device to use for the linear layers.</p> required <code>dtype</code> <code>dtype | None</code> <p>The data type to use for the linear layers.</p> required <code>initialize_output_to_zero</code> <code>bool</code> <p>Whether to initialize the output layer weights to zero. Default is False.</p> <code>False</code> <code>recompute</code> <code>bool</code> <p>Whether to recompute the forward pass during backpropagation. This can save memory but increase computation time. Default is False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>linear1</code> <code>Linear</code> <p>The first linear layer.</p> <code>linear2</code> <code>Linear</code> <p>The second linear layer.</p> <code>activation</code> <code>Activation</code> <p>The activation function to use.</p> Example <p>mlp = MLP(size=128, hidden_size=256, activation='gelu', device='cuda') x = torch.randn(32, 128, device='cuda', dtype=torch.float32) output = mlp(x)</p>"},{"location":"reference/tabpfn/model/mlp/#tabpfn.model.mlp.MLP.forward","title":"forward","text":"<pre><code>forward(\n    x: Tensor,\n    *,\n    add_input: bool = False,\n    allow_inplace: bool = False,\n    save_peak_mem_factor: int | None = None\n) -&gt; Tensor\n</code></pre> <p>Performs the forward pass of the MLP.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>add_input</code> <code>bool</code> <p>Whether to add input to the output. Default is False.</p> <code>False</code> <code>allow_inplace</code> <code>bool</code> <p>Indicates that 'x' is not used after the call and its buffer can be reused for the output. The operation is not guaranteed to be inplace. Default is False.</p> <code>False</code> <code>save_peak_mem_factor</code> <code>int | None</code> <p>If provided, enables a memory-saving technique that reduces peak memory usage during the forward pass. This requires 'add_input' and 'allow_inplace' to be True. See the documentation of the decorator 'support_save_peak_mem_factor' for details. Default is None.</p> <code>None</code>"},{"location":"reference/tabpfn/model/multi_head_attention/","title":"Multi head attention","text":""},{"location":"reference/tabpfn/model/multi_head_attention/#tabpfn.model.multi_head_attention","title":"multi_head_attention","text":""},{"location":"reference/tabpfn/model/multi_head_attention/#tabpfn.model.multi_head_attention.MultiHeadAttention","title":"MultiHeadAttention","text":"<p>             Bases: <code>Module</code></p>"},{"location":"reference/tabpfn/model/multi_head_attention/#tabpfn.model.multi_head_attention.MultiHeadAttention.forward","title":"forward","text":"<pre><code>forward(\n    x: Tensor,\n    x_kv: Tensor | None = None,\n    *,\n    cache_kv: bool = False,\n    add_input: bool = False,\n    allow_inplace: bool = False,\n    save_peak_mem_factor: int | None = None,\n    reuse_first_head_kv: bool = False,\n    only_cache_first_head_kv: bool = False,\n    use_cached_kv: bool = False,\n    use_second_set_of_queries: bool = False\n)\n</code></pre> <p>X is the current hidden and has a shape of [batch, ..., seq_len, input_size]. If keys and values are present in the cache and 'freeze_kv' is not set, they are obtained from there and 'x_kv' has to be None. Else, if 'x_kv' is not None, keys and values are obtained by applying the respective linear transformations to 'x_kv'. Else, keys and values are attained by applying the respective linear transformations to 'x' (self attention).</p>"},{"location":"reference/tabpfn/model/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing","title":"preprocessing","text":""},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.AddFingerprintFeaturesStep","title":"AddFingerprintFeaturesStep","text":"<p>             Bases: <code>FeaturePreprocessingTransformerStep</code></p> <p>Adds a fingerprint feature to the features based on hash of each row.</p> <p>If <code>is_test = True</code>, it keeps the first hash even if there are collisions. If <code>is_test = False</code>, it handles hash collisions by counting up and rehashing until a unique hash is found.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.AddFingerprintFeaturesStep.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fits the preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.AddFingerprintFeaturesStep.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transforms the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.FeaturePreprocessingTransformerStep","title":"FeaturePreprocessingTransformerStep","text":"<p>Base class for feature preprocessing steps.</p> <p>It's main abstraction is really just to provide categorical indices along the pipeline.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.FeaturePreprocessingTransformerStep.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fits the preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.FeaturePreprocessingTransformerStep.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transforms the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.KDITransformerWithNaN","title":"KDITransformerWithNaN","text":"<p>             Bases: <code>KDITransformer</code></p> <p>KDI transformer that can handle NaN values. It performs KDI with NaNs replaced by mean values and then fills the NaN values with NaNs after the transformation.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.RemoveConstantFeaturesStep","title":"RemoveConstantFeaturesStep","text":"<p>             Bases: <code>FeaturePreprocessingTransformerStep</code></p> <p>Remove features that are constant in the training data.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.RemoveConstantFeaturesStep.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fits the preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.RemoveConstantFeaturesStep.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transforms the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ReshapeFeatureDistributionsStep","title":"ReshapeFeatureDistributionsStep","text":"<p>             Bases: <code>FeaturePreprocessingTransformerStep</code></p> <p>Reshape the feature distributions using different transformations.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ReshapeFeatureDistributionsStep.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fits the preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ReshapeFeatureDistributionsStep.get_adaptive_preprocessors","title":"get_adaptive_preprocessors  <code>staticmethod</code>","text":"<pre><code>get_adaptive_preprocessors(\n    num_examples: int = 100, random_state: int | None = None\n) -&gt; dict[str, ColumnTransformer]\n</code></pre> <p>Returns a dictionary of adaptive column transformers that can be used to preprocess the data. Adaptive column transformers are used to preprocess the data based on the column type, they receive a pandas dataframe with column names, that indicate the column type. Column types are not datatypes, but rather a string that indicates how the data should be preprocessed.</p> <p>Parameters:</p> Name Type Description Default <code>num_examples</code> <code>int</code> <p>The number of examples in the dataset.</p> <code>100</code> <code>random_state</code> <code>int | None</code> <p>The random state to use for the transformers.</p> <code>None</code>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ReshapeFeatureDistributionsStep.get_column_types","title":"get_column_types  <code>staticmethod</code>","text":"<pre><code>get_column_types(X: ndarray) -&gt; list[str]\n</code></pre> <p>Returns a list of column types for the given data, that indicate how the data should be preprocessed.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ReshapeFeatureDistributionsStep.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transforms the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.SafePowerTransformer","title":"SafePowerTransformer","text":"<p>             Bases: <code>PowerTransformer</code></p> <p>Power Transformer which reverts features back to their original values if they are transformed to large values or the output column does not have unit variance. This happens e.g. when the input data has a large number of outliers.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.SequentialFeatureTransformer","title":"SequentialFeatureTransformer","text":"<p>             Bases: <code>UserList</code></p> <p>A transformer that applies a sequence of feature preprocessing steps. This is very related to sklearn's Pipeline, but it is designed to work with categorical_features lists that are always passed on.</p> <p>Currently this class is only used once, thus this could also be made less general if needed.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.SequentialFeatureTransformer.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fit all the steps in the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.SequentialFeatureTransformer.fit_transform","title":"fit_transform","text":"<pre><code>fit_transform(\n    X: ndarray, categorical_features: list[int]\n) -&gt; _TransformResult\n</code></pre> <p>Fit and transform the data using the fitted pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical features.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.SequentialFeatureTransformer.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transform the data using the fitted pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ShuffleFeaturesStep","title":"ShuffleFeaturesStep","text":"<p>             Bases: <code>FeaturePreprocessingTransformerStep</code></p> <p>Shuffle the features in the data.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ShuffleFeaturesStep.fit","title":"fit","text":"<pre><code>fit(X: ndarray, categorical_features: list[int]) -&gt; Self\n</code></pre> <p>Fits the preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features)</p> required <code>categorical_features</code> <code>list[int]</code> <p>list of indices of categorical feature.</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.ShuffleFeaturesStep.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; _TransformResult\n</code></pre> <p>Transforms the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2d array of shape (n_samples, n_features).</p> required"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.add_safe_standard_to_safe_power_without_standard","title":"add_safe_standard_to_safe_power_without_standard","text":"<pre><code>add_safe_standard_to_safe_power_without_standard(\n    input_transformer: TransformerMixin,\n) -&gt; Pipeline\n</code></pre> <p>In edge cases PowerTransformer can create inf values and similar. Then, the post standard scale crashes. This fixes this issue.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.make_box_cox_safe","title":"make_box_cox_safe","text":"<pre><code>make_box_cox_safe(\n    input_transformer: TransformerMixin | Pipeline,\n) -&gt; Pipeline\n</code></pre> <p>Make box cox save.</p> <p>The Box-Cox transformation can only be applied to strictly positive data. With first MinMax scaling, we achieve this without loss of function. Additionally, for test data, we also need clipping.</p>"},{"location":"reference/tabpfn/model/preprocessing/#tabpfn.model.preprocessing.skew","title":"skew","text":"<pre><code>skew(x: ndarray) -&gt; float\n</code></pre>"},{"location":"reference/tabpfn/model/transformer/","title":"Transformer","text":""},{"location":"reference/tabpfn/model/transformer/#tabpfn.model.transformer","title":"transformer","text":""},{"location":"reference/tabpfn/model/transformer/#tabpfn.model.transformer.LayerStack","title":"LayerStack","text":"<p>             Bases: <code>Module</code></p> <p>Similar to nn.Sequential, but with support for passing keyword arguments to layers and stacks the same layer multiple times.</p>"},{"location":"reference/tabpfn/model/transformer/#tabpfn.model.transformer.PerFeatureTransformer","title":"PerFeatureTransformer","text":"<p>             Bases: <code>Module</code></p> <p>A Transformer model processes a token per feature and sample.</p> <p>This model extends the standard Transformer architecture to operate on a per-feature basis. It allows for processing each feature separately while still leveraging the power of self-attention.</p> <p>The model consists of an encoder, decoder, and optional components such as a feature positional embedding and a separate decoder for each feature.</p>"},{"location":"reference/tabpfn/model/transformer/#tabpfn.model.transformer.PerFeatureTransformer.forward","title":"forward","text":"<pre><code>forward(*args: Any, **kwargs: Any) -&gt; dict[str, Tensor]\n</code></pre> <p>Performs a forward pass through the model.</p> <p>This method supports multiple calling conventions:</p> <ul> <li><code>model((x,y), **kwargs)</code></li> <li><code>model(train_x, train_y, test_x, **kwargs)</code></li> <li><code>model((style,x,y), **kwargs)</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>train_x</code> <p>torch.Tensor | None The input data for the training set.</p> required <code>train_y</code> <p>torch.Tensor | None The target data for the training set.</p> required <code>test_x</code> <p>torch.Tensor | None The input data for the test set.</p> required <code>x</code> <p>torch.Tensor The input data.</p> required <code>y</code> <p>torch.Tensor | None The target data.</p> required <code>style</code> <p>torch.Tensor | None The style vector.</p> required <code>single_eval_pos</code> <p>int The position to evaluate at.</p> required <code>only_return_standard_out</code> <p>bool Whether to only return the standard output.</p> required <code>data_dags</code> <p>Any The data DAGs for each example.</p> required <code>categorical_inds</code> <p>list[int] The indices of categorical features.</p> required <code>freeze_kv</code> <p>bool Whether to freeze the key and value weights.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>The output of the model, which can be a tensor or a dictionary of tensors.</p>"},{"location":"reference/tabpfn/model/transformer/#tabpfn.model.transformer.PerFeatureTransformer.reset_save_peak_mem_factor","title":"reset_save_peak_mem_factor","text":"<pre><code>reset_save_peak_mem_factor(\n    factor: int | None = None,\n) -&gt; None\n</code></pre> <p>Sets the save_peak_mem_factor for all layers.</p> <p>This factor controls how much memory is saved during the forward pass in inference mode.</p> <p>Setting this factor &gt; 1 will cause the model to save more memory during the forward pass in inference mode.</p> <p>A value of 8 is good for a 4x larger width in the fully-connected layers. and yields a situation were we need around <code>2*num_features*num_items*emsize*2</code> bytes of memory</p> <p>for a forward pass (using mixed precision).</p> <p>WARNING: It should only be used with post-norm.</p> <p>Parameters:</p> Name Type Description Default <code>factor</code> <code>int | None</code> <p>The save_peak_mem_factor to set. Recommended value is 8.</p> <code>None</code>"},{"location":"reference/tabpfn_client/browser_auth/","title":"Browser auth","text":""},{"location":"reference/tabpfn_client/browser_auth/#tabpfn_client.browser_auth","title":"browser_auth","text":""},{"location":"reference/tabpfn_client/browser_auth/#tabpfn_client.browser_auth.BrowserAuthHandler","title":"BrowserAuthHandler","text":""},{"location":"reference/tabpfn_client/browser_auth/#tabpfn_client.browser_auth.BrowserAuthHandler.try_browser_login","title":"try_browser_login","text":"<pre><code>try_browser_login() -&gt; Tuple[bool, Optional[str]]\n</code></pre> <p>Attempts to perform browser-based login Returns (success: bool, token: Optional[str])</p>"},{"location":"reference/tabpfn_client/client/","title":"Client","text":""},{"location":"reference/tabpfn_client/client/#tabpfn_client.client","title":"client","text":""},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager","title":"DatasetUIDCacheManager","text":"<p>Manages a cache of the last 50 uploaded datasets, tracking dataset hashes and their UIDs.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager.add_dataset_uid","title":"add_dataset_uid","text":"<pre><code>add_dataset_uid(hash: str, dataset_uid: str)\n</code></pre> <p>Adds a new dataset to the cache, removing the oldest item if the cache exceeds 50 entries. Assumes the dataset is not already in the cache.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager.delete_uid","title":"delete_uid","text":"<pre><code>delete_uid(dataset_uid: str) -&gt; Optional[str]\n</code></pre> <p>Deletes an entry from the cache based on the dataset UID.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager.get_dataset_uid","title":"get_dataset_uid","text":"<pre><code>get_dataset_uid(*args)\n</code></pre> <p>Generates hash by all received arguments and returns cached dataset uid if in cache, otherwise None.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager.load_cache","title":"load_cache","text":"<pre><code>load_cache()\n</code></pre> <p>Loads the cache from disk if it exists, otherwise initializes an empty cache.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.DatasetUIDCacheManager.save_cache","title":"save_cache","text":"<pre><code>save_cache()\n</code></pre> <p>Saves the current cache to disk.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.GCPOverloaded","title":"GCPOverloaded","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when the Google Cloud Platform service is overloaded or unavailable.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient","title":"ServiceClient","text":"<p>             Bases: <code>Singleton</code></p> <p>Singleton class for handling communication with the server. It encapsulates all the API calls to the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.delete_all_datasets","title":"delete_all_datasets  <code>classmethod</code>","text":"<pre><code>delete_all_datasets() -&gt; [str]\n</code></pre> <p>Delete all datasets uploaded by the user from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.delete_all_datasets--returns","title":"Returns","text":"<p>deleted_dataset_uids : [str]     The list of deleted dataset UIDs.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.delete_dataset","title":"delete_dataset  <code>classmethod</code>","text":"<pre><code>delete_dataset(dataset_uid: str) -&gt; list[str]\n</code></pre> <p>Delete the dataset with the provided UID from the server. Note that deleting a train set with lead to deleting all associated test sets.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.delete_dataset--parameters","title":"Parameters","text":"<p>dataset_uid : str     The UID of the dataset to be deleted.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.delete_dataset--returns","title":"Returns","text":"<p>deleted_dataset_uids : [str]     The list of deleted dataset UIDs.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.download_all_data","title":"download_all_data  <code>classmethod</code>","text":"<pre><code>download_all_data(save_dir: Path) -&gt; Union[Path, None]\n</code></pre> <p>Download all data uploaded by the user from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.download_all_data--returns","title":"Returns","text":"<p>save_path : Path | None     The path to the downloaded file. Return None if download fails.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.fit","title":"fit  <code>classmethod</code>","text":"<pre><code>fit(X, y, config=None) -&gt; str\n</code></pre> <p>Upload a train set to server and return the train set UID if successful.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.fit--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     The training input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs)     The target values. config : dict, optional     Configuration for the fit method. Includes tabpfn_systems and paper_version.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.fit--returns","title":"Returns","text":"<p>train_set_uid : str     The unique ID of the train set in the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.get_api_usage","title":"get_api_usage  <code>classmethod</code>","text":"<pre><code>get_api_usage(access_token: str)\n</code></pre> <p>Retrieve current API usage data of the user from the server. Returns summary: str</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.get_data_summary","title":"get_data_summary  <code>classmethod</code>","text":"<pre><code>get_data_summary() -&gt; dict\n</code></pre> <p>Get the data summary of the user from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.get_data_summary--returns","title":"Returns","text":"<p>data_summary : dict     The data summary returned from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.get_password_policy","title":"get_password_policy  <code>classmethod</code>","text":"<pre><code>get_password_policy() -&gt; dict\n</code></pre> <p>Get the password policy from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.get_password_policy--returns","title":"Returns","text":"<p>password_policy : {}     The password policy returned from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.is_auth_token_outdated","title":"is_auth_token_outdated  <code>classmethod</code>","text":"<pre><code>is_auth_token_outdated(access_token) -&gt; Union[bool, None]\n</code></pre> <p>Check if the provided access token is valid and return True if successful.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.login","title":"login  <code>classmethod</code>","text":"<pre><code>login(email: str, password: str) -&gt; tuple[str, str]\n</code></pre> <p>Login with the provided credentials and return the access token if successful.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.login--parameters","title":"Parameters","text":"<p>email : str password : str</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.login--returns","title":"Returns","text":"<p>access_token : str | None     The access token returned from the server. Return None if login fails. message : str     The message returned from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.predict","title":"predict  <code>classmethod</code>","text":"<pre><code>predict(\n    train_set_uid: str,\n    x_test,\n    task: Literal[\"classification\", \"regression\"],\n    predict_params: Union[dict, None] = None,\n    tabpfn_config: Union[dict, None] = None,\n    X_train=None,\n    y_train=None,\n) -&gt; dict[str, ndarray]\n</code></pre> <p>Predict the class labels for the provided data (test set).</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.predict--parameters","title":"Parameters","text":"<p>train_set_uid : str     The unique ID of the train set in the server. x_test : array-like of shape (n_samples, n_features)     The test input.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.predict--returns","title":"Returns","text":"<p>y_pred : array-like of shape (n_samples,)     The predicted class labels.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(\n    email: str,\n    password: str,\n    password_confirm: str,\n    validation_link: str,\n    additional_info: dict,\n)\n</code></pre> <p>Register a new user with the provided credentials.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.register--parameters","title":"Parameters","text":"<p>email : str password : str password_confirm : str validation_link: str additional_info : dict</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.register--returns","title":"Returns","text":"<p>is_created : bool     True if the user is created successfully. message : str     The message returned from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.retrieve_greeting_messages","title":"retrieve_greeting_messages  <code>classmethod</code>","text":"<pre><code>retrieve_greeting_messages() -&gt; list[str]\n</code></pre> <p>Retrieve greeting messages that are new for the user.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.send_reset_password_email","title":"send_reset_password_email  <code>classmethod</code>","text":"<pre><code>send_reset_password_email(email: str) -&gt; tuple[bool, str]\n</code></pre> <p>Let the server send an email for resetting the password.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.send_verification_email","title":"send_verification_email  <code>classmethod</code>","text":"<pre><code>send_verification_email(\n    access_token: str,\n) -&gt; tuple[bool, str]\n</code></pre> <p>Let the server send an email for verifying the email.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.try_browser_login","title":"try_browser_login  <code>classmethod</code>","text":"<pre><code>try_browser_login() -&gt; tuple[bool, str]\n</code></pre> <p>Attempts browser-based login flow Returns (success: bool, message: str)</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.try_connection","title":"try_connection  <code>classmethod</code>","text":"<pre><code>try_connection() -&gt; bool\n</code></pre> <p>Check if server is reachable and accepts the connection.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.validate_email","title":"validate_email  <code>classmethod</code>","text":"<pre><code>validate_email(email: str) -&gt; tuple[bool, str]\n</code></pre> <p>Send entered email to server that checks if it is valid and not already in use.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.validate_email--parameters","title":"Parameters","text":"<p>email : str</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.validate_email--returns","title":"Returns","text":"<p>is_valid : bool     True if the email is valid. message : str     The message returned from the server.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.verify_email","title":"verify_email  <code>classmethod</code>","text":"<pre><code>verify_email(\n    token: str, access_token: str\n) -&gt; tuple[bool, str]\n</code></pre> <p>Verify the email with the provided token.</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.verify_email--parameters","title":"Parameters","text":"<p>token : str access_token : str</p>"},{"location":"reference/tabpfn_client/client/#tabpfn_client.client.ServiceClient.verify_email--returns","title":"Returns","text":"<p>is_verified : bool     True if the email is verified successfully. message : str     The message returned from the server.</p>"},{"location":"reference/tabpfn_client/config/","title":"Config","text":""},{"location":"reference/tabpfn_client/config/#tabpfn_client.config","title":"config","text":""},{"location":"reference/tabpfn_client/config/#tabpfn_client.config.Config","title":"Config","text":""},{"location":"reference/tabpfn_client/constants/","title":"Constants","text":""},{"location":"reference/tabpfn_client/constants/#tabpfn_client.constants","title":"constants","text":""},{"location":"reference/tabpfn_client/estimator/","title":"Estimator","text":""},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator","title":"estimator","text":""},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNClassifier","title":"TabPFNClassifier","text":"<p>             Bases: <code>ClassifierMixin</code>, <code>BaseEstimator</code>, <code>TabPFNModelSelection</code></p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict class labels for samples in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The input samples.</p> required <p>Returns:</p> Type Description <p>The predicted class labels.</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict class probabilities for X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The input samples.</p> required <p>Returns:</p> Type Description <p>The class probabilities of the input samples.</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNModelSelection","title":"TabPFNModelSelection","text":"<p>Base class for TabPFN model selection and path handling.</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNRegressor","title":"TabPFNRegressor","text":"<p>             Bases: <code>RegressorMixin</code>, <code>BaseEstimator</code>, <code>TabPFNModelSelection</code></p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(\n    X: ndarray,\n    output_type: Literal[\n        \"mean\",\n        \"median\",\n        \"mode\",\n        \"quantiles\",\n        \"full\",\n        \"main\",\n    ] = \"mean\",\n    quantiles: Optional[list[float]] = None,\n) -&gt; Union[ndarray, list[ndarray], dict[str, ndarray]]\n</code></pre> <p>Predict regression target for X.</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNRegressor.predict--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     The input samples. output_type : str, default=\"mean\"     The type of prediction to return:     - \"mean\": Return mean prediction     - \"median\": Return median prediction     - \"mode\": Return mode prediction     - \"quantiles\": Return predictions for specified quantiles     - \"full\": Return full prediction details     - \"main\": Return main prediction metrics quantiles : list[float] or None, default=None     Quantiles to compute when output_type=\"quantiles\".     Default is [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.TabPFNRegressor.predict--returns","title":"Returns","text":"<p>array-like or dict     The predicted values.</p>"},{"location":"reference/tabpfn_client/estimator/#tabpfn_client.estimator.validate_data_size","title":"validate_data_size","text":"<pre><code>validate_data_size(\n    X: ndarray, y: Union[ndarray, None] = None\n)\n</code></pre> <p>Check the integrity of the training data. - check if the number of rows between X and y is consistent     if y is not None (ValueError) - check if the number of rows is less than MAX_ROWS (ValueError) - check if the number of columns is less than MAX_COLS (ValueError)</p>"},{"location":"reference/tabpfn_client/mock_prediction/","title":"Mock prediction","text":""},{"location":"reference/tabpfn_client/mock_prediction/#tabpfn_client.mock_prediction","title":"mock_prediction","text":""},{"location":"reference/tabpfn_client/mock_prediction/#tabpfn_client.mock_prediction.check_api_credits","title":"check_api_credits","text":"<pre><code>check_api_credits(func)\n</code></pre> <p>Decorator that first runs the decorated function in mock mode to simulate its credit usage. If user has enough credits, function is then executed for real.</p>"},{"location":"reference/tabpfn_client/mock_prediction/#tabpfn_client.mock_prediction.mock_mode","title":"mock_mode","text":"<pre><code>mock_mode()\n</code></pre> <p>Context manager that enables mock mode in the current thread.</p>"},{"location":"reference/tabpfn_client/mock_prediction/#tabpfn_client.mock_prediction.mock_predict","title":"mock_predict","text":"<pre><code>mock_predict(\n    X_test,\n    task: Literal[\"classification\", \"regression\"],\n    train_set_uid: str,\n    X_train,\n    y_train,\n    config=None,\n    predict_params=None,\n)\n</code></pre> <p>Mock function for prediction, which can be called instead of the real prediction function. Outputs random results in the expacted format and keeps track of the simulated cost and time.</p>"},{"location":"reference/tabpfn_client/prompt_agent/","title":"Prompt agent","text":""},{"location":"reference/tabpfn_client/prompt_agent/#tabpfn_client.prompt_agent","title":"prompt_agent","text":""},{"location":"reference/tabpfn_client/prompt_agent/#tabpfn_client.prompt_agent.PromptAgent","title":"PromptAgent","text":""},{"location":"reference/tabpfn_client/prompt_agent/#tabpfn_client.prompt_agent.PromptAgent.password_req_to_policy","title":"password_req_to_policy  <code>staticmethod</code>","text":"<pre><code>password_req_to_policy(password_req: list[str])\n</code></pre> <p>Small function that receives password requirements as a list of strings like \"Length(8)\" and returns a corresponding PasswordPolicy object.</p>"},{"location":"reference/tabpfn_client/service_wrapper/","title":"Service wrapper","text":""},{"location":"reference/tabpfn_client/service_wrapper/#tabpfn_client.service_wrapper","title":"service_wrapper","text":""},{"location":"reference/tabpfn_client/service_wrapper/#tabpfn_client.service_wrapper.InferenceClient","title":"InferenceClient","text":"<p>             Bases: <code>ServiceClientWrapper</code>, <code>Singleton</code></p> <p>Wrapper of ServiceClient to handle inference, including: - fitting - prediction - mock prediction</p>"},{"location":"reference/tabpfn_client/service_wrapper/#tabpfn_client.service_wrapper.UserAuthenticationClient","title":"UserAuthenticationClient","text":"<p>             Bases: <code>ServiceClientWrapper</code>, <code>Singleton</code></p> <p>Wrapper of ServiceClient to handle user authentication, including: - user registration and login - access token caching</p> <p>This is implemented as a singleton class with classmethods.</p>"},{"location":"reference/tabpfn_client/service_wrapper/#tabpfn_client.service_wrapper.UserAuthenticationClient.try_browser_login","title":"try_browser_login  <code>classmethod</code>","text":"<pre><code>try_browser_login() -&gt; tuple[bool, str]\n</code></pre> <p>Try to authenticate using browser-based login</p>"},{"location":"reference/tabpfn_client/service_wrapper/#tabpfn_client.service_wrapper.UserDataClient","title":"UserDataClient","text":"<p>             Bases: <code>ServiceClientWrapper</code>, <code>Singleton</code></p> <p>Wrapper of ServiceClient to handle user data, including: - query, or delete user account data - query, download, or delete uploaded data</p>"},{"location":"reference/tabpfn_client/tabpfn_common_utils/expense_estimation/","title":"Expense estimation","text":""},{"location":"reference/tabpfn_client/tabpfn_common_utils/expense_estimation/#tabpfn_client.tabpfn_common_utils.expense_estimation","title":"expense_estimation","text":""},{"location":"reference/tabpfn_client/tabpfn_common_utils/expense_estimation/#tabpfn_client.tabpfn_common_utils.expense_estimation.estimate_duration","title":"estimate_duration","text":"<pre><code>estimate_duration(\n    num_rows: int,\n    num_features: int,\n    task: Literal[\"classification\", \"regression\"],\n    tabpfn_config: dict = {},\n    duration_factor: float = VERTEX_GPU_FACTOR,\n    latency_offset: float = 0.0,\n) -&gt; float\n</code></pre> <p>Estimates the duration of a prediction task.</p>"},{"location":"reference/tabpfn_client/tabpfn_common_utils/regression_pred_result/","title":"Regression pred result","text":""},{"location":"reference/tabpfn_client/tabpfn_common_utils/regression_pred_result/#tabpfn_client.tabpfn_common_utils.regression_pred_result","title":"regression_pred_result","text":""},{"location":"reference/tabpfn_client/tabpfn_common_utils/utils/","title":"Utils","text":""},{"location":"reference/tabpfn_client/tabpfn_common_utils/utils/#tabpfn_client.tabpfn_common_utils.utils","title":"utils","text":""},{"location":"reference/tabpfn_client/tabpfn_common_utils/utils/#tabpfn_client.tabpfn_common_utils.utils.PreprocessorConfig","title":"PreprocessorConfig  <code>dataclass</code>","text":"<p>Configuration for data preprocessors.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['per_feature', 'power', 'safepower', 'power_box', 'safepower_box', 'quantile_uni_coarse', 'quantile_norm_coarse', 'quantile_uni', 'quantile_norm', 'quantile_uni_fine', 'quantile_norm_fine', 'robust', 'kdi', 'none', 'kdi_random_alpha', 'kdi_uni', 'kdi_random_alpha_uni', 'adaptive', 'norm_and_kdi', 'kdi_alpha_0.3_uni', 'kdi_alpha_0.5_uni', 'kdi_alpha_0.8_uni', 'kdi_alpha_1.0_uni', 'kdi_alpha_1.2_uni', 'kdi_alpha_1.5_uni', 'kdi_alpha_2.0_uni', 'kdi_alpha_3.0_uni', 'kdi_alpha_5.0_uni', 'kdi_alpha_0.3', 'kdi_alpha_0.5', 'kdi_alpha_0.8', 'kdi_alpha_1.0', 'kdi_alpha_1.2', 'kdi_alpha_1.5', 'kdi_alpha_2.0', 'kdi_alpha_3.0', 'kdi_alpha_5.0']</code> <p>Name of the preprocessor.</p> <code>categorical_name</code> <code>Literal['none', 'numeric', 'onehot', 'ordinal', 'ordinal_shuffled', 'ordinal_very_common_categories_shuffled']</code> <p>Name of the categorical encoding method. Options: \"none\", \"numeric\", \"onehot\", \"ordinal\", \"ordinal_shuffled\", \"none\".</p> <code>append_original</code> <code>bool</code> <p>Whether to append original features to the transformed features</p> <code>subsample_features</code> <code>float</code> <p>Fraction of features to subsample. -1 means no subsampling.</p> <code>global_transformer_name</code> <code>Union[str, None]</code> <p>Name of the global transformer to use.</p>"},{"location":"reference/tabpfn_client/tabpfn_common_utils/utils/#tabpfn_client.tabpfn_common_utils.utils.PreprocessorConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert the PreprocessorConfig instance to a dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing the configuration parameters.</p>"},{"location":"reference/tabpfn_extensions/utils/","title":"Utils","text":""},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils","title":"utils","text":""},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.ClientTabPFNClassifier","title":"ClientTabPFNClassifier","text":"<p>             Bases: <code>TabPFNClassifier</code></p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.ClientTabPFNClassifier.get_params","title":"get_params","text":"<pre><code>get_params(deep: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Return parameters for this estimator.</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.ClientTabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict class labels for samples in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The input samples.</p> required <p>Returns:</p> Type Description <p>The predicted class labels.</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.ClientTabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict class probabilities for X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The input samples.</p> required <p>Returns:</p> Type Description <p>The class probabilities of the input samples.</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.ClientTabPFNRegressor","title":"ClientTabPFNRegressor","text":"<p>             Bases: <code>TabPFNRegressor</code></p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.ClientTabPFNRegressor.get_params","title":"get_params","text":"<pre><code>get_params(deep: bool = True) -&gt; dict[str, Any]\n</code></pre> <p>Return parameters for this estimator.</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.ClientTabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(X, output_type=None, **kwargs)\n</code></pre> <p>Predict target values for X.</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.ClientTabPFNRegressor.predict--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     The input samples.</p> str, default=None <p>Type of output to return. Options are: - None: Default prediction (mean) - \"full\": Return distribution dictionary with criterion object - Other values are passed to the parent predict</p> <p>**kwargs : Additional keyword arguments     Passed to the parent predict method.</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.ClientTabPFNRegressor.predict--returns","title":"Returns:","text":"<p>y : array-like of shape (n_samples,) or dict     The predicted values or the full distribution output dictionary.</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.get_device","title":"get_device","text":"<pre><code>get_device(device: str | None = 'auto') -&gt; str\n</code></pre> <p>Determine the appropriate device for computation.</p> <p>This function implements automatic device selection, defaulting to CUDA if available, otherwise falling back to CPU.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str | None</code> <p>Device specification, options are: - \"auto\": Automatically use CUDA if available, otherwise CPU - \"cpu\": Force CPU usage - \"cuda\": Force CUDA usage (raises error if not available) - None: Same as \"auto\"</p> <code>'auto'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The resolved device string (\"cpu\" or \"cuda\")</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If \"cuda\" is explicitly requested but not available</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.get_tabpfn_models","title":"get_tabpfn_models","text":"<pre><code>get_tabpfn_models() -&gt; tuple[type, type]\n</code></pre> <p>Get TabPFN models with fallback between different versions.</p> <p>Attempts to import TabPFN models in the following order: 1. Standard TabPFN package (if USE_TABPFN_LOCAL is True) 2. TabPFN client</p> <p>Returns:</p> Type Description <code>tuple[type, type]</code> <p>tuple[type, type]: A tuple containing (TabPFNClassifier, TabPFNRegressor) classes</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If none of the TabPFN implementations could be imported</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.infer_categorical_features","title":"infer_categorical_features","text":"<pre><code>infer_categorical_features(\n    X: ndarray,\n    categorical_features: list[int] | None = None,\n) -&gt; list[int]\n</code></pre> <p>Infer the categorical features from the input data.</p> <p>Features are identified as categorical if any of these conditions are met: 1. The feature index is in the provided categorical_features list AND has few unique values 2. The feature has few unique values compared to the dataset size 3. The feature has string/object/category data type (pandas DataFrame) 4. The feature contains string values (numpy array)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or DataFrame</code> <p>The input data.</p> required <code>categorical_features</code> <code>list[int]</code> <p>Initial list of categorical feature indices. If None, will start with an empty list.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>list[int]: The indices of the categorical features.</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.is_tabpfn","title":"is_tabpfn","text":"<pre><code>is_tabpfn(estimator: Any) -&gt; bool\n</code></pre> <p>Check if an estimator is a TabPFN model.</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.product_dict","title":"product_dict","text":"<pre><code>product_dict(\n    d: dict[str, list[T]]\n) -&gt; Iterator[dict[str, T]]\n</code></pre> <p>Cartesian product of a dictionary of lists.</p> <p>This function takes a dictionary where each value is a list, and returns an iterator over dictionaries where each key is mapped to one element from the corresponding list.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict[str, list[T]]</code> <p>A dictionary mapping keys to lists of values.</p> required <p>Returns:</p> Type Description <code>dict[str, T]</code> <p>An iterator over dictionaries, each being one element of the cartesian</p> <code>dict[str, T]</code> <p>product of the input dictionary.</p> Example <p>list(product_dict({'a': [1, 2], 'b': ['x', 'y']})) [{'a': 1, 'b': 'x'}, {'a': 1, 'b': 'y'}, {'a': 2, 'b': 'x'}, {'a': 2, 'b': 'y'}]</p>"},{"location":"reference/tabpfn_extensions/utils/#tabpfn_extensions.utils.softmax","title":"softmax","text":"<pre><code>softmax(logits: NDArray) -&gt; NDArray\n</code></pre> <p>Apply softmax function to convert logits to probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>NDArray</code> <p>Input logits array of shape (n_samples, n_classes) or (n_classes,)</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Probabilities where values sum to 1 across the last dimension</p>"},{"location":"reference/tabpfn_extensions/benchmarking/experiment/","title":"Experiment","text":""},{"location":"reference/tabpfn_extensions/benchmarking/experiment/#tabpfn_extensions.benchmarking.experiment","title":"experiment","text":""},{"location":"reference/tabpfn_extensions/benchmarking/experiment/#tabpfn_extensions.benchmarking.experiment.Experiment","title":"Experiment","text":"<p>Base class for experiments. Experiments should be reproducible, i.e. the settings should give all the information     needed to run the experiment. Experiments should be deterministic, i.e. the same settings should always give the same results.</p>"},{"location":"reference/tabpfn_extensions/benchmarking/experiment/#tabpfn_extensions.benchmarking.experiment.Experiment.run","title":"run","text":"<pre><code>run(tabpfn, **kwargs)\n</code></pre> <p>Runs the experiment.</p> <p>Should set self.results</p>"},{"location":"reference/tabpfn_extensions/embedding/tabpfn_embedding/","title":"Tabpfn embedding","text":""},{"location":"reference/tabpfn_extensions/embedding/tabpfn_embedding/#tabpfn_extensions.embedding.tabpfn_embedding","title":"tabpfn_embedding","text":""},{"location":"reference/tabpfn_extensions/embedding/tabpfn_embedding/#tabpfn_extensions.embedding.tabpfn_embedding.TabPFNEmbedding","title":"TabPFNEmbedding","text":"<p>TabPFNEmbedding is a utility for extracting embeddings from TabPFNClassifier or TabPFNRegressor models. It supports standard training (vanilla embedding) as well as K-fold cross-validation for embedding extraction.</p> <ul> <li>When <code>n_fold=0</code>, the model extracts vanilla embeddings by training on the entire dataset.</li> <li>When <code>n_fold&gt;0</code>, K-fold cross-validation is applied based on the method proposed in   \"A Closer Look at TabPFN v2: Strength, Limitation, and Extension\" (https://arxiv.org/abs/2502.17361),   where a larger <code>n_fold</code> improves embedding effectiveness.</li> </ul> <p>NOTE: This functionality requires the full TabPFN implementation (pip install tabpfn) and is not compatible with the TabPFN client (pip install tabpfn-client). The client version does not provide access to model embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>tabpfn_clf</code> <p>TabPFNClassifier, optional An instance of TabPFNClassifier to handle classification tasks.</p> <code>None</code> <code>tabpfn_reg</code> <p>TabPFNRegressor, optional An instance of TabPFNRegressor to handle regression tasks.</p> <code>None</code> <code>n_fold</code> <p>int, default=0 Number of folds for K-fold cross-validation. If set to 0, standard training is used.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>model</code> <p>TabPFNClassifier or TabPFNRegressor The model used for embedding extraction.</p> <pre><code>&gt;&gt;&gt; from tabpfn_extensions import TabPFNClassifier  # Must use full TabPFN package\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from sklearn.datasets import fetch_openml\n&gt;&gt;&gt; X, y = fetch_openml(name='kc1', version=1, as_frame=False, return_X_y=True)\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n&gt;&gt;&gt; clf = TabPFNClassifier(n_estimators=1)\n&gt;&gt;&gt; embedding_extractor = TabPFNEmbedding(tabpfn_clf=clf, n_fold=0)\n&gt;&gt;&gt; train_embeddings = embedding_extractor.get_embeddings(X_train, y_train, X_test, data_source=\"train\")\n&gt;&gt;&gt; test_embeddings = embedding_extractor.get_embeddings(X_train, y_train, X_test, data_source=\"test\")\n</code></pre>"},{"location":"reference/tabpfn_extensions/embedding/tabpfn_embedding/#tabpfn_extensions.embedding.tabpfn_embedding.TabPFNEmbedding.fit","title":"fit","text":"<pre><code>fit(X_train: ndarray, y_train: ndarray) -&gt; None\n</code></pre> <p>Trains the TabPFN model on the given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Training feature data.</p> required <code>y_train</code> <code>ndarray</code> <p>Training target labels.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If no model is set before calling fit.</p>"},{"location":"reference/tabpfn_extensions/embedding/tabpfn_embedding/#tabpfn_extensions.embedding.tabpfn_embedding.TabPFNEmbedding.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    X_train: ndarray,\n    y_train: ndarray,\n    X: ndarray,\n    data_source: str,\n) -&gt; ndarray\n</code></pre> <p>Extracts embeddings for the given dataset using the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Training feature data.</p> required <code>y_train</code> <code>ndarray</code> <p>Training target labels.</p> required <code>X</code> <code>ndarray</code> <p>Data for which embeddings are to be extracted.</p> required <code>data_source</code> <code>str</code> <p>Specifies the data source (\"test\" for test data).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The extracted embeddings.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no model is set before calling get_embeddings.</p>"},{"location":"reference/tabpfn_extensions/hpo/search_space/","title":"Search space","text":""},{"location":"reference/tabpfn_extensions/hpo/search_space/#tabpfn_extensions.hpo.search_space","title":"search_space","text":"<p>Search spaces for hyperparameter optimization of TabPFN models.</p> <p>This module provides predefined search spaces for TabPFN classifier and regressor hyperparameter optimization. It also includes utilities for customizing search spaces.</p>"},{"location":"reference/tabpfn_extensions/hpo/search_space/#tabpfn_extensions.hpo.search_space.TabPFNSearchSpace","title":"TabPFNSearchSpace","text":"<p>Utility class for creating and customizing TabPFN hyperparameter search spaces.</p> <p>This class provides methods to generate default search spaces for both classification and regression tasks, as well as customizing parameter ranges.</p> <p>Examples:</p> <pre><code># Get default classifier search space\nclf_space = TabPFNSearchSpace.get_classifier_space()\n\n# Get customized classifier search space\ncustom_space = TabPFNSearchSpace.get_classifier_space(\n    n_ensemble_range=(5, 15),\n    temp_range=(0.1, 0.5)\n)\n\n# Use with TunedTabPFNClassifier\nfrom tabpfn_extensions.hpo import TunedTabPFNClassifier\n\nclf = TunedTabPFNClassifier(\n    n_trials=50,\n    search_space=custom_space\n)\n</code></pre>"},{"location":"reference/tabpfn_extensions/hpo/search_space/#tabpfn_extensions.hpo.search_space.TabPFNSearchSpace.get_classifier_space","title":"get_classifier_space  <code>staticmethod</code>","text":"<pre><code>get_classifier_space(\n    n_ensemble_range: tuple[int, int] = (1, 8),\n    temp_range: tuple[float, float] = (0.75, 1.0),\n) -&gt; dict[str, Any]\n</code></pre> <p>Get a search space for classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>n_ensemble_range</code> <code>tuple[int, int]</code> <p>Range for n_estimators parameter as (min, max)</p> <code>(1, 8)</code> <code>temp_range</code> <code>tuple[float, float]</code> <p>Range for softmax_temperature as (min, max)</p> <code>(0.75, 1.0)</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with search space parameters</p>"},{"location":"reference/tabpfn_extensions/hpo/search_space/#tabpfn_extensions.hpo.search_space.TabPFNSearchSpace.get_regressor_space","title":"get_regressor_space  <code>staticmethod</code>","text":"<pre><code>get_regressor_space(\n    n_ensemble_range: tuple[int, int] = (1, 8),\n    temp_range: tuple[float, float] = (0.75, 1.0),\n) -&gt; dict[str, Any]\n</code></pre> <p>Get a search space for regression tasks.</p> <p>Parameters:</p> Name Type Description Default <code>n_ensemble_range</code> <code>tuple[int, int]</code> <p>Range for n_estimators parameter as (min, max)</p> <code>(1, 8)</code> <code>temp_range</code> <code>tuple[float, float]</code> <p>Range for softmax_temperature as (min, max)</p> <code>(0.75, 1.0)</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with search space parameters</p>"},{"location":"reference/tabpfn_extensions/hpo/search_space/#tabpfn_extensions.hpo.search_space.get_param_grid_hyperopt","title":"get_param_grid_hyperopt","text":"<pre><code>get_param_grid_hyperopt(task_type: str) -&gt; dict\n</code></pre> <p>Generate the full hyperopt search space for TabPFN optimization.</p> <p>Parameters:</p> Name Type Description Default <code>task_type</code> <code>str</code> <p>Either \"multiclass\" or \"regression\"</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Hyperopt search space dictionary</p>"},{"location":"reference/tabpfn_extensions/hpo/tuned_tabpfn/","title":"Tuned tabpfn","text":""},{"location":"reference/tabpfn_extensions/hpo/tuned_tabpfn/#tabpfn_extensions.hpo.tuned_tabpfn","title":"tuned_tabpfn","text":"<p>Hyperparameter Optimization (HPO) for TabPFN models.</p> <p>This module provides automatic tuning capabilities for TabPFN models using Bayesian optimization via Hyperopt. It finds optimal hyperparameters for both the TabPFN model and its inference configuration.</p> <p>Key features: - Optimized search spaces for classification and regression tasks - Support for multiple evaluation metrics (accuracy, ROC-AUC, F1, RMSE, MAE) - Proper handling of categorical features through automatic encoding - Compatible with both TabPFN and TabPFN-client backends - Implements scikit-learn's estimator interface for easy integration - Built-in validation strategies for reliable performance estimation</p> Example usage <pre><code>from tabpfn_extensions.hpo import TunedTabPFNClassifier\n\n# Create a tuned classifier with 50 optimization trials\ntuned_clf = TunedTabPFNClassifier(\n    n_trials=50,                    # Number of hyperparameter configurations to try\n    metric='accuracy',              # Metric to optimize\n    categorical_feature_indices=[0, 2],  # Categorical features\n    random_state=42                 # For reproducibility\n)\n\n# Fit will automatically find the best hyperparameters\ntuned_clf.fit(X_train, y_train)\n\n# Use like any scikit-learn estimator\ny_pred = tuned_clf.predict(X_test)\n</code></pre>"},{"location":"reference/tabpfn_extensions/hpo/tuned_tabpfn/#tabpfn_extensions.hpo.tuned_tabpfn.MetricType","title":"MetricType","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Supported evaluation metrics for TabPFN hyperparameter tuning.</p> <p>This enum defines the metrics that can be used to evaluate and select the best hyperparameter configuration during optimization.</p> Values"},{"location":"reference/tabpfn_extensions/hpo/tuned_tabpfn/#tabpfn_extensions.hpo.tuned_tabpfn.TunedTabPFNBase","title":"TunedTabPFNBase","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>Base class for tuned TabPFN models with proper categorical handling.</p>"},{"location":"reference/tabpfn_extensions/hpo/tuned_tabpfn/#tabpfn_extensions.hpo.tuned_tabpfn.TunedTabPFNClassifier","title":"TunedTabPFNClassifier","text":"<p>             Bases: <code>TunedTabPFNBase</code>, <code>ClassifierMixin</code></p> <p>TabPFN Classifier with hyperparameter tuning and proper categorical handling.</p>"},{"location":"reference/tabpfn_extensions/hpo/tuned_tabpfn/#tabpfn_extensions.hpo.tuned_tabpfn.TunedTabPFNRegressor","title":"TunedTabPFNRegressor","text":"<p>             Bases: <code>TunedTabPFNBase</code>, <code>RegressorMixin</code></p> <p>TabPFN Regressor with hyperparameter tuning and proper categorical handling.</p>"},{"location":"reference/tabpfn_extensions/interpretability/experiments/","title":"Experiments","text":""},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments","title":"experiments","text":""},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments.Experiment","title":"Experiment","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for experiments.</p>"},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments.Experiment.plot","title":"plot  <code>abstractmethod</code>","text":"<pre><code>plot(**kwargs)\n</code></pre> <p>Plot experiment results.</p>"},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments.Experiment.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(*args, **kwargs)\n</code></pre> <p>Run the experiment.</p>"},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments.FeatureSelectionExperiment","title":"FeatureSelectionExperiment","text":"<p>             Bases: <code>Experiment</code></p> <p>This class is used to run experiments on generating synthetic data.</p>"},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments.FeatureSelectionExperiment.run","title":"run","text":"<pre><code>run(tabpfn, **kwargs)\n</code></pre> <p>:param tabpfn: :param kwargs:     indices: list of indices from X features to use :return:</p>"},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments.FeatureSelectionInPredictExperiment","title":"FeatureSelectionInPredictExperiment","text":"<p>             Bases: <code>Experiment</code></p> <p>This class is used to run experiments on generating synthetic data.</p>"},{"location":"reference/tabpfn_extensions/interpretability/experiments/#tabpfn_extensions.interpretability.experiments.FeatureSelectionInPredictExperiment.run","title":"run","text":"<pre><code>run(tabpfn, **kwargs)\n</code></pre> <p>:param tabpfn: :param kwargs:     indices: list of indices from X features to use :return:</p>"},{"location":"reference/tabpfn_extensions/interpretability/feature_selection/","title":"Feature selection","text":""},{"location":"reference/tabpfn_extensions/interpretability/feature_selection/#tabpfn_extensions.interpretability.feature_selection","title":"feature_selection","text":""},{"location":"reference/tabpfn_extensions/interpretability/feature_selection/#tabpfn_extensions.interpretability.feature_selection.feature_selection","title":"feature_selection","text":"<pre><code>feature_selection(\n    estimator: BaseEstimator,\n    X: ndarray,\n    y: ndarray,\n    n_features_to_select: int = 3,\n    feature_names: list[str] | None = None,\n    **kwargs\n) -&gt; SequentialFeatureSelector\n</code></pre> <p>Perform feature selection to find the most important features.</p> <p>Uses forward sequential feature selection to identify the most important features for the given estimator and data.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>BaseEstimator</code> <p>The model to use for feature selection</p> required <code>X</code> <code>ndarray</code> <p>Input features, shape (n_samples, n_features)</p> required <code>y</code> <code>ndarray</code> <p>Target values, shape (n_samples,)</p> required <code>n_features_to_select</code> <code>int</code> <p>Number of features to select</p> <code>3</code> <code>feature_names</code> <code>list[str] | None</code> <p>Names of the features (optional)</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters to pass to SequentialFeatureSelector</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SequentialFeatureSelector</code> <code>SequentialFeatureSelector</code> <p>Fitted feature selector that can be used to transform data to use only the selected features</p>"},{"location":"reference/tabpfn_extensions/interpretability/shap/","title":"Shap","text":""},{"location":"reference/tabpfn_extensions/interpretability/shap/#tabpfn_extensions.interpretability.shap","title":"shap","text":"<p>SHAP value computation and visualization for TabPFN.</p> <p>This module provides functions to calculate and visualize SHAP (SHapley Additive exPlanations) values for TabPFN models. SHAP values help understand model predictions by attributing the contribution of each input feature to the output prediction.</p> <p>Key features: - Efficient parallel computation of SHAP values - Support for both TabPFN and TabPFN-client backends - Specialized explainers for TabPFN models - Visualization functions for feature importance and interactions - Backend-specific optimizations for faster SHAP computation</p> Example usage <pre><code>from tabpfn import TabPFNClassifier\nfrom tabpfn_extensions.interpretability import get_shap_values, plot_shap\n\n# Train a TabPFN model\nmodel = TabPFNClassifier()\nmodel.fit(X_train, y_train)\n\n# Calculate SHAP values\nshap_values = get_shap_values(model, X_test)\n\n# Visualize feature importance\nplot_shap(shap_values)\n</code></pre>"},{"location":"reference/tabpfn_extensions/interpretability/shap/#tabpfn_extensions.interpretability.shap.calculate_shap_subset","title":"calculate_shap_subset","text":"<pre><code>calculate_shap_subset(args: tuple) -&gt; ndarray\n</code></pre> <p>Calculate SHAP values for a specific feature in a parallel context.</p> <p>This helper function is used by parallel_permutation_shap to enable efficient parallel computation of SHAP values for each feature.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>tuple</code> <p>A tuple containing: - X_subset: Feature matrix for which to calculate SHAP values - background: Background data for the explainer - model: The model for which to calculate SHAP values - feature_idx: The index of the feature to calculate SHAP values for</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: SHAP values for the specified feature</p>"},{"location":"reference/tabpfn_extensions/interpretability/shap/#tabpfn_extensions.interpretability.shap.get_default_explainer","title":"get_default_explainer","text":"<pre><code>get_default_explainer(\n    estimator: Any,\n    test_x: DataFrame,\n    predict_function_for_shap: str | Callable = \"predict\",\n    **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Create a standard SHAP explainer for non-TabPFN models.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>The model to explain.</p> required <code>test_x</code> <code>DataFrame</code> <p>The input features to compute SHAP values for.</p> required <code>predict_function_for_shap</code> <code>str | Callable</code> <p>Function name or callable to use for prediction. Defaults to \"predict\".</p> <code>'predict'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the SHAP explainer.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A configured SHAP explainer for the model.</p>"},{"location":"reference/tabpfn_extensions/interpretability/shap/#tabpfn_extensions.interpretability.shap.get_shap_values","title":"get_shap_values","text":"<pre><code>get_shap_values(\n    estimator: Any,\n    test_x: DataFrame | ndarray | Tensor,\n    attribute_names: list[str] | None = None,\n    **kwargs: Any\n) -&gt; ndarray\n</code></pre> <p>Compute SHAP values for a model's predictions on input features.</p> <p>This function calculates SHAP (SHapley Additive exPlanations) values that attribute the contribution of each input feature to the model's output. It automatically selects the appropriate SHAP explainer based on the model.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>The model to explain, typically a TabPFNClassifier or scikit-learn compatible model.</p> required <code>test_x</code> <code>DataFrame | ndarray | Tensor</code> <p>The input features to compute SHAP values for.</p> required <code>attribute_names</code> <code>list[str] | None</code> <p>Column names for the features when test_x is a numpy array.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the SHAP explainer.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The computed SHAP values with shape (n_samples, n_features).</p>"},{"location":"reference/tabpfn_extensions/interpretability/shap/#tabpfn_extensions.interpretability.shap.get_tabpfn_explainer","title":"get_tabpfn_explainer","text":"<pre><code>get_tabpfn_explainer(\n    estimator: Any,\n    test_x: DataFrame,\n    predict_function_for_shap: str | Callable = \"predict\",\n    **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Create a SHAP explainer specifically optimized for TabPFN models.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>The TabPFN model to explain.</p> required <code>test_x</code> <code>DataFrame</code> <p>The input features to compute SHAP values for.</p> required <code>predict_function_for_shap</code> <code>str | Callable</code> <p>Function name or callable to use for prediction. Defaults to \"predict\".</p> <code>'predict'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the SHAP explainer.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A configured SHAP explainer for the TabPFN model.</p>"},{"location":"reference/tabpfn_extensions/interpretability/shap/#tabpfn_extensions.interpretability.shap.parallel_permutation_shap","title":"parallel_permutation_shap","text":"<pre><code>parallel_permutation_shap(\n    model: Any,\n    X: ndarray | DataFrame,\n    background: ndarray | DataFrame | None = None,\n    n_jobs: int = -1,\n) -&gt; ndarray\n</code></pre> <p>Calculate SHAP values efficiently using parallel processing.</p> <p>This function distributes the SHAP value calculation across multiple processes, with each process computing values for a different feature. This is much faster than calculating all SHAP values at once, especially for large datasets or complex models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The model for which to calculate SHAP values. Must have a prediction method.</p> required <code>X</code> <code>ndarray | DataFrame</code> <p>Feature matrix for which to calculate SHAP values.</p> required <code>background</code> <code>ndarray | DataFrame | None</code> <p>Background data for the explainer. If None, X is used as background data.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of processes to use for parallel computation. If -1, all available CPU cores are used.</p> <code>-1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Matrix of SHAP values with shape (n_samples, n_features).</p>"},{"location":"reference/tabpfn_extensions/interpretability/shap/#tabpfn_extensions.interpretability.shap.plot_shap","title":"plot_shap","text":"<pre><code>plot_shap(shap_values: ndarray) -&gt; None\n</code></pre> <p>Plot SHAP values for the given test data.</p> <p>This function creates several visualizations of SHAP values: 1. Aggregated feature importances across all examples 2. Per-sample feature importances 3. Important feature interactions (if multiple samples provided)</p> <p>Parameters:</p> Name Type Description Default <code>shap_values</code> <code>ndarray</code> <p>The SHAP values to plot, typically from get_shap_values().</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>This function only produces visualizations.</p>"},{"location":"reference/tabpfn_extensions/interpretability/shap/#tabpfn_extensions.interpretability.shap.plot_shap_feature","title":"plot_shap_feature","text":"<pre><code>plot_shap_feature(\n    shap_values_: Any,\n    feature_name: int | str,\n    n_plots: int = 1,\n) -&gt; None\n</code></pre> <p>Plot feature interactions for a specific feature based on SHAP values.</p> <p>Parameters:</p> Name Type Description Default <code>shap_values_</code> <code>Any</code> <p>SHAP values object containing the data to plot.</p> required <code>feature_name</code> <code>int | str</code> <p>The feature index or name to plot interactions for.</p> required <code>n_plots</code> <code>int</code> <p>Number of interaction plots to create. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>This function only produces visualizations.</p>"},{"location":"reference/tabpfn_extensions/interpretability/shapiq/","title":"Shapiq","text":""},{"location":"reference/tabpfn_extensions/interpretability/shapiq/#tabpfn_extensions.interpretability.shapiq","title":"shapiq","text":"<p>TabPFN explainer adapters that use the ShapIQ library for model interpretability.</p> <p>This module provides functions to create shapiq explainers for TabPFN models that support both basic Shapley values and interaction indices for more detailed model explanations.</p>"},{"location":"reference/tabpfn_extensions/interpretability/shapiq/#tabpfn_extensions.interpretability.shapiq.get_tabpfn_explainer","title":"get_tabpfn_explainer","text":"<pre><code>get_tabpfn_explainer(\n    model: TabPFNRegressor | TabPFNClassifier,\n    data: DataFrame | ndarray,\n    labels: DataFrame | ndarray,\n    index: str = \"k-SII\",\n    max_order: int = 2,\n    class_index: int | None = None,\n    **kwargs\n)\n</code></pre> <p>Get a TabPFNExplainer from shapiq.</p> <p>This function returns the TabPFN explainer from the shapiq[1]_ library. The explainer uses a remove-and-recontextualize paradigm of model explanation[2][3] to explain the predictions of a TabPFN model. See <code>shapiq.TabPFNExplainer</code> documentation for more information regarding the explainer object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>TabPFNRegressor or TabPFNClassifier</code> <p>The TabPFN model to explain.</p> required <code>data</code> <code>DataFrame or ndarray</code> <p>The background data to use for the explainer.</p> required <code>labels</code> <code>DataFrame or ndarray</code> <p>The labels for the background data.</p> required <code>index</code> <code>str</code> <p>The index to use for the explanation. See shapiq documentation for more information and an up-to-date list of available indices. Defaults to \"k-SII\" and \"SV\" (Shapley Values like SHAP) with <code>max_order=1</code>.</p> <code>'k-SII'</code> <code>max_order</code> <code>int</code> <p>The maximum order of interactions to consider. Defaults to 2.</p> <code>2</code> <code>class_index</code> <code>int</code> <p>The class index of the model to explain. If not provided, the class index will be set to 1 per default for classification models. This argument is ignored for regression models. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the explainer.</p> <code>{}</code> <p>Returns:</p> Type Description <p>shapiq.TabPFNExplainer: The TabPFN explainer.</p> References <p>.. [1] shapiq repository: https://github.com/mmschlk/shapiq .. [2] Muschalik, M., Baniecki, H., Fumagalli, F., Kolpaczki, P., Hammer, B., H\u00fcllermeier, E. (2024). shapiq: Shapley Interactions for Machine Learning. In: The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. url: https://openreview.net/forum?id=knxGmi6SJi .. [3] Rundel, D., Kobialka, J., von Crailsheim, C., Feurer, M., Nagler, T., R\u00fcgamer, D. (2024). Interpretable Machine Learning for TabPFN. In: Longo, L., Lapuschkin, S., Seifert, C. (eds) Explainable Artificial Intelligence. xAI 2024. Communications in Computer and Information Science, vol 2154. Springer, Cham. https://doi.org/10.1007/978-3-031-63797-1_23</p>"},{"location":"reference/tabpfn_extensions/interpretability/shapiq/#tabpfn_extensions.interpretability.shapiq.get_tabpfn_imputation_explainer","title":"get_tabpfn_imputation_explainer","text":"<pre><code>get_tabpfn_imputation_explainer(\n    model: TabPFNRegressor | TabPFNClassifier,\n    data: DataFrame | ndarray,\n    index: str = \"k-SII\",\n    max_order: int = 2,\n    imputer: str = \"marginal\",\n    class_index: int | None = None,\n    **kwargs\n)\n</code></pre> <p>Gets a TabularExplainer from shapiq with using imputation.</p> <p>This function returns the TabularExplainer from the shapiq[1][2] library. The explainer uses an imputation-based paradigm of feature removal for the explanations similar to SHAP[3]_. See <code>shapiq.TabularExplainer</code> documentation for more information regarding the explainer object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>TabPFNRegressor or TabPFNClassifier</code> <p>The TabPFN model to explain.</p> required <code>data</code> <code>DataFrame or ndarray</code> <p>The background data to use for the explainer.</p> required <code>index</code> <code>str</code> <p>The index to use for the explanation. See shapiq documentation for more information and an up-to-date list of available indices. Defaults to \"k-SII\" and \"SV\" (Shapley Values like SHAP) with <code>max_order=1</code>.</p> <code>'k-SII'</code> <code>max_order</code> <code>int</code> <p>The maximum order of interactions to consider. Defaults to 2.</p> <code>2</code> <code>imputer</code> <code>str</code> <p>The imputation method to use. See <code>shapiq.TabularExplainer</code> documentation for more information and an up-to-date list of available imputation methods.</p> <code>'marginal'</code> <code>class_index</code> <code>int</code> <p>The class index of the model to explain. If not provided, the class index will be set to 1 per default for classification models. This argument is ignored for regression models. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the explainer.</p> <code>{}</code> <p>Returns:</p> Type Description <p>shapiq.TabularExplainer: The TabularExplainer.</p> References <p>.. [1] shapiq repository: https://github.com/mmschlk/shapiq .. [2] Muschalik, M., Baniecki, H., Fumagalli, F., Kolpaczki, P., Hammer, B., H\u00fcllermeier, E. (2024). shapiq: Shapley Interactions for Machine Learning. In: The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. url: https://openreview.net/forum?id=knxGmi6SJi .. [3] Lundberg, S. M., &amp; Lee, S. I. (2017). A Unified Approach to Interpreting Model Predictions. Advances in Neural Information Processing Systems 30 (pp. 4765--4774).</p>"},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/","title":"Many class classifier","text":""},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/#tabpfn_extensions.many_class.many_class_classifier","title":"many_class_classifier","text":"<p>Development Notebook: https://colab.research.google.com/drive/1HWF5IF0IN21G8FZdLVwBbLBkCMu94yBA?usp=sharing</p> <p>This module provides a classifier that overcomes TabPFN's limitation on the number of classes (typically 10) by using a meta-classifier approach based on output coding. It works by breaking down multi-class problems into multiple sub-problems, each within TabPFN's class limit.</p> <p>This version aims to be very close to an original structural design, with key improvements in codebook generation and using a custom <code>validate_data</code> function for scikit-learn compatibility.</p> <p>Key features (compared to a very basic output coder): - Improved codebook generation: Uses a strategy that attempts to balance the   number of times each class is explicitly represented and guarantees coverage. - Codebook statistics: Optionally prints statistics about the generated codebook. - Uses a custom <code>validate_data</code> for potentially better cross-sklearn-version   compatibility for data validation. - Robustness: Minor changes for better scikit-learn compatibility (e.g.,   ensuring the wrapper is properly \"fitted\", setting n_features_in_).</p> <p>Original structural aspects retained: - Fitting of base estimators for sub-problems largely occurs during predict_proba calls.</p> Example usage <pre><code>import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tabpfn import TabPFNClassifier # Assuming TabPFN is installed\nfrom sklearn.datasets import make_classification\n\n# Create synthetic data with many classes\nn_classes_total = 15 # TabPFN might struggle with &gt;10 if not configured\nX, y = make_classification(n_samples=300, n_features=20, n_informative=15,\n                           n_redundant=0, n_classes=n_classes_total,\n                           n_clusters_per_class=1, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n                                                    random_state=42,\n                                                    stratify=y)\n\n# Create a TabPFN base classifier\n# Adjust N_ensemble_configurations and device as needed/available\n# TabPFN's default class limit is often 10 for the public model.\nbase_clf = TabPFNClassifier(device='cpu', N_ensemble_configurations=4)\n\n# Wrap it with ManyClassClassifier\nmany_class_clf = ManyClassClassifier(\n    estimator=base_clf,\n    alphabet_size=10, # Max classes the base_clf sub-problems will handle\n                      # This should align with TabPFN's actual capability.\n    n_estimators_redundancy=3,\n    random_state=42,\n    log_proba_aggregation=True,\n    verbose=1 # Print codebook stats\n)\n\n# Use like any scikit-learn classifier\nmany_class_clf.fit(X_train, y_train)\ny_pred = many_class_clf.predict(X_test)\ny_proba = many_class_clf.predict_proba(X_test)\n\nprint(f\"Prediction shape: {y_pred.shape}\")\nprint(f\"Probability shape: {y_proba.shape}\")\nif hasattr(many_class_clf, 'codebook_stats_'):\n    print(f\"Codebook Stats: {many_class_clf.codebook_stats_}\")\n</code></pre>"},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/#tabpfn_extensions.many_class.many_class_classifier.ManyClassClassifier","title":"ManyClassClassifier","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Output-Code multiclass strategy to extend classifiers beyond their class limit.</p> <p>This version adheres closely to an original structural design, with key improvements in codebook generation and using a custom <code>validate_data</code> function for scikit-learn compatibility. Fitting for sub-problems primarily occurs during prediction.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>BaseEstimator</code> <p>A classifier implementing fit() and predict_proba() methods.</p> required <code>alphabet_size</code> <code>int</code> <p>Maximum number of classes the base estimator can handle. If None, attempts to infer from <code>estimator.max_num_classes_</code>.</p> <code>None</code> <code>n_estimators</code> <code>int</code> <p>Number of base estimators (sub-problems). If None, calculated based on other parameters.</p> <code>None</code> <code>n_estimators_redundancy</code> <code>int</code> <p>Redundancy factor for auto-calculated <code>n_estimators</code>. Defaults to 4.</p> <code>4</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls randomization for codebook generation.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>Controls verbosity. If &gt; 0, prints codebook stats. Defaults to 0.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>classes_</code> <code>ndarray</code> <p>Unique target labels.</p> <code>code_book_</code> <code>ndarray | None</code> <p>Generated codebook if mapping is needed.</p> <code>codebook_stats_</code> <code>dict</code> <p>Statistics about the generated codebook.</p> <code>estimators_</code> <code>list | None</code> <p>Stores the single fitted base estimator only if <code>no_mapping_needed_</code> is True.</p> <code>no_mapping_needed_</code> <code>bool</code> <p>True if n_classes &lt;= alphabet_size.</p> <code>classes_index_</code> <code>dict | None</code> <p>Maps class labels to indices.</p> <code>X_train</code> <code>ndarray | None</code> <p>Stored training features if mapping needed.</p> <code>Y_train_per_estimator</code> <code>ndarray | None</code> <p>Encoded training labels for each sub-problem.                             Shape (n_estimators, n_samples).</p> <code>n_features_in_</code> <code>int</code> <p>Number of features seen during <code>fit</code>.</p> <code>feature_names_in_</code> <code>ndarray | None</code> <p>Names of features seen during <code>fit</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from tabpfn import TabPFNClassifier\n&gt;&gt;&gt; from tabpfn_extensions.many_class import ManyClassClassifier\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; X, y = load_iris(return_X_y=True)\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n&gt;&gt;&gt; base_clf = TabPFNClassifier()\n&gt;&gt;&gt; many_clf = ManyClassClassifier(base_clf, alphabet_size=base_clf.max_num_classes_)\n&gt;&gt;&gt; many_clf.fit(X_train, y_train)\n&gt;&gt;&gt; y_pred = many_clf.predict(X_test)\n</code></pre>"},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/#tabpfn_extensions.many_class.many_class_classifier.ManyClassClassifier.codebook_statistics_","title":"codebook_statistics_  <code>property</code>","text":"<pre><code>codebook_statistics_\n</code></pre> <p>Returns statistics about the generated codebook.</p>"},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/#tabpfn_extensions.many_class.many_class_classifier.ManyClassClassifier.fit","title":"fit","text":"<pre><code>fit(X, y, **fit_params) -&gt; ManyClassClassifier\n</code></pre> <p>Prepare classifier using custom validate_data. Actual fitting of sub-estimators happens in predict_proba if mapping is needed.</p>"},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/#tabpfn_extensions.many_class.many_class_classifier.ManyClassClassifier.predict","title":"predict","text":"<pre><code>predict(X) -&gt; ndarray\n</code></pre> <p>Predict multi-class targets for X.</p>"},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/#tabpfn_extensions.many_class.many_class_classifier.ManyClassClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X) -&gt; ndarray\n</code></pre> <p>Predict class probabilities for X. Sub-estimators are fitted here if mapping is used.</p>"},{"location":"reference/tabpfn_extensions/many_class/many_class_classifier/#tabpfn_extensions.many_class.many_class_classifier.ManyClassClassifier.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(\n    categorical_features: list[int],\n) -&gt; None\n</code></pre> <p>Attempts to set categorical features on the base estimator.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/","title":"Abstract validation utils","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/#tabpfn_extensions.post_hoc_ensembles.abstract_validation_utils","title":"abstract_validation_utils","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/#tabpfn_extensions.post_hoc_ensembles.abstract_validation_utils.AbstractValidationUtils","title":"AbstractValidationUtils","text":"<p>             Bases: <code>ABC</code>, <code>BaseEstimator</code></p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/#tabpfn_extensions.post_hoc_ensembles.abstract_validation_utils.AbstractValidationUtils.get_oof_per_estimator","title":"get_oof_per_estimator","text":"<pre><code>get_oof_per_estimator(\n    X: ndarray,\n    y: ndarray,\n    *,\n    return_loss_per_estimator: bool = False,\n    impute_dropped_instances: bool = True,\n    _extra_processing: bool = False\n) -&gt; list[ndarray] | tuple[list[ndarray], list[float]]\n</code></pre> <p>Get OOF predictions for each base model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>training data (features)</p> required <code>y</code> <code>ndarray</code> <p>training labels</p> required <code>return_loss_per_estimator</code> <code>bool</code> <p>if True, also return the loss per estimator.</p> <code>False</code> <code>impute_dropped_instances</code> <code>bool</code> <p>if True, impute instances that were dropped during the splits (e.g., due to not enough instances per class).</p> <code>True</code> <code>_extra_processing</code> <code>bool</code> <code>False</code> <p>either only OOF predictions or OOF predictions and loss per estimator.</p> Type Description <code>list[ndarray] | tuple[list[ndarray], list[float]]</code> <p>If self.is_holdout is True, the OOF predictions can return NaN values for instances not covered during repeated holdout.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/#tabpfn_extensions.post_hoc_ensembles.abstract_validation_utils.AbstractValidationUtils.not_enough_time","title":"not_enough_time","text":"<pre><code>not_enough_time(current_repeat: int) -&gt; bool\n</code></pre> <p>Simple heuristic to stop cross-validation early if not enough time is left for another repeat.</p> <p>Parameters:</p> Name Type Description Default <code>current_repeat</code> <code>int</code> <p>The current repeat index</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if there likely isn't enough time for another repeat, False otherwise</p> Note <p>This is a heuristic based on average time per repeat so far and may not be exact.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/#tabpfn_extensions.post_hoc_ensembles.abstract_validation_utils.AbstractValidationUtils.set_time_limit","title":"set_time_limit","text":"<pre><code>set_time_limit() -&gt; None\n</code></pre> <p>Initialize the timer for time-limited execution.</p> <p>Sets the start time for time limit tracking and logs the time limit info. This method should be called at the beginning of validation.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/abstract_validation_utils/#tabpfn_extensions.post_hoc_ensembles.abstract_validation_utils.AbstractValidationUtils.time_limit_reached","title":"time_limit_reached","text":"<pre><code>time_limit_reached() -&gt; bool\n</code></pre> <p>Check if the time limit for execution has been reached.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the time limit has been reached, False otherwise or if no time limit was set</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/","title":"Greedy weighted ensemble","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble","title":"greedy_weighted_ensemble","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsemble","title":"GreedyWeightedEnsemble","text":"<p>             Bases: <code>AbstractValidationUtils</code></p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsemble.get_oof_per_estimator","title":"get_oof_per_estimator","text":"<pre><code>get_oof_per_estimator(\n    X: ndarray,\n    y: ndarray,\n    *,\n    return_loss_per_estimator: bool = False,\n    impute_dropped_instances: bool = True,\n    _extra_processing: bool = False\n) -&gt; list[ndarray] | tuple[list[ndarray], list[float]]\n</code></pre> <p>Get OOF predictions for each base model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>training data (features)</p> required <code>y</code> <code>ndarray</code> <p>training labels</p> required <code>return_loss_per_estimator</code> <code>bool</code> <p>if True, also return the loss per estimator.</p> <code>False</code> <code>impute_dropped_instances</code> <code>bool</code> <p>if True, impute instances that were dropped during the splits (e.g., due to not enough instances per class).</p> <code>True</code> <code>_extra_processing</code> <code>bool</code> <code>False</code> <p>either only OOF predictions or OOF predictions and loss per estimator.</p> Type Description <code>list[ndarray] | tuple[list[ndarray], list[float]]</code> <p>If self.is_holdout is True, the OOF predictions can return NaN values for instances not covered during repeated holdout.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsemble.not_enough_time","title":"not_enough_time","text":"<pre><code>not_enough_time(current_repeat: int) -&gt; bool\n</code></pre> <p>Simple heuristic to stop cross-validation early if not enough time is left for another repeat.</p> <p>Parameters:</p> Name Type Description Default <code>current_repeat</code> <code>int</code> <p>The current repeat index</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if there likely isn't enough time for another repeat, False otherwise</p> Note <p>This is a heuristic based on average time per repeat so far and may not be exact.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsemble.set_time_limit","title":"set_time_limit","text":"<pre><code>set_time_limit() -&gt; None\n</code></pre> <p>Initialize the timer for time-limited execution.</p> <p>Sets the start time for time limit tracking and logs the time limit info. This method should be called at the beginning of validation.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsemble.time_limit_reached","title":"time_limit_reached","text":"<pre><code>time_limit_reached() -&gt; bool\n</code></pre> <p>Check if the time limit for execution has been reached.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the time limit has been reached, False otherwise or if no time limit was set</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleClassifier","title":"GreedyWeightedEnsembleClassifier","text":"<p>             Bases: <code>GreedyWeightedEnsemble</code>, <code>AbstractValidationUtilsClassification</code></p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleClassifier.get_oof_per_estimator","title":"get_oof_per_estimator","text":"<pre><code>get_oof_per_estimator(\n    X: ndarray,\n    y: ndarray,\n    *,\n    return_loss_per_estimator: bool = False,\n    impute_dropped_instances: bool = True,\n    _extra_processing: bool = False\n) -&gt; list[ndarray] | tuple[list[ndarray], list[float]]\n</code></pre> <p>Get OOF predictions for each base model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>training data (features)</p> required <code>y</code> <code>ndarray</code> <p>training labels</p> required <code>return_loss_per_estimator</code> <code>bool</code> <p>if True, also return the loss per estimator.</p> <code>False</code> <code>impute_dropped_instances</code> <code>bool</code> <p>if True, impute instances that were dropped during the splits (e.g., due to not enough instances per class).</p> <code>True</code> <code>_extra_processing</code> <code>bool</code> <code>False</code> <p>either only OOF predictions or OOF predictions and loss per estimator.</p> Type Description <code>list[ndarray] | tuple[list[ndarray], list[float]]</code> <p>If self.is_holdout is True, the OOF predictions can return NaN values for instances not covered during repeated holdout.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleClassifier.not_enough_time","title":"not_enough_time","text":"<pre><code>not_enough_time(current_repeat: int) -&gt; bool\n</code></pre> <p>Simple heuristic to stop cross-validation early if not enough time is left for another repeat.</p> <p>Parameters:</p> Name Type Description Default <code>current_repeat</code> <code>int</code> <p>The current repeat index</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if there likely isn't enough time for another repeat, False otherwise</p> Note <p>This is a heuristic based on average time per repeat so far and may not be exact.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleClassifier.set_time_limit","title":"set_time_limit","text":"<pre><code>set_time_limit() -&gt; None\n</code></pre> <p>Initialize the timer for time-limited execution.</p> <p>Sets the start time for time limit tracking and logs the time limit info. This method should be called at the beginning of validation.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleClassifier.time_limit_reached","title":"time_limit_reached","text":"<pre><code>time_limit_reached() -&gt; bool\n</code></pre> <p>Check if the time limit for execution has been reached.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the time limit has been reached, False otherwise or if no time limit was set</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleRegressor","title":"GreedyWeightedEnsembleRegressor","text":"<p>             Bases: <code>GreedyWeightedEnsemble</code>, <code>AbstractValidationUtilsRegression</code></p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleRegressor.get_oof_per_estimator","title":"get_oof_per_estimator","text":"<pre><code>get_oof_per_estimator(\n    X: ndarray,\n    y: ndarray,\n    *,\n    return_loss_per_estimator: bool = False,\n    impute_dropped_instances: bool = True,\n    _extra_processing: bool = False\n) -&gt; list[ndarray] | tuple[list[ndarray], list[float]]\n</code></pre> <p>Get OOF predictions for each base model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>training data (features)</p> required <code>y</code> <code>ndarray</code> <p>training labels</p> required <code>return_loss_per_estimator</code> <code>bool</code> <p>if True, also return the loss per estimator.</p> <code>False</code> <code>impute_dropped_instances</code> <code>bool</code> <p>if True, impute instances that were dropped during the splits (e.g., due to not enough instances per class).</p> <code>True</code> <code>_extra_processing</code> <code>bool</code> <code>False</code> <p>either only OOF predictions or OOF predictions and loss per estimator.</p> Type Description <code>list[ndarray] | tuple[list[ndarray], list[float]]</code> <p>If self.is_holdout is True, the OOF predictions can return NaN values for instances not covered during repeated holdout.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleRegressor.not_enough_time","title":"not_enough_time","text":"<pre><code>not_enough_time(current_repeat: int) -&gt; bool\n</code></pre> <p>Simple heuristic to stop cross-validation early if not enough time is left for another repeat.</p> <p>Parameters:</p> Name Type Description Default <code>current_repeat</code> <code>int</code> <p>The current repeat index</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if there likely isn't enough time for another repeat, False otherwise</p> Note <p>This is a heuristic based on average time per repeat so far and may not be exact.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleRegressor.set_time_limit","title":"set_time_limit","text":"<pre><code>set_time_limit() -&gt; None\n</code></pre> <p>Initialize the timer for time-limited execution.</p> <p>Sets the start time for time limit tracking and logs the time limit info. This method should be called at the beginning of validation.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.GreedyWeightedEnsembleRegressor.time_limit_reached","title":"time_limit_reached","text":"<pre><code>time_limit_reached() -&gt; bool\n</code></pre> <p>Check if the time limit for execution has been reached.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the time limit has been reached, False otherwise or if no time limit was set</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/greedy_weighted_ensemble/#tabpfn_extensions.post_hoc_ensembles.greedy_weighted_ensemble.caruana_weighted","title":"caruana_weighted","text":"<pre><code>caruana_weighted(\n    predictions: list[ndarray],\n    labels: ndarray,\n    seed,\n    n_iterations,\n    loss_function,\n)\n</code></pre> <p>Caruana's ensemble selection with replacement.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/","title":"Pfn phe","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/#tabpfn_extensions.post_hoc_ensembles.pfn_phe","title":"pfn_phe","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/#tabpfn_extensions.post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor","title":"AutoPostHocEnsemblePredictor","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>A wrapper to effectively performing post hoc ensemble with TabPFN models.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/#tabpfn_extensions.post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray,\n    y: ndarray,\n    categorical_feature_indices: list[int] | None = None,\n) -&gt; AutoPostHocEnsemblePredictor\n</code></pre> <p>Fits the post hoc ensemble on the given data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data to fit the ensemble on.</p> required <code>y</code> <code>ndarray</code> <p>The target values to fit the ensemble on.</p> required <code>categorical_feature_indices</code> <code>list[int] | None</code> <p>The indices of the categorical features in the data. If None, no categorical features are assumed to be present.</p> <code>None</code>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/#tabpfn_extensions.post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; ndarray\n</code></pre> <p>Predicts the target values for the given data.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/#tabpfn_extensions.post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X: ndarray) -&gt; ndarray\n</code></pre> <p>Predicts the target values for the given data.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/pfn_phe/#tabpfn_extensions.post_hoc_ensembles.pfn_phe.TabPFNPostHocEnsemble","title":"TabPFNPostHocEnsemble","text":"<p>Simple wrapper around AutoTabPFNClassifier for backward compatibility.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/save_splitting/","title":"Save splitting","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/save_splitting/#tabpfn_extensions.post_hoc_ensembles.save_splitting","title":"save_splitting","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/save_splitting/#tabpfn_extensions.post_hoc_ensembles.save_splitting.assert_valid_splits","title":"assert_valid_splits","text":"<pre><code>assert_valid_splits(\n    splits: list[list[list[int], list[int]]],\n    y: ndarray,\n    *,\n    non_empty: bool = True,\n    each_selected_class_in_each_split_subset: bool = True,\n    same_length_training_splits: bool = True\n)\n</code></pre> <p>Verify that the splits are valid.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/save_splitting/#tabpfn_extensions.post_hoc_ensembles.save_splitting.fix_split_by_dropping_classes","title":"fix_split_by_dropping_classes","text":"<pre><code>fix_split_by_dropping_classes(\n    x: ndarray,\n    y: ndarray,\n    n_splits: int,\n    spliter_kwargs: dict,\n) -&gt; list[list[list[int], list[int]]]\n</code></pre> <p>Fixes stratifed splits for edge case.</p> <p>For each class that has fewer instances than number of splits, we oversample before split to n_splits and then remove all oversamples and original samples from the splits; effectively removing the class from the data without touching the indices.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/save_splitting/#tabpfn_extensions.post_hoc_ensembles.save_splitting.get_cv_split_for_data","title":"get_cv_split_for_data","text":"<pre><code>get_cv_split_for_data(\n    x: ndarray,\n    y: ndarray,\n    splits_seed: int,\n    n_splits: int,\n    *,\n    stratified_split: bool,\n    safety_shuffle: bool = True,\n    auto_fix_stratified_splits: bool = False,\n    force_same_length_training_splits: bool = False\n) -&gt; list[list[list[int], list[int]]] | str\n</code></pre> <p>Safety shuffle and generate (safe) splits.</p> <p>If it returns str at the first entry, no valid split could be generated and the str is the reason why. Due to the safety shuffle, the original x and y are also returned and must be used.</p> <p>Note: the function does not support repeated splits at this point. Simply call this function multiple times with different seeds to get repeated splits.</p> <p>Test with:</p> <pre><code>    if __name__ == \"__main__\":\n    print(\n        get_cv_split_for_data(\n            x=np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]).T,\n            y=np.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4]),\n            splits_seed=42,\n            n_splits=3,\n            stratified_split=True,\n            auto_fix_stratified_splits=True,\n        )\n    )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The data to split.</p> required <code>y</code> <code>ndarray</code> <p>The labels to split.</p> required <code>splits_seed</code> <code>int</code> <p>The seed to use for the splits. Or a RandomState object.</p> required <code>n_splits</code> <code>int</code> <p>The number of splits to generate.</p> required <code>stratified_split</code> <code>bool</code> <p>Whether to use stratified splits.</p> required <code>safety_shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting.</p> <code>True</code> <code>auto_fix_stratified_splits</code> <code>bool</code> <p>Whether to try to fix stratified splits automatically. Fix by dropping classes with less than n_splits samples.</p> <code>False</code> <code>force_same_length_training_splits</code> <code>bool</code> <p>Whether to force the training splits to have the same amount of samples. Force by duplicating random instance in the training subset of a too small split until all training splits have the same amount of samples.</p> <code>False</code>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/","title":"Sklearn interface","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/#tabpfn_extensions.post_hoc_ensembles.sklearn_interface","title":"sklearn_interface","text":""},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/#tabpfn_extensions.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier","title":"AutoTabPFNClassifier","text":"<p>             Bases: <code>ClassifierMixin</code>, <code>BaseEstimator</code></p> <p>Automatic Post Hoc Ensemble Classifier for TabPFN models.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/#tabpfn_extensions.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier--parameters","title":"Parameters","text":"<pre><code>max_time : int | None, default=None\n    The maximum time to spend on fitting the post hoc ensemble.\npreset: {\"default\", \"custom_hps\", \"avoid_overfitting\"}, default=\"default\"\n    The preset to use for the post hoc ensemble.\nges_scoring_string : str, default=\"roc\"\n    The scoring string to use for the greedy ensemble search.\n    Allowed values are: {\"accuracy\", \"roc\" / \"auroc\", \"f1\", \"log_loss\"}.\ndevice : {\"cpu\", \"cuda\"}, default=\"auto\"\n    The device to use for training and prediction.\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness base models and the post hoc ensembling method.\ncategorical_feature_indices: list[int] or None, default=None\n    The indices of the categorical features in the input data. Can also be passed to `fit()`.\nignore_pretraining_limits: bool, default=False\n    Whether to ignore the pretraining limits of the TabPFN base models.\nphe_init_args : dict | None, default=None\n    The initialization arguments for the post hoc ensemble predictor.\n    See post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more options and all details.\n</code></pre> <pre><code>predictor_ : AutoPostHocEnsemblePredictor\n    The predictor interface used to make predictions, see post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more.\nphe_init_args_ : dict\n    The optional initialization arguments used for the post hoc ensemble predictor.\n</code></pre>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/#tabpfn_extensions.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor","title":"AutoTabPFNRegressor","text":"<p>             Bases: <code>RegressorMixin</code>, <code>BaseEstimator</code></p> <p>Automatic Post Hoc Ensemble Regressor for TabPFN models.</p>"},{"location":"reference/tabpfn_extensions/post_hoc_ensembles/sklearn_interface/#tabpfn_extensions.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor--parameters","title":"Parameters","text":"<pre><code>max_time : int | None, default=None\n    The maximum time to spend on fitting the post hoc ensemble.\npreset: {\"default\", \"custom_hps\", \"avoid_overfitting\"}, default=\"default\"\n    The preset to use for the post hoc ensemble.\nges_scoring_string : str, default=\"mse\"\n    The scoring string to use for the greedy ensemble search.\n    Allowed values are: {\"rmse\", \"mse\", \"mae\"}.\ndevice : {\"cpu\", \"cuda\"}, default=\"cuda\"\n    The device to use for training and prediction.\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness base models and the post hoc ensembling method.\ncategorical_feature_indices: list[int] or None, default=None\n    The indices of the categorical features in the input data. Can also be passed to `fit()`.\nignore_pretraining_limits: bool, default=False\n    Whether to ignore the pretraining limits of the TabPFN base models.\nphe_init_args : dict | None, default=None\n    The initialization arguments for the post hoc ensemble predictor.\n    See post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more options and all details.\n</code></pre> <pre><code>predictor_ : AutoPostHocEnsemblePredictor\n    The predictor interface used to make predictions, see post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more.\nphe_init_args_ : dict\n    The optional initialization arguments used for the post hoc ensemble predictor.\n</code></pre>"},{"location":"reference/tabpfn_extensions/rf_pfn/configs/","title":"Configs","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/configs/#tabpfn_extensions.rf_pfn.configs","title":"configs","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/","title":"Sklearn based decision tree tabpfn","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn","title":"sklearn_based_decision_tree_tabpfn","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNBase","title":"DecisionTreeTabPFNBase","text":"<p>             Bases: <code>BaseDecisionTree</code>, <code>BaseEstimator</code></p> <p>Abstract base class combining a scikit-learn Decision Tree with TabPFN at the leaves.</p> <p>This class provides a hybrid approach by combining the standard decision tree splitting algorithm from scikit-learn with TabPFN models at the leaves or internal nodes. This allows for both interpretable tree-based partitioning and high-performance TabPFN prediction.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNBase--key-features","title":"Key features:","text":"<p>\u2022 Inherits from sklearn's BaseDecisionTree to leverage standard tree splitting algorithms \u2022 Uses TabPFN (Classifier or Regressor) to fit leaf nodes (or all internal nodes) \u2022 Provides adaptive pruning logic (optional) that dynamically determines optimal tree depth \u2022 Supports both classification and regression through specialized subclasses</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNBase--subclasses","title":"Subclasses:","text":"<p>\u2022 DecisionTreeTabPFNClassifier - for classification tasks \u2022 DecisionTreeTabPFNRegressor - for regression tasks</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNBase--parameters","title":"Parameters","text":"<p>tabpfn : Any     A TabPFN instance (TabPFNClassifier or TabPFNRegressor) that will be used at tree nodes. criterion : str     The function to measure the quality of a split (from sklearn). splitter : str     The strategy used to choose the split at each node (e.g. \"best\" or \"random\"). max_depth : int, optional     The maximum depth of the tree (None means unlimited). min_samples_split : int     The minimum number of samples required to split an internal node. min_samples_leaf : int     The minimum number of samples required to be at a leaf node. min_weight_fraction_leaf : float     The minimum weighted fraction of the sum total of weights required to be at a leaf node. max_features : Union[int, float, str, None]     The number of features to consider when looking for the best split. random_state : Union[int, np.random.RandomState, None]     Controls the randomness of the estimator. max_leaf_nodes : Optional[int]     If not None, grow a tree with max_leaf_nodes in best-first fashion. min_impurity_decrease : float     A node will be split if this split induces a decrease of the impurity &gt;= this value. class_weight : Optional[Union[Dict[int, float], str]]     Only used in classification. Dict of class -&gt; weight or \u201cbalanced\u201d. ccp_alpha : float     Complexity parameter used for Minimal Cost-Complexity Pruning (non-negative). monotonic_cst : Any     Optional monotonicity constraints (depending on sklearn version). categorical_features : Optional[List[int]]     Indices of categorical features for TabPFN usage (if any). verbose : Union[bool, int]     Verbosity level; higher values produce more output. show_progress : bool     Whether to show progress bars for leaf/node fitting using TabPFN. fit_nodes : bool     Whether to fit TabPFN at internal nodes (True) or only final leaves (False). tree_seed : int     Used to set seeds for TabPFN fitting in each node. adaptive_tree : bool     Whether to do adaptive node-by-node pruning using a hold-out strategy. adaptive_tree_min_train_samples : int     Minimum number of training samples required to fit a TabPFN in a node. adaptive_tree_max_train_samples : int     Maximum number of training samples above which a node might be pruned if not a final leaf. adaptive_tree_min_valid_samples_fraction_of_train : float     Fraction controlling the minimum valid/test points to consider a node for re-fitting. adaptive_tree_overwrite_metric : Optional[str]     If set, overrides the default metric for pruning. E.g., \"roc\" or \"rmse\". adaptive_tree_test_size : float     Fraction of data to hold out for adaptive pruning if no separate valid set is provided. average_logits : bool     Whether to average logits (True) or probabilities (False) when combining predictions. adaptive_tree_skip_class_missing : bool     If True, skip re-fitting if the nodes training set does not contain all classes (classification only).</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNBase.tree_","title":"tree_  <code>property</code>","text":"<pre><code>tree_\n</code></pre> <p>Expose the fitted tree for sklearn compatibility.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNBase.tree_--returns","title":"Returns:","text":"<p>sklearn.tree._tree.Tree     Underlying scikit-learn tree object.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNBase.fit","title":"fit","text":"<pre><code>fit(\n    X: NDArray[float64],\n    y: NDArray[Any],\n    sample_weight: NDArray[float64] | None = None,\n    check_input: bool = True,\n) -&gt; DecisionTreeTabPFNBase\n</code></pre> <p>Fit the DecisionTree + TabPFN model.</p> <p>This method trains the hybrid model by: 1. Building a decision tree structure 2. Fitting TabPFN models at the leaves (or at all nodes if fit_nodes=True) 3. Optionally performing adaptive pruning if adaptive_tree=True</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>NDArray[float64]</code> <p>The training input samples, shape (n_samples, n_features).</p> required <code>y</code> <code>NDArray[Any]</code> <p>The target values (class labels for classification, real values for regression), shape (n_samples,) or (n_samples, n_outputs).</p> required <code>sample_weight</code> <code>NDArray[float64] | None</code> <p>Sample weights. If None, then samples are equally weighted.</p> <code>None</code> <code>check_input</code> <code>bool</code> <p>Whether to validate the input data arrays. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>self</code> <code>DecisionTreeTabPFNBase</code> <p>Fitted estimator.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNBase.fit_leaves","title":"fit_leaves","text":"<pre><code>fit_leaves(train_X: ndarray, train_y: ndarray) -&gt; None\n</code></pre> <p>Fit a TabPFN model in each leaf node (or each node, if self.fit_nodes=True).</p> <p>This populates an internal dictionary of training data for each leaf so that TabPFN can make predictions at these leaves.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNBase.fit_leaves--parameters","title":"Parameters","text":"<p>train_X : np.ndarray     Training features for all samples. train_y : np.ndarray     Training labels/targets for all samples.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNBase.get_tree","title":"get_tree","text":"<pre><code>get_tree() -&gt; BaseDecisionTree\n</code></pre> <p>Return the underlying fitted sklearn decision tree.</p> <p>Returns:</p> Type Description <code>BaseDecisionTree</code> <p>DecisionTreeClassifier or DecisionTreeRegressor: The fitted decision tree.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If the model has not been fitted yet.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNClassifier","title":"DecisionTreeTabPFNClassifier","text":"<p>             Bases: <code>DecisionTreeTabPFNBase</code>, <code>ClassifierMixin</code></p> <p>Decision tree that uses TabPFNClassifier at the leaves.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNClassifier.tree_","title":"tree_  <code>property</code>","text":"<pre><code>tree_\n</code></pre> <p>Expose the fitted tree for sklearn compatibility.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNClassifier.tree_--returns","title":"Returns:","text":"<p>sklearn.tree._tree.Tree     Underlying scikit-learn tree object.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNClassifier.fit","title":"fit","text":"<pre><code>fit(\n    X: NDArray[float64],\n    y: NDArray[Any],\n    sample_weight: NDArray[float64] | None = None,\n    check_input: bool = True,\n) -&gt; DecisionTreeTabPFNBase\n</code></pre> <p>Fit the DecisionTree + TabPFN model.</p> <p>This method trains the hybrid model by: 1. Building a decision tree structure 2. Fitting TabPFN models at the leaves (or at all nodes if fit_nodes=True) 3. Optionally performing adaptive pruning if adaptive_tree=True</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>NDArray[float64]</code> <p>The training input samples, shape (n_samples, n_features).</p> required <code>y</code> <code>NDArray[Any]</code> <p>The target values (class labels for classification, real values for regression), shape (n_samples,) or (n_samples, n_outputs).</p> required <code>sample_weight</code> <code>NDArray[float64] | None</code> <p>Sample weights. If None, then samples are equally weighted.</p> <code>None</code> <code>check_input</code> <code>bool</code> <p>Whether to validate the input data arrays. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>self</code> <code>DecisionTreeTabPFNBase</code> <p>Fitted estimator.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNClassifier.fit_leaves","title":"fit_leaves","text":"<pre><code>fit_leaves(train_X: ndarray, train_y: ndarray) -&gt; None\n</code></pre> <p>Fit a TabPFN model in each leaf node (or each node, if self.fit_nodes=True).</p> <p>This populates an internal dictionary of training data for each leaf so that TabPFN can make predictions at these leaves.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNClassifier.fit_leaves--parameters","title":"Parameters","text":"<p>train_X : np.ndarray     Training features for all samples. train_y : np.ndarray     Training labels/targets for all samples.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNClassifier.get_tree","title":"get_tree","text":"<pre><code>get_tree() -&gt; BaseDecisionTree\n</code></pre> <p>Return the underlying fitted sklearn decision tree.</p> <p>Returns:</p> Type Description <code>BaseDecisionTree</code> <p>DecisionTreeClassifier or DecisionTreeRegressor: The fitted decision tree.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If the model has not been fitted yet.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X: ndarray, check_input: bool = True) -&gt; ndarray\n</code></pre> <p>Predict class labels for X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input features.</p> required <code>check_input</code> <code>bool</code> <p>Whether to validate input arrays. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predicted class labels.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(\n    X: ndarray, check_input: bool = True\n) -&gt; ndarray\n</code></pre> <p>Predict class probabilities for X using the TabPFN leaves.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input features.</p> required <code>check_input</code> <code>bool</code> <p>Whether to validate input arrays. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predicted probabilities of shape (n_samples, n_classes).</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor","title":"DecisionTreeTabPFNRegressor","text":"<p>             Bases: <code>DecisionTreeTabPFNBase</code>, <code>RegressorMixin</code></p> <p>Decision tree that uses TabPFNRegressor at the leaves.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.tree_","title":"tree_  <code>property</code>","text":"<pre><code>tree_\n</code></pre> <p>Expose the fitted tree for sklearn compatibility.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.tree_--returns","title":"Returns:","text":"<p>sklearn.tree._tree.Tree     Underlying scikit-learn tree object.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.fit","title":"fit","text":"<pre><code>fit(\n    X: NDArray[float64],\n    y: NDArray[Any],\n    sample_weight: NDArray[float64] | None = None,\n    check_input: bool = True,\n) -&gt; DecisionTreeTabPFNBase\n</code></pre> <p>Fit the DecisionTree + TabPFN model.</p> <p>This method trains the hybrid model by: 1. Building a decision tree structure 2. Fitting TabPFN models at the leaves (or at all nodes if fit_nodes=True) 3. Optionally performing adaptive pruning if adaptive_tree=True</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>NDArray[float64]</code> <p>The training input samples, shape (n_samples, n_features).</p> required <code>y</code> <code>NDArray[Any]</code> <p>The target values (class labels for classification, real values for regression), shape (n_samples,) or (n_samples, n_outputs).</p> required <code>sample_weight</code> <code>NDArray[float64] | None</code> <p>Sample weights. If None, then samples are equally weighted.</p> <code>None</code> <code>check_input</code> <code>bool</code> <p>Whether to validate the input data arrays. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>self</code> <code>DecisionTreeTabPFNBase</code> <p>Fitted estimator.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.fit_leaves","title":"fit_leaves","text":"<pre><code>fit_leaves(train_X: ndarray, train_y: ndarray) -&gt; None\n</code></pre> <p>Fit a TabPFN model in each leaf node (or each node, if self.fit_nodes=True).</p> <p>This populates an internal dictionary of training data for each leaf so that TabPFN can make predictions at these leaves.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.fit_leaves--parameters","title":"Parameters","text":"<p>train_X : np.ndarray     Training features for all samples. train_y : np.ndarray     Training labels/targets for all samples.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.get_tree","title":"get_tree","text":"<pre><code>get_tree() -&gt; BaseDecisionTree\n</code></pre> <p>Return the underlying fitted sklearn decision tree.</p> <p>Returns:</p> Type Description <code>BaseDecisionTree</code> <p>DecisionTreeClassifier or DecisionTreeRegressor: The fitted decision tree.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If the model has not been fitted yet.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(X: ndarray, check_input: bool = True) -&gt; ndarray\n</code></pre> <p>Predict regression values using the TabPFN leaves.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.predict--parameters","title":"Parameters","text":"<p>X : np.ndarray     Input features. check_input : bool, default=True     Whether to validate the input arrays.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.predict--returns","title":"Returns:","text":"<p>np.ndarray     Continuous predictions of shape (n_samples,).</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.predict_full","title":"predict_full","text":"<pre><code>predict_full(X: ndarray) -&gt; ndarray\n</code></pre> <p>Convenience method to predict with no input checks (optional).</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.predict_full--parameters","title":"Parameters","text":"<p>X : np.ndarray     Input features.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_decision_tree_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_decision_tree_tabpfn.DecisionTreeTabPFNRegressor.predict_full--returns","title":"Returns:","text":"<p>np.ndarray     Continuous predictions of shape (n_samples,).</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/","title":"Sklearn based random forest tabpfn","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn","title":"sklearn_based_random_forest_tabpfn","text":"<p>Random Forest implementation that uses TabPFN at the leaf nodes.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNBase","title":"RandomForestTabPFNBase","text":"<p>Base Class for common functionalities.</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNBase.fit","title":"fit","text":"<pre><code>fit(X: ndarray, y: ndarray, sample_weight: ndarray = None)\n</code></pre> <p>Fits RandomForestTabPFN.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature training data</p> required <code>y</code> <code>ndarray</code> <p>Label training data</p> required <code>sample_weight</code> <code>ndarray</code> <p>Weights of each sample</p> <code>None</code> <p>Returns:</p> Type Description <p>Fitted model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n_estimators is not positive</p> <code>ValueError</code> <p>If tabpfn is None</p> <code>TypeError</code> <p>If tabpfn is not of the expected type</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNBase.get_n_estimators","title":"get_n_estimators","text":"<pre><code>get_n_estimators(X: ndarray) -&gt; int\n</code></pre> <p>Get the number of estimators to use.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input features</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of estimators</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNClassifier","title":"RandomForestTabPFNClassifier","text":"<p>             Bases: <code>RandomForestTabPFNBase</code>, <code>RandomForestClassifier</code></p> <p>RandomForestTabPFNClassifier implements Random Forest using TabPFN at leaf nodes.</p> <p>This classifier combines decision trees with TabPFN models at the leaf nodes for improved performance on tabular data. It extends scikit-learn's RandomForestClassifier with TabPFN's neural network capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>tabpfn</code> <p>TabPFNClassifier instance to use at leaf nodes</p> <code>None</code> <code>n_jobs</code> <p>Number of parallel jobs</p> <code>1</code> <code>categorical_features</code> <p>List of categorical feature indices</p> <code>None</code> <code>show_progress</code> <p>Whether to display progress during fitting</p> <code>False</code> <code>verbose</code> <p>Verbosity level (0=quiet, &gt;0=verbose)</p> <code>0</code> <code>adaptive_tree</code> <p>Whether to use adaptive tree-based method</p> <code>True</code> <code>fit_nodes</code> <p>Whether to fit the leaf node models</p> <code>True</code> <code>adaptive_tree_overwrite_metric</code> <p>Metric used for adaptive node fitting</p> <code>'log_loss'</code> <code>adaptive_tree_test_size</code> <p>Test size for adaptive node fitting</p> <code>0.2</code> <code>adaptive_tree_min_train_samples</code> <p>Minimum samples for training leaf nodes</p> <code>100</code> <code>adaptive_tree_max_train_samples</code> <p>Maximum samples for training leaf nodes</p> <code>5000</code> <code>adaptive_tree_min_valid_samples_fraction_of_train</code> <p>Min fraction of validation samples</p> <code>0.2</code> <code>preprocess_X_once</code> <p>Whether to preprocess X only once</p> <code>False</code> <code>max_predict_time</code> <p>Maximum time allowed for prediction (seconds)</p> <code>60</code> <code>rf_average_logits</code> <p>Whether to average logits instead of probabilities</p> <code>True</code> <code>dt_average_logits</code> <p>Whether to average logits in decision trees</p> <code>True</code> <code>adaptive_tree_skip_class_missing</code> <p>Whether to skip classes missing in nodes</p> <code>True</code> <code>n_estimators</code> <p>Number of trees in the forest</p> <code>100</code> <code>criterion</code> <p>Function to measure split quality</p> <code>'gini'</code> <code>max_depth</code> <p>Maximum depth of the trees</p> <code>5</code> <code>min_samples_split</code> <p>Minimum samples required to split a node</p> <code>1000</code> <code>min_samples_leaf</code> <p>Minimum samples required at a leaf node</p> <code>5</code> <code>min_weight_fraction_leaf</code> <p>Minimum weighted fraction of sum total</p> <code>0.0</code> <code>max_features</code> <p>Number of features to consider for best split</p> <code>'sqrt'</code> <code>max_leaf_nodes</code> <p>Maximum number of leaf nodes</p> <code>None</code> <code>min_impurity_decrease</code> <p>Minimum impurity decrease required for split</p> <code>0.0</code> <code>bootstrap</code> <p>Whether to use bootstrap samples</p> <code>True</code> <code>oob_score</code> <p>Whether to use out-of-bag samples</p> <code>False</code> <code>random_state</code> <p>Controls randomness of the estimator</p> <code>None</code> <code>warm_start</code> <p>Whether to reuse previous solution</p> <code>False</code> <code>class_weight</code> <p>Weights associated with classes</p> <code>None</code> <code>ccp_alpha</code> <p>Complexity parameter for minimal cost-complexity pruning</p> <code>0.0</code> <code>max_samples</code> <p>Number of samples to draw to train each tree</p> <code>None</code>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNClassifier.fit","title":"fit","text":"<pre><code>fit(X: ndarray, y: ndarray, sample_weight: ndarray = None)\n</code></pre> <p>Fits RandomForestTabPFN.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature training data</p> required <code>y</code> <code>ndarray</code> <p>Label training data</p> required <code>sample_weight</code> <code>ndarray</code> <p>Weights of each sample</p> <code>None</code> <p>Returns:</p> Type Description <p>Fitted model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n_estimators is not positive</p> <code>ValueError</code> <p>If tabpfn is None</p> <code>TypeError</code> <p>If tabpfn is not of the expected type</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNClassifier.get_n_estimators","title":"get_n_estimators","text":"<pre><code>get_n_estimators(X: ndarray) -&gt; int\n</code></pre> <p>Get the number of estimators to use.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input features</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of estimators</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNClassifier.init_base_estimator","title":"init_base_estimator","text":"<pre><code>init_base_estimator()\n</code></pre> <p>Initialize a base decision tree estimator.</p> <p>Returns:</p> Type Description <p>A new DecisionTreeTabPFNClassifier instance</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; ndarray\n</code></pre> <p>Predict class for X.</p> <p>The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>{array-like, sparse matrix} of shape (n_samples, n_features) The input samples.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>ndarray of shape (n_samples,) The predicted classes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model is not fitted</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X: ndarray) -&gt; ndarray\n</code></pre> <p>Predict class probabilities for X.</p> <p>The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>{array-like, sparse matrix} of shape (n_samples, n_features) The input samples.</p> required <p>Returns:</p> Name Type Description <code>p</code> <code>ndarray</code> <p>ndarray of shape (n_samples, n_classes) The class probabilities of the input samples.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model is not fitted</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNRegressor","title":"RandomForestTabPFNRegressor","text":"<p>             Bases: <code>RandomForestTabPFNBase</code>, <code>RandomForestRegressor</code></p> <p>RandomForestTabPFNRegressor implements a Random Forest using TabPFN at leaf nodes.</p> <p>This regressor combines decision trees with TabPFN models at the leaf nodes for improved regression performance on tabular data. It extends scikit-learn's RandomForestRegressor with TabPFN's neural network capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>tabpfn</code> <p>TabPFNRegressor instance to use at leaf nodes</p> <code>None</code> <code>n_jobs</code> <p>Number of parallel jobs</p> <code>1</code> <code>categorical_features</code> <p>List of categorical feature indices</p> <code>None</code> <code>show_progress</code> <p>Whether to display progress during fitting</p> <code>False</code> <code>verbose</code> <p>Verbosity level (0=quiet, &gt;0=verbose)</p> <code>0</code> <code>adaptive_tree</code> <p>Whether to use adaptive tree-based method</p> <code>True</code> <code>fit_nodes</code> <p>Whether to fit the leaf node models</p> <code>True</code> <code>adaptive_tree_overwrite_metric</code> <p>Metric used for adaptive node fitting</p> <code>'rmse'</code> <code>adaptive_tree_test_size</code> <p>Test size for adaptive node fitting</p> <code>0.2</code> <code>adaptive_tree_min_train_samples</code> <p>Minimum samples for training leaf nodes</p> <code>100</code> <code>adaptive_tree_max_train_samples</code> <p>Maximum samples for training leaf nodes</p> <code>5000</code> <code>adaptive_tree_min_valid_samples_fraction_of_train</code> <p>Min fraction of validation samples</p> <code>0.2</code> <code>preprocess_X_once</code> <p>Whether to preprocess X only once</p> <code>False</code> <code>max_predict_time</code> <p>Maximum time allowed for prediction (seconds)</p> <code>-1</code> <code>rf_average_logits</code> <p>Whether to average logits instead of raw predictions</p> <code>False</code> <code>n_estimators</code> <p>Number of trees in the forest</p> <code>16</code> <code>criterion</code> <p>Function to measure split quality</p> <code>'friedman_mse'</code> <code>max_depth</code> <p>Maximum depth of the trees</p> <code>5</code> <code>min_samples_split</code> <p>Minimum samples required to split a node</p> <code>300</code> <code>min_samples_leaf</code> <p>Minimum samples required at a leaf node</p> <code>5</code> <code>min_weight_fraction_leaf</code> <p>Minimum weighted fraction of sum total</p> <code>0.0</code> <code>max_features</code> <p>Number of features to consider for best split</p> <code>'sqrt'</code> <code>max_leaf_nodes</code> <p>Maximum number of leaf nodes</p> <code>None</code> <code>min_impurity_decrease</code> <p>Minimum impurity decrease required for split</p> <code>0.0</code> <code>bootstrap</code> <p>Whether to use bootstrap samples</p> <code>True</code> <code>oob_score</code> <p>Whether to use out-of-bag samples</p> <code>False</code> <code>random_state</code> <p>Controls randomness of the estimator</p> <code>None</code> <code>warm_start</code> <p>Whether to reuse previous solution</p> <code>False</code> <code>ccp_alpha</code> <p>Complexity parameter for minimal cost-complexity pruning</p> <code>0.0</code> <code>max_samples</code> <p>Number of samples to draw to train each tree</p> <code>None</code>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNRegressor.fit","title":"fit","text":"<pre><code>fit(X: ndarray, y: ndarray, sample_weight: ndarray = None)\n</code></pre> <p>Fits RandomForestTabPFN.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature training data</p> required <code>y</code> <code>ndarray</code> <p>Label training data</p> required <code>sample_weight</code> <code>ndarray</code> <p>Weights of each sample</p> <code>None</code> <p>Returns:</p> Type Description <p>Fitted model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n_estimators is not positive</p> <code>ValueError</code> <p>If tabpfn is None</p> <code>TypeError</code> <p>If tabpfn is not of the expected type</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNRegressor.get_n_estimators","title":"get_n_estimators","text":"<pre><code>get_n_estimators(X: ndarray) -&gt; int\n</code></pre> <p>Get the number of estimators to use.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input features</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of estimators</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNRegressor.init_base_estimator","title":"init_base_estimator","text":"<pre><code>init_base_estimator()\n</code></pre> <p>Initialize a base decision tree estimator.</p> <p>Returns:</p> Type Description <p>A new DecisionTreeTabPFNRegressor instance</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.RandomForestTabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; ndarray\n</code></pre> <p>Predict regression target for X.</p> <p>The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>{array-like, sparse matrix} of shape (n_samples, n_features) The input samples.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray</code> <p>ndarray of shape (n_samples,) or (n_samples, n_outputs) The predicted values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model is not fitted</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/sklearn_based_random_forest_tabpfn/#tabpfn_extensions.rf_pfn.sklearn_based_random_forest_tabpfn.softmax_numpy","title":"softmax_numpy","text":"<pre><code>softmax_numpy(logits: ndarray) -&gt; ndarray\n</code></pre> <p>Apply softmax to numpy array of logits.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>ndarray</code> <p>Input logits array</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Probabilities after softmax</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/utils/","title":"Utils","text":""},{"location":"reference/tabpfn_extensions/rf_pfn/utils/#tabpfn_extensions.rf_pfn.utils","title":"utils","text":"<p>Copyright 2023.</p> <p>Author: Lukas Schweizer schweizer.lukas@web.de</p>"},{"location":"reference/tabpfn_extensions/rf_pfn/utils/#tabpfn_extensions.rf_pfn.utils.preprocess_data","title":"preprocess_data","text":"<pre><code>preprocess_data(\n    data,\n    nan_values=True,\n    one_hot_encoding=False,\n    normalization=True,\n    categorical_indices=None,\n)\n</code></pre> <p>This method preprocesses data regarding missing values, categorical features and data normalization (for the kNN Model) :param data: Data to preprocess :param nan_values: Preprocesses nan values if True :param one_hot_encoding: Whether use OHE for categoricals :param normalization: Normalizes data if True :param categorical_indices: Categorical columns of data :return: Preprocessed version of the data.</p>"},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/","title":"Scoring utils","text":""},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/#tabpfn_extensions.scoring.scoring_utils","title":"scoring_utils","text":""},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/#tabpfn_extensions.scoring.scoring_utils.safe_roc_auc_score","title":"safe_roc_auc_score","text":"<pre><code>safe_roc_auc_score(y_true, y_score, **kwargs)\n</code></pre> <p>Compute the Area Under the Receiver Operating Characteristic Curve (ROC AUC) score.</p> <p>This function is a safe wrapper around <code>sklearn.metrics.roc_auc_score</code> that handles cases where the input data may have missing classes or binary classification problems.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>array-like of shape (n_samples,) True binary labels or binary label indicators.</p> required <code>y_score</code> <p>array-like of shape (n_samples,) or (n_samples, n_classes) Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.</p> required <code>**kwargs</code> <p>dict Additional keyword arguments to pass to <code>sklearn.metrics.roc_auc_score</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The ROC AUC score.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are missing classes in <code>y_true</code> that cannot be handled.</p>"},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/#tabpfn_extensions.scoring.scoring_utils.score_classification","title":"score_classification","text":"<pre><code>score_classification(\n    optimize_metric: Literal[\n        \"roc\", \"auroc\", \"accuracy\", \"f1\", \"log_loss\"\n    ],\n    y_true,\n    y_pred,\n    sample_weight=None,\n    *,\n    y_pred_is_labels: bool = False\n)\n</code></pre> <p>General function to score classification predictions.</p> <p>Parameters:</p> Name Type Description Default <code>optimize_metric</code> <p>{\"roc\", \"auroc\", \"accuracy\", \"f1\", \"log_loss\"} The metric to use for scoring the predictions.</p> required <code>y_true</code> <p>array-like of shape (n_samples,) True labels or binary label indicators.</p> required <code>y_pred</code> <p>array-like of shape (n_samples,) or (n_samples, n_classes) Predicted labels, probabilities, or confidence values.</p> required <code>sample_weight</code> <p>array-like of shape (n_samples,), default=None Sample weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The score for the specified metric.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown metric is specified.</p>"},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/#tabpfn_extensions.scoring.scoring_utils.score_regression","title":"score_regression","text":"<pre><code>score_regression(\n    optimize_metric: Literal[\"rmse\", \"mse\", \"mae\"],\n    y_true,\n    y_pred,\n    sample_weight=None,\n)\n</code></pre> <p>General function to score regression predictions.</p> <p>Parameters:</p> Name Type Description Default <code>optimize_metric</code> <p>{\"rmse\", \"mse\", \"mae\"} The metric to use for scoring the predictions.</p> required <code>y_true</code> <p>array-like of shape (n_samples,) True target values.</p> required <code>y_pred</code> <p>array-like of shape (n_samples,) Predicted target values.</p> required <code>sample_weight</code> <p>array-like of shape (n_samples,), default=None Sample weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The score for the specified metric.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown metric is specified.</p>"},{"location":"reference/tabpfn_extensions/scoring/scoring_utils/#tabpfn_extensions.scoring.scoring_utils.score_survival","title":"score_survival","text":"<pre><code>score_survival(\n    optimize_metric: Literal[\"cindex\"],\n    y_true,\n    y_pred,\n    event_observed,\n    sample_weight=None,\n)\n</code></pre> <p>General function to score regression predictions.</p> <p>Parameters:</p> Name Type Description Default <code>optimize_metric</code> <p>{\"rmse\", \"mse\", \"mae\"} The metric to use for scoring the predictions.</p> required <code>y_true</code> <p>array-like of shape (n_samples,) True target values.</p> required <code>y_pred</code> <p>array-like of shape (n_samples,) Predicted target values.</p> required <code>sample_weight</code> <p>array-like of shape (n_samples,), default=None Sample weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The score for the specified metric.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown metric is specified.</p>"},{"location":"reference/tabpfn_extensions/sklearn_ensembles/configs/","title":"Configs","text":""},{"location":"reference/tabpfn_extensions/sklearn_ensembles/configs/#tabpfn_extensions.sklearn_ensembles.configs","title":"configs","text":""},{"location":"reference/tabpfn_extensions/sklearn_ensembles/meta_models/","title":"Meta models","text":""},{"location":"reference/tabpfn_extensions/sklearn_ensembles/meta_models/#tabpfn_extensions.sklearn_ensembles.meta_models","title":"meta_models","text":""},{"location":"reference/tabpfn_extensions/sklearn_ensembles/meta_models/#tabpfn_extensions.sklearn_ensembles.meta_models.get_single_tabpfn","title":"get_single_tabpfn","text":"<pre><code>get_single_tabpfn(config, **kwargs)\n</code></pre> <p>Create a single TabPFN model based on the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>Configuration object with parameters for the TabPFN model</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the TabPFN constructor</p> <code>{}</code> <p>Returns:</p> Type Description <p>A TabPFN model (classifier or regressor) configured according to the parameters</p>"},{"location":"reference/tabpfn_extensions/sklearn_ensembles/meta_models/#tabpfn_extensions.sklearn_ensembles.meta_models.get_tabpfn_outer_ensemble","title":"get_tabpfn_outer_ensemble","text":"<pre><code>get_tabpfn_outer_ensemble(config: TabPFNConfig, **kwargs)\n</code></pre> <p>This will create a model very similar to our standard TabPFN estimators, but it uses multiple model weights to generate predictions. Thus the <code>configs.TabPFNModelPathsConfig</code> can contain multiple paths which are all used.</p> <p>A product of the preprocessor_trasnforms and paths is created to yield interesting ensemble members.</p> <p>This only supports multiclass for now. If you want to add regression, you probably want to add the y_transforms to the relevant_config_product. :param config: TabPFNConfig :param kwargs: kwargs are passed to get_single_tabpfn, e.g. device :return: A TabPFNEnsemble, which is a soft voting classifier that mixes multiple standard TabPFN estimators.</p>"},{"location":"reference/tabpfn_extensions/sklearn_ensembles/weighted_ensemble/","title":"Weighted ensemble","text":""},{"location":"reference/tabpfn_extensions/sklearn_ensembles/weighted_ensemble/#tabpfn_extensions.sklearn_ensembles.weighted_ensemble","title":"weighted_ensemble","text":""},{"location":"reference/tabpfn_extensions/unsupervised/experiments/","title":"Experiments","text":""},{"location":"reference/tabpfn_extensions/unsupervised/experiments/#tabpfn_extensions.unsupervised.experiments","title":"experiments","text":""},{"location":"reference/tabpfn_extensions/unsupervised/experiments/#tabpfn_extensions.unsupervised.experiments.EmbeddingUnsupervisedExperiment","title":"EmbeddingUnsupervisedExperiment","text":"<p>             Bases: <code>Experiment</code></p> <p>This class is used to run experiments on synthetic toy functions.</p>"},{"location":"reference/tabpfn_extensions/unsupervised/experiments/#tabpfn_extensions.unsupervised.experiments.GenerateSyntheticDataExperiment","title":"GenerateSyntheticDataExperiment","text":"<p>             Bases: <code>Experiment</code></p> <p>This class is used to run experiments on generating synthetic data.</p>"},{"location":"reference/tabpfn_extensions/unsupervised/experiments/#tabpfn_extensions.unsupervised.experiments.GenerateSyntheticDataExperiment.run","title":"run","text":"<pre><code>run(tabpfn, **kwargs)\n</code></pre> <p>:param tabpfn: :param kwargs:     indices: list of indices from X features to use :return:</p>"},{"location":"reference/tabpfn_extensions/unsupervised/experiments/#tabpfn_extensions.unsupervised.experiments.OutlierDetectionUnsupervisedExperiment","title":"OutlierDetectionUnsupervisedExperiment","text":"<p>             Bases: <code>Experiment</code></p> <p>This class is used to run experiments for outlier detection.</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/","title":"Unsupervised","text":""},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised","title":"unsupervised","text":"<p>This module enables TabPFN to be used for unsupervised learning tasks including missing value imputation, outlier detection, and synthetic data generation. It leverages TabPFN's probabilistic nature to model joint data distributions without training labels.</p> <p>Key features: - Missing value imputation with probabilistic sampling - Outlier detection based on feature-wise probability estimation - Synthetic data generation with controllable randomness - Compatibility with both TabPFN and TabPFN-client backends - Support for mixed data types (categorical and numerical features) - Flexible permutation-based approach for feature dependencies</p> Example usage <pre><code>from tabpfn import TabPFNClassifier, TabPFNRegressor\nfrom tabpfn_extensions.unsupervised import TabPFNUnsupervisedModel\n\n# Create TabPFN models for classification and regression\nclf = TabPFNClassifier()\nreg = TabPFNRegressor()\n\n# Create the unsupervised model\nmodel = TabPFNUnsupervisedModel(tabpfn_clf=clf, tabpfn_reg=reg)\n\n# Fit the model on data without labels\nmodel.fit(X_train)\n\n# Different unsupervised tasks\nX_imputed = model.impute(X_with_missing_values)  # Fill missing values\noutlier_scores = model.outliers(X_test)          # Detect outliers\nX_synthetic = model.generate_synthetic_data(100)  # Generate new samples\n</code></pre>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel","title":"TabPFNUnsupervisedModel","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>TabPFN experiments model for imputation, outlier detection, and synthetic data generation.</p> <p>This model combines a TabPFNClassifier for categorical features and a TabPFNRegressor for numerical features to perform various experiments learning tasks on tabular data.</p> <p>Parameters:</p> Name Type Description Default <code>tabpfn_clf</code> <p>TabPFNClassifier, optional TabPFNClassifier instance for handling categorical features. If not provided, the model assumes that there are no categorical features in the data.</p> <code>None</code> <code>tabpfn_reg</code> <p>TabPFNRegressor, optional TabPFNRegressor instance for handling numerical features. If not provided, the model assumes that there are no numerical features in the data.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>categorical_features</code> <p>list List of indices of categorical features in the input data.</p> Example<pre><code>&gt;&gt;&gt; tabpfn_clf = TabPFNClassifier()\n&gt;&gt;&gt; tabpfn_reg = TabPFNRegressor()\n&gt;&gt;&gt; model = TabPFNUnsupervisedModel(tabpfn_clf, tabpfn_reg)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\n&gt;&gt;&gt; model.fit(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_imputed = model.impute(X)\n&gt;&gt;&gt; X_outliers = model.outliers(X)\n&gt;&gt;&gt; X_synthetic = model.generate_synthetic_data(n_samples=100)\n</code></pre>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.density_","title":"density_","text":"<pre><code>density_(\n    X_predict: Tensor,\n    X_fit: Tensor,\n    conditional_idx: list[int],\n    column_idx: int,\n) -&gt; tuple[Any, Tensor, Tensor]\n</code></pre> <p>Generate density predictions for a specific feature based on other features.</p> <p>This internal method is used by the imputation and outlier detection algorithms to model the conditional probability distribution of one feature given others.</p> <p>Parameters:</p> Name Type Description Default <code>X_predict</code> <code>Tensor</code> <p>Input data for which to make predictions</p> required <code>X_fit</code> <code>Tensor</code> <p>Training data to fit the model</p> required <code>conditional_idx</code> <code>list[int]</code> <p>Indices of features to condition on</p> required <code>column_idx</code> <code>int</code> <p>Index of the feature to predict</p> required <p>Returns:</p> Type Description <code>tuple[Any, Tensor, Tensor]</code> <p>tuple containing: - The fitted model (classifier or regressor) - The filtered features used for prediction - The target feature values to predict</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray | Tensor | DataFrame,\n    y: ndarray | Tensor | Series | None = None,\n) -&gt; TabPFNUnsupervisedModel\n</code></pre> <p>Fit the model to the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray | Tensor | DataFrame</code> <p>Union[np.ndarray, torch.Tensor, pd.DataFrame] Input data to fit the model, shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray | Tensor | Series | None</code> <p>Optional[Union[np.ndarray, torch.Tensor, pd.Series]], default=None Target values, shape (n_samples,). Optional since this is an unsupervised model.</p> <code>None</code> <p>Returns:</p> Type Description <code>TabPFNUnsupervisedModel</code> <p>TabPFNUnsupervisedModel Fitted model instance (self).</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.generate_synthetic_data","title":"generate_synthetic_data","text":"<pre><code>generate_synthetic_data(\n    n_samples: int = 100,\n    t: float = 1.0,\n    n_permutations: int = 3,\n) -&gt; Tensor\n</code></pre> <p>Generate synthetic tabular data samples using the fitted TabPFN models.</p> <p>This method uses imputation to create synthetic data, starting with a matrix of NaN values and filling in each feature sequentially. Samples are generated feature by feature in a single pass, with each feature conditioned on previously generated features.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>int, default=100 Number of synthetic samples to generate</p> <code>100</code> <code>t</code> <code>float</code> <p>float, default=1.0 Temperature parameter for sampling. Controls randomness: - Higher values (e.g., 1.0) produce more diverse samples - Lower values (e.g., 0.1) produce more deterministic samples</p> <code>1.0</code> <code>n_permutations</code> <code>int</code> <p>int, default=3 Number of feature permutations to use for generation More permutations may provide more robust results but increase computation time</p> <code>3</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Generated synthetic data of shape (n_samples, n_features)</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the model is not fitted (self.X_ does not exist)</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    X: tensor, per_column: bool = False\n) -&gt; tensor\n</code></pre> <p>Get the transformer embeddings for the test data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>tensor</code> required <p>Returns:</p> Type Description <code>tensor</code> <p>torch.Tensor of shape (n_samples, embedding_dim)</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.get_embeddings_per_column","title":"get_embeddings_per_column","text":"<pre><code>get_embeddings_per_column(X: tensor) -&gt; tensor\n</code></pre> <p>Alternative implementation for get_embeddings, where we get the embeddings for each column as a label separately and concatenate the results. This alternative way needs more passes but might be more accurate.</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.impute","title":"impute","text":"<pre><code>impute(\n    X: Tensor | ndarray | DataFrame,\n    t: float = 1e-09,\n    n_permutations: int = 10,\n) -&gt; Tensor\n</code></pre> <p>Impute missing values in the input data using the fitted TabPFN models.</p> <p>This method fills missing values (np.nan) in the input data by predicting each missing value based on the observed values in the same sample. The imputation uses multiple random feature permutations to improve robustness.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor | ndarray | DataFrame</code> <p>Union[torch.Tensor, np.ndarray, pd.DataFrame] Input data of shape (n_samples, n_features) with missing values encoded as np.nan.</p> required <code>t</code> <code>float</code> <p>float, default=0.000000001 Temperature for sampling from the imputation distribution. Lower values result in more deterministic imputations, while higher values introduce more randomness.</p> <code>1e-09</code> <code>n_permutations</code> <code>int</code> <p>int, default=10 Number of random feature permutations to use for imputation. Higher values may improve robustness but increase computation time.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor Imputed data with missing values replaced, of shape (n_samples, n_features).</p> Note <p>The model must be fitted with training data before calling this method.</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.impute_","title":"impute_","text":"<pre><code>impute_(\n    X: Tensor,\n    t: float = 1e-09,\n    n_permutations: int = 10,\n    condition_on_all_features: bool = True,\n    fast_mode: bool = False,\n) -&gt; Tensor\n</code></pre> <p>Impute missing values (np.nan) in X by sampling all cells independently from the trained models.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>torch.Tensor Input data of shape (n_samples, n_features) with missing values encoded as np.nan</p> required <code>t</code> <code>float</code> <p>float, default=0.000000001 Temperature for sampling from the imputation distribution, lower values are more deterministic</p> <code>1e-09</code> <code>n_permutations</code> <code>int</code> <p>int, default=10 Number of permutations to use for imputation</p> <code>10</code> <code>condition_on_all_features</code> <code>bool</code> <p>bool, default=True Whether to condition on all other features (True) or only previous features (False)</p> <code>True</code> <code>fast_mode</code> <code>bool</code> <p>bool, default=False Whether to use faster settings for testing</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Imputed data with missing values replaced</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.impute_single_permutation_","title":"impute_single_permutation_","text":"<pre><code>impute_single_permutation_(\n    X: Tensor,\n    feature_permutation: list[int] | tuple[int, ...],\n    t: float = 1e-09,\n    condition_on_all_features: bool = True,\n) -&gt; tuple[Tensor, dict[str, Tensor]]\n</code></pre> <p>Impute missing values (np.nan) in X by sampling all cells independently from the trained models.</p> <p>:param X: Input data of the shape (num_examples, num_features) with missing values encoded as np.nan :param t: Temperature for sampling from the imputation distribution, lower values are more deterministic :return: Imputed data, with missing values replaced</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.init_model_and_get_model_config","title":"init_model_and_get_model_config","text":"<pre><code>init_model_and_get_model_config() -&gt; None\n</code></pre> <p>Initialize TabPFN models for use in unsupervised learning.</p> <p>This function provides compatibility with different TabPFN implementations. It tries to initialize the model using the appropriate method based on the TabPFN implementation in use.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If model initialization fails</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.outliers","title":"outliers","text":"<pre><code>outliers(\n    X: Tensor | ndarray | DataFrame,\n    n_permutations: int = 10,\n) -&gt; Tensor\n</code></pre> <p>Calculate outlier scores for each sample in the input data.</p> <p>This is the preferred implementation for outlier detection, which calculates sample probability for each sample in X by multiplying the probabilities of each feature according to chain rule of probability. Lower probabilities indicate samples that are more likely to be outliers.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor | ndarray | DataFrame</code> <p>Union[torch.Tensor, np.ndarray, pd.DataFrame] Samples to calculate outlier scores for, shape (n_samples, n_features)</p> required <code>n_permutations</code> <code>int</code> <p>int, default=10 Number of permutations to use for more robust probability estimates. Higher values may produce more stable results but increase computation time.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Tensor of outlier scores (lower values indicate more likely outliers), shape (n_samples,)</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model initialization fails</p> <code>ValueError</code> <p>If the input data has incompatible dimensions</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.outliers_pdf","title":"outliers_pdf","text":"<pre><code>outliers_pdf(X: Tensor, n_permutations: int = 10) -&gt; Tensor\n</code></pre> <p>Calculate outlier scores based on probability density functions for continuous features.</p> <p>This method filters out categorical features and only considers numerical features for outlier detection using probability density functions.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Input data tensor</p> required <code>n_permutations</code> <code>int</code> <p>Number of permutations to use for the outlier calculation</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of outlier scores (lower values indicate more likely outliers)</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.outliers_pmf","title":"outliers_pmf","text":"<pre><code>outliers_pmf(X: Tensor, n_permutations: int = 10) -&gt; Tensor\n</code></pre> <p>Calculate outlier scores based on probability mass functions for categorical features.</p> <p>This method filters out numerical features and only considers categorical features for outlier detection using probability mass functions.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Input data tensor</p> required <code>n_permutations</code> <code>int</code> <p>Number of permutations to use for the outlier calculation</p> <code>10</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of outlier scores (lower values indicate more likely outliers)</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.sample_from_model_prediction_","title":"sample_from_model_prediction_","text":"<pre><code>sample_from_model_prediction_(\n    column_idx: int,\n    X_fit: Tensor,\n    model: Any,\n    X_predict: Tensor,\n    t: float,\n) -&gt; tuple[dict[str, Any] | ndarray, Tensor]\n</code></pre> <p>Sample values from a model's prediction distribution.</p> <p>Parameters:</p> Name Type Description Default <code>column_idx</code> <code>int</code> <p>Index of the column being predicted</p> required <code>X_fit</code> <code>Tensor</code> <p>Training data used to determine feature type</p> required <code>model</code> <code>Any</code> <p>The trained model (classifier or regressor)</p> required <code>X_predict</code> <code>Tensor</code> <p>Input data for prediction</p> required <code>t</code> <code>float</code> <p>Temperature parameter for sampling (lower values = more deterministic)</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any] | ndarray, Tensor]</code> <p>tuple containing: - The raw prediction output (dictionary for regressors, array for classifiers) - The sampled values as a tensor</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(\n    categorical_features: list[int],\n) -&gt; None\n</code></pre> <p>Set categorical feature indices for the model.</p> <p>Parameters:</p> Name Type Description Default <code>categorical_features</code> <code>list[int]</code> <p>List of indices of categorical features</p> required"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.TabPFNUnsupervisedModel.use_classifier_","title":"use_classifier_","text":"<pre><code>use_classifier_(\n    column_idx: int, y: Tensor | ndarray\n) -&gt; bool\n</code></pre> <p>Determine whether to use a classifier or regressor for a feature.</p> <p>Parameters:</p> Name Type Description Default <code>column_idx</code> <code>int</code> <p>Index of the column to check</p> required <code>y</code> <code>Tensor | ndarray</code> <p>Values of the feature</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if a classifier should be used, False for a regressor</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.efficient_random_permutation","title":"efficient_random_permutation","text":"<pre><code>efficient_random_permutation(\n    indices: list[int], n_permutations: int = 10\n) -&gt; list[tuple[int, ...]]\n</code></pre> <p>Generate multiple unique random permutations of the given indices.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list[int]</code> <p>List of indices to permute</p> required <code>n_permutations</code> <code>int</code> <p>Number of unique permutations to generate</p> <code>10</code> <p>Returns:</p> Type Description <code>list[tuple[int, ...]]</code> <p>List of unique permutations</p>"},{"location":"reference/tabpfn_extensions/unsupervised/unsupervised/#tabpfn_extensions.unsupervised.unsupervised.efficient_random_permutation_","title":"efficient_random_permutation_","text":"<pre><code>efficient_random_permutation_(\n    indices: list[int],\n) -&gt; tuple[int, ...]\n</code></pre> <p>Generate a single random permutation from the given indices.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list[int]</code> <p>List of indices to permute</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>A tuple representing a random permutation of the input indices</p>"},{"location":"research/papers/","title":"Papers","text":""},{"location":"research/papers/#tabpfn-followups","title":"TabPFN Followups","text":"<p>Forecastpfn: Synthetically-trained zero-shot forecasting    Dooley, Khurana, Mohapatra, Naidu, White Advances in Neural Information Processing Systems, 2024, Volume 36.</p> <p>Interpretable machine learning for TabPFN    Rundel, Kobialka, von Crailsheim, Feurer, Nagler, R{\"u}gamer World Conference on Explainable Artificial Intelligence, 2024, Pages 465--476.</p> <p>Scaling tabpfn: Sketching and feature selection for tabular prior-data fitted networks    Feuer, Hegde, Cohen arXiv preprint arXiv:2311.10609, 2023.</p> <p>In-Context Data Distillation with TabPFN    Ma, Thomas, Yu, Caterini arXiv preprint arXiv:2402.06971, 2024.</p> <p>Tokenize features, enhancing tables: the FT-TABPFN model for tabular classification    Liu, Yang, Liang, Pang, Zou arXiv preprint arXiv:2406.06891, 2024.</p> <p>Towards Localization via Data Embedding for TabPFN    Koshil, Nagler, Feurer, Eggensperger NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Enhancing Classification Performance Through the Synergistic Use of XGBoost, TABPFN, and LGBM Models    Prabowo, others 2023 15<sup>th</sup> International Congress on Advanced Applied Informatics Winter (IIAI-AAI-Winter), 2023, Pages 255--259.</p> <p>The Tabular Foundation Model TabPFN Outperforms Specialized Time Series Forecasting Models Based on Simple Features    Hoo, M{\"u}ller, Salinas, Hutter NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabPFGen--Tabular Data Generation with TabPFN    Ma, Dankar, Stein, Yu, Caterini arXiv preprint arXiv:2406.05216, 2024.</p> <p>Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data    Helli, Schnurr, Hollmann, M{\"u}ller, Hutter arXiv preprint arXiv:2411.10634, 2024.</p> <p>TabFlex: Scaling Tabular Learning to Millions with Linear Attention    Zeng, Kang, Mueller NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Retrieval \\&amp; Fine-Tuning for In-Context Tabular Models    Thomas, Ma, Hosseinzadeh, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2406.05207, 2024.</p> <p>TabDPT: Scaling Tabular Foundation Models    Ma, Thomas, Hosseinzadeh, Kamkari, Labach, Cresswell, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2410.18164, 2024.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    Breejen, Bae, Cha, Yun arXiv preprint arXiv:2405.13396, 2024.</p> <p>MotherNet: Fast Training and Inference via Hyper-Network Transformers    Mueller, Curino, Ramakrishnan NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Mixture of In-Context Prompters for Tabular PFNs    Xu, Cirit, Asadi, Sun, Wang arXiv preprint arXiv:2405.16156, 2024.</p> <p>Fast and Accurate Zero-Training Classification for Tabular Engineering Data    Picard, Ahmed arXiv preprint arXiv:2401.06948, 2024.</p> <p>Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning    den Breejen, Bae, Cha, Kim, Koh, Yun NeurIPS 2023 Second Table Representation Learning Workshop, 2023.</p> <p>TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks    Feuer, Schirrmeister, Cherepanova, Hegde, Hutter, Goldblum, Cohen, White arXiv preprint arXiv:2402.11137, 2024.</p> <p>Exploration of autoregressive models for in-context learning on tabular data    Baur, Kim NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabMDA: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting    Margeloiu, Bazaga, Simidjievski, Li{`o}, Jamnik arXiv preprint arXiv:2406.01805, 2024.</p> <p>Large Scale Transfer Learning for Tabular Data via Language Modeling    Gardner, Perdomo, Schmidt arXiv preprint arXiv:2406.12031, 2024.</p> <p>AnnotatedTables: A Large Tabular Dataset with Language Model Annotations    Hu, Fountalis, Tian, Vasiloglou arXiv preprint arXiv:2406.16349, 2024.</p> <p>TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling    Gorishniy, Kotelnikov, Babenko arXiv preprint arXiv:2410.24210, 2024.</p> <p>Pre-Trained Tabular Transformer for Real-Time, Efficient, Stable Radiomics Data Processing: A Comprehensive Study    Jiang, Jia, Zhang, Li 2023 IEEE International Conference on E-health Networking, Application \\&amp; Services (Healthcom), 2023, Pages 276--281.</p> <p>TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik arXiv preprint arXiv:2409.16118, 2024.</p> <p>Augmenting Small-size Tabular Data with Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>FORECASTPFN: ZERO-SHOT LOW-RESOURCE FORECASTING    Khurana, Dooley, Naidu, White, AI No Source, No Year.</p> <p>What exactly has TabPFN learned to do?    McCarter The Third Blogpost Track at ICLR 2024, No Year.</p> <p>Statistical foundations of prior-data fitted networks    Nagler International Conference on Machine Learning, 2023, Pages 25660--25676.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    den Breejen, Bae, Cha, Yun arXiv e-prints, 2024, Pages arXiv--2405.</p>"},{"location":"research/papers/#tabpfn-application","title":"TabPFN Application","text":"<p>Large-scale chemoproteomics expedites ligand discovery and predicts ligand behavior in cells    Offensperger, Tin, Duran-Frigola, Hahn, Dobner, Ende, Strohbach, Rukavina, Brennsteiner, Ogilvie, others Science, 2024, Volume 384, Issue 6694, Pages eadk5864.</p> <p>Deep learning for cross-selling health insurance classification    Chu, Than, Jo 2024 International Conference on Green Energy, Computing and Sustainable Technology (GECOST), 2024, Pages 453--457.</p> <p>Early fault classification in rotating machinery with limited data using TabPFN    Magad{'a}n, Rold{'a}n-G{'o}mez, Granda, Su{'a}rez IEEE Sensors Journal, 2023.</p> <p>Artificial intelligence-driven predictive framework for early detection of still birth    Alzakari, Aldrees, Umer, Cascone, Innab, Ashraf SLAS technology, 2024, Volume 29, Issue 6, Pages 100203.</p> <p>Prostate Cancer Diagnosis via Visual Representation of Tabular Data and Deep Transfer Learning    El-Melegy, Mamdouh, Ali, Badawy, El-Ghar, Alghamdi, El-Baz Bioengineering, 2024, Volume 11, Issue 7, Pages 635.</p> <p>A machine learning-based approach for individualized prediction of short-term outcomes after anterior cervical corpectomy    Karabacak, Schupper, Carr, Margetis Asian Spine Journal, 2024, Volume 18, Issue 4, Pages 541.</p> <p>Comparing the Performance of a Deep Learning Model (TabPFN) for Predicting River Algal Blooms with Varying Data Composition    Yang, Park Journal of Wetlands Research, 2024, Volume 26, Issue 3, Pages 197--203.</p> <p>Adapting TabPFN for Zero-Inflated Metagenomic Data    Perciballi, Granese, Fall, Zehraoui, Prifti, Zucker NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Comprehensive peripheral blood immunoprofiling reveals five immunotypes with immunotherapy response characteristics in patients with cancer    Dyikanov, Zaitsev, Vasileva, Wang, Sokolov, Bolshakov, Frank, Turova, Golubeva, Gantseva, others Cancer Cell, 2024, Volume 42, Issue 5, Pages 759--779.</p> <p>Predicting dementia in Parkinson's disease on a small tabular dataset using hybrid LightGBM--TabPFN and SHAP    Tran, Byeon Digital Health, 2024, Volume 10, Pages 20552076241272585.</p> <p>Enhancing actuarial non-life pricing models via transformers    Brauer European Actuarial Journal, 2024, Pages 1--22.</p> <p>Machine learning-based diagnostic prediction of minimal change disease: model development study    Noda, Ichikawa, Shibagaki Scientific Reports, 2024, Volume 14, Issue 1, Pages 23460.</p> <p>Using AutoML and generative AI to predict the type of wildfire propagation in Canadian conifer forests    Khanmohammadi, Cruz, Perrakis, Alexander, Arashpour Ecological Informatics, 2024, Volume 82, Pages 102711.</p> <p>Machine learning applications on lunar meteorite minerals: From classification to mechanical properties prediction    Pe{~n}a-Asensio, Trigo-Rodr{'\\i}guez, Sort, Ib{'a}{~n}ez-Insa, Rimola International Journal of Mining Science and Technology, 2024.</p> <p>Data-Driven Prognostication in Distal Medium Vessel Occlusions Using Explainable Machine Learning    Karabacak, Ozkara, Faizy, Hardigan, Heit, Lakhani, Margetis, Mocco, Nael, Wintermark, others American Journal of Neuroradiology, 2024.</p>"},{"location":"tutorials/cheat_sheet/","title":"Cheat Sheet / Best practices","text":"<p>Look at Autogluon cheat sheet [https://auto.gluon.ai/stable/cheatsheet.html]</p>"},{"location":"tutorials/classification/","title":"Classification","text":"<p>TabPFN provides a powerful interface for handling classification tasks on tabular data. The <code>TabPFNClassifier</code> class can be used for binary and multi-class classification problems.</p>"},{"location":"tutorials/classification/#example","title":"Example","text":"<p>Below is an example of how to use <code>TabPFNClassifier</code> for a multi-class classification task:</p> Python API Client (No GPU, Online)Python Local (GPU) <pre><code>from tabpfn_client import TabPFNClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train classifier\nclassifier = TabPFNClassifier(device='cuda', N_ensemble_configurations=10)\nclassifier.fit(X_train, y_train)\n\n# Evaluate\ny_pred = classifier.predict(X_test)\nprint('Test Accuracy:', accuracy_score(y_test, y_pred))\n</code></pre> <pre><code>from tabpfn import TabPFNClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train classifier\nclassifier = TabPFNClassifier(device='cuda', N_ensemble_configurations=10)\nclassifier.fit(X_train, y_train)\n\n# Evaluate\ny_pred = classifier.predict(X_test)\nprint('Test Accuracy:', accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"tutorials/classification/#example-with-autotabpfnclassifier","title":"Example with AutoTabPFNClassifier","text":"<p>Abstract</p> <p>AutoTabPFNClassifier yields the most accurate predictions for TabPFN and is recommended for most use cases. The AutoTabPFNClassifier and AutoTabPFNRegressor automatically run a hyperparameter search and build an ensemble of strong hyperparameters. You can control the runtime using \u00b4max_time\u00b4 and need to make no further adjustments to get best results.</p> <pre><code>from tabpfn_extensions.post_hoc_ensembles.sklearn_interface import AutoTabPFNClassifier\n\n# we refer to the PHE variant of TabPFN as AutoTabPFN in the code\nclf = AutoTabPFNClassifier(device='auto', max_time=30)\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict_proba(X_test)\ny_eval = np.argmax(preds, axis=1)\n\nprint('ROC AUC: ',  sklearn.metrics.roc_auc_score(y_test, preds[:,1], multi_class='ovr'), 'Accuracy', sklearn.metrics.accuracy_score(y_test, y_eval))\n</code></pre>"},{"location":"tutorials/distshift/","title":"TabPFN's Out-of-Distribution Excellence","text":"<p>Recent research demonstrates TabPFN's out-of-distribution (OOD) performance on tabular data, with further improvements through Drift-Resilient modifications.</p>"},{"location":"tutorials/distshift/#key-performance-metrics","title":"Key Performance Metrics","text":"Model OOD Accuracy OOD ROC AUC TabPFN Base 0.688 0.786 TabPFN + Drift-Resilient 0.744 0.832 XGBoost 0.664 0.754 CatBoost 0.677 0.766"},{"location":"tutorials/distshift/#technical-improvements","title":"Technical Improvements","text":"<p>The Drift-Resilient modifications introduce:</p> <ul> <li>2<sup>nd</sup>-order structural causal model for temporal adaptation</li> <li>Enhanced pattern recognition across distribution shifts </li> <li>Zero hyperparameter tuning requirement</li> <li>Inference in seconds on small/medium datasets</li> </ul>"},{"location":"tutorials/distshift/#benchmark","title":"Benchmark","text":"<p>The enhanced model shows robust generalization across:</p> <ul> <li>18 diverse datasets (synthetic + real-world)</li> <li>Various temporal shift patterns</li> <li>Multiple industry applications</li> </ul> <p>For comprehensive documentation and implementation details, visit the GitHub repository.</p>"},{"location":"tutorials/distshift/#citation","title":"Citation","text":"<pre><code>@inproceedings{\n  helli2024driftresilient,\n  title={Drift-Resilient Tab{PFN}: In-Context Learning Temporal Distribution Shifts on Tabular Data},\n  author={Kai Helli and David Schnurr and Noah Hollmann and Samuel M{\\\"u}ller and Frank Hutter},\n  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\n  year={2024},\n  url={https://openreview.net/forum?id=p3tSEFMwpG}\n}\n</code></pre>"},{"location":"tutorials/regression/","title":"Regression","text":"<p>TabPFN can also be applied to regression tasks using the <code>TabPFNRegressor</code> class. This allows for predictive modeling of continuous outcomes.</p>"},{"location":"tutorials/regression/#example","title":"Example","text":"<p>An example usage of <code>TabPFNRegressor</code> is shown below:</p> Python API Client (No GPU, Online)Python Local (GPU) <pre><code>from tabpfn_client import TabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = TabPFNRegressor(device='auto')\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre> <pre><code>from tabpfn import TabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = TabPFNRegressor(device='auto')\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre> <p>This example demonstrates how to train and evaluate a regression model. For more details on TabPFNRegressor and its parameters, refer to the API Reference section.</p>"},{"location":"tutorials/regression/#example-with-autotabpfnregressor","title":"Example with AutoTabPFNRegressor","text":"<p>Abstract</p> <p>AutoTabPFNRegressor yields the most accurate predictions for TabPFN and is recommended for most use cases. The AutoTabPFNClassifier and AutoTabPFNRegressor automatically run a hyperparameter search and build an ensemble of strong hyperparameters. You can control the runtime using \u00b4max_time\u00b4 and need to make no further adjustments to get best results.</p> <pre><code>from tabpfn_extensions.post_hoc_ensembles.sklearn_interface import AutoTabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = AutoTabPFNRegressor(max_time=30) # runs for 30 seconds\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre>"},{"location":"tutorials/timeseries/","title":"Time Series Tutorial","text":"<p>TabPFN can be used for time series forecasting by framing it as a tabular regression problem. This tutorial demonstrates how to use the TabPFN Time Series package for accurate zero-shot forecasting. It was developed by Shi Bin Hoo, Samuel M\u00fcller, David Salinas and Frank Hutter.</p>"},{"location":"tutorials/timeseries/#quick-start","title":"Quick Start","text":"<p>First, install the package:</p> <pre><code>!git clone https://github.com/liam-sbhoo/tabpfn-time-series.git\n!pip install -r tabpfn-time-series/requirements.txt\n</code></pre> <p>See the demo notebook for a complete example.</p>"},{"location":"tutorials/timeseries/#how-it-works","title":"How It Works","text":"<p>TabPFN performs time series forecasting by:</p> <ol> <li>Converting time series data into a tabular format</li> <li>Extracting temporal features (trends, seasonality, etc.)</li> <li>Using TabPFN's regression capabilities for prediction</li> <li>Converting predictions back to time series format</li> </ol> <p>This approach provides several benefits:</p> <ul> <li>Zero-shot forecasting: No training required - just fit and predict</li> <li>Both point and probabilistic forecasts: Get confidence intervals with your predictions</li> <li>Support for exogenous variables: Easily incorporate external factors</li> <li>Fast inference: Uses tabpfn-client for GPU-accelerated predictions</li> </ul>"},{"location":"tutorials/timeseries/#additional-resources","title":"Additional Resources","text":"<ul> <li>GitHub Repository</li> <li>Research Paper</li> </ul>"},{"location":"tutorials/timeseries/#getting-help","title":"Getting Help","text":"<p>Join our Discord community for support and discussions about TabPFN time series forecasting.</p>"},{"location":"tutorials/unsupervised/","title":"Unsupervised functionalities","text":"<p>Warning</p> <p>This functionality is currently only supported using the Local TabPFN Version but not the API.</p>"},{"location":"tutorials/unsupervised/#data-generation","title":"Data Generation","text":"<pre><code>import numpy as np\nimport torch\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom tabpfn_extensions import TabPFNClassifier, TabPFNRegressor\nfrom tabpfn_extensions import unsupervised\n\n# Load the breast cancer dataset\ndf = load_breast_cancer(return_X_y=False)\nX, y = df[\"data\"], df[\"target\"]\nattribute_names = df[\"feature_names\"]\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=42\n)\n\n# Initialize TabPFN models\nclf = TabPFNClassifier(n_estimators=3)\nreg = TabPFNClassifier(n_estimators=3)\n\n# Initialize unsupervised model\nmodel_unsupervised = unsupervised.TabPFNUnsupervisedModel(\n    tabpfn_clf=clf, tabpfn_reg=reg\n)\n\n# Select features for analysis (e.g., first two features)\nfeature_indices = [0, 1]\n\n# Create and run synthetic experiment\nexp_synthetic = unsupervised.experiments.GenerateSyntheticDataExperiment(\n    task_type=\"unsupervised\"\n)\n\n# Convert data to torch tensors\nX_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_tensor = torch.tensor(y_train, dtype=torch.float32)\n\n# Run the experiment\nresults = exp_synthetic.run(\n    tabpfn=model_unsupervised,\n    X=X_tensor,\n    y=y_tensor,\n    attribute_names=attribute_names,\n    temp=1.0,\n    n_samples=X_train.shape[0] * 3,  # Generate 3x original samples\n    indices=feature_indices,\n)\n</code></pre>"},{"location":"tutorials/unsupervised/#outlier-detection","title":"Outlier Detection","text":"<pre><code>import torch\nfrom sklearn.datasets import load_breast_cancer\nfrom tabpfn_extensions import unsupervised\nfrom tabpfn_extensions import TabPFNClassifier, TabPFNRegressor\n\n# Load data\ndf = load_breast_cancer(return_X_y=False)\nX, y = df[\"data\"], df[\"target\"]\nattribute_names = df[\"feature_names\"]\n\n# Initialize models\nclf = TabPFNClassifier(n_estimators=4)\nreg = TabPFNRegressor(n_estimators=4)\nmodel_unsupervised = unsupervised.TabPFNUnsupervisedModel(\n    tabpfn_clf=clf, tabpfn_reg=reg\n)\n\n# Run outlier detection\nexp_outlier = unsupervised.experiments.OutlierDetectionUnsupervisedExperiment(\n    task_type=\"unsupervised\"\n)\nresults = exp_outlier.run(\n    tabpfn=model_unsupervised,\n    X=torch.tensor(X),\n    y=torch.tensor(y),\n    attribute_names=attribute_names,\n    indices=[4, 12],  # Analyze features 4 and 12\n)\n</code></pre>"}]}