{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TabPFN Tabular Foundation Model","text":"<p>This page contains usage examples and installation instructions of TabPFN. Please find additional instructions on our Classifiers and Regressors on the respective subpages. An in-depth technical documentation of our software interfaces can be found in the API Reference</p>"},{"location":"#installation","title":"Installation","text":"<p>You can access our models through our API (https://github.com/automl/tabpfn-client) or via our user interface built on top of the API (https://www.priorlabs.ai/tabpfn-gui). We will release open weights models soon, currently we are available via api and via our user interface built on top of the API.</p>"},{"location":"#example-usage","title":"Example usage","text":"ClassificationRegression <pre><code>import numpy as np\nimport sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nfrom tabpfn import TabPFNClassifier\n\n# Create a classifier\nclf = TabPFNClassifier(fit_at_predict_time=True)\n\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nclf.fit(X_train, y_train)\npreds = clf.predict_proba(X_test)\ny_eval = np.argmax(preds, axis=1)\n\nprint('ROC AUC: ', sklearn.metrics.roc_auc_score(y_test, preds, multi_class='ovr'), 'Accuracy', sklearn.metrics.accuracy_score(y_test, y_eval))\n</code></pre> <pre><code>from tabpfn import TabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = TabPFNRegressor(device='auto')\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre>"},{"location":"api_reference/","title":"API Reference","text":"<p>PreprocessorConfig </p> <p>ClassificationOptimizationMetricType </p> <p>RegressionOptimizationMetricType </p>"},{"location":"api_reference/#scripts.estimator.PreprocessorConfig","title":"PreprocessorConfig  <code>dataclass</code>","text":"<p>Configuration for data preprocessors.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal</code> <p>Name of the preprocessor.</p> <code>categorical_name</code> <code>Literal</code> <p>Name of the categorical encoding method. Valid options are \"none\", \"numeric\",                     \"onehot\", \"ordinal\", \"ordinal_shuffled\". Default is \"none\".</p> <code>append_original</code> <code>bool</code> <p>Whether to append the original features to the transformed features. Default is False.</p> <code>subsample_features</code> <code>float</code> <p>Fraction of features to subsample. -1 means no subsampling. Default is -1.</p> <code>global_transformer_name</code> <code>str</code> <p>Name of the global transformer to use. Default is None.</p>"},{"location":"api_reference/#scripts.estimator.ClassificationOptimizationMetricType","title":"ClassificationOptimizationMetricType  <code>module-attribute</code>","text":"<pre><code>ClassificationOptimizationMetricType = Literal[\n    \"auroc\",\n    \"roc\",\n    \"auroc_ovo\",\n    \"balanced_acc\",\n    \"acc\",\n    \"log_loss\",\n    None,\n]\n</code></pre>"},{"location":"api_reference/#scripts.estimator.RegressionOptimizationMetricType","title":"RegressionOptimizationMetricType  <code>module-attribute</code>","text":"<pre><code>RegressionOptimizationMetricType = Literal[\n    \"mse\",\n    \"rmse\",\n    \"mae\",\n    \"r2\",\n    \"mean\",\n    \"median\",\n    \"mode\",\n    \"exact_match\",\n    None,\n]\n</code></pre>"},{"location":"cheat_sheet/","title":"Cheat Sheet / Best practices","text":"<p>Look at Autogluon cheat sheet [https://auto.gluon.ai/stable/cheatsheet.html]</p>"},{"location":"classification/","title":"Classification","text":"<p>TabPFN provides a powerful interface for handling classification tasks on tabular data. The <code>TabPFNClassifier</code> class can be used for binary and multi-class classification problems.</p>"},{"location":"classification/#example","title":"Example","text":"<p>Below is an example of how to use <code>TabPFNClassifier</code> for a multi-class classification task:</p> <pre><code>from tabpfn import TabPFNClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train classifier\nclassifier = TabPFNClassifier(device='cuda', N_ensemble_configurations=10)\nclassifier.fit(X_train, y_train)\n\n# Evaluate\ny_pred = classifier.predict(X_test)\nprint('Test Accuracy:', accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"classification/#example-with-autotabpfnclassifier","title":"Example with AutoTabPFNClassifier","text":"<p>Abstract</p> <p>The AutoTabPFNClassifier and AutoTabPFNRegressor automatically run a hyperparameter search and build an ensemble of strong hyperparameters. You can control the runtime using \u00b4max_time\u00b4 and need to make no further adjustments to get best results.</p> <pre><code>from tabpfn.scripts.estimator.post_hoc_ensembles import AutoTabPFNClassifier, AutoTabPFNRegressor\n# we refer to the PHE variant of TabPFN as AutoTabPFN in the code\nclf = AutoTabPFNClassifier(device='auto', max_time=30)\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict_proba(X_test)\ny_eval = np.argmax(preds, axis=1)\n\nprint('ROC AUC: ',  sklearn.metrics.roc_auc_score(y_test, preds[:,1], multi_class='ovr'), 'Accuracy', sklearn.metrics.accuracy_score(y_test, y_eval))\n</code></pre>"},{"location":"code_download/","title":"Code download","text":"<p>Our code is currently stored in a private repository on GitHub, to avoid indexing at this point. We do not share this links on this public website. To access our code and any example notebooks, please use the notebook provided in the links to our submission via the \u201clinktree\u201d in our main paper or the code submission checklist. </p>"},{"location":"contribute/","title":"Contribute","text":"<p>Put out project that people could contribute to and provide instructions for contributing</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#citing","title":"Citing","text":""},{"location":"credits/#acknowledgments","title":"Acknowledgments","text":"<p>We express our gratitude to the following individuals for their valuable contributions and support:</p> <p>Eddie Bergman for his assistance with the evaluation of TabPFN and for his efforts in improving the code quality and documentation. His contributions were instrumental in benchmarking TabPFN and ensuring the reproducibility of our results.</p> <p>Liam Shi Bin Hoo, Anshul Gupta, and David Otte for their work on the Inference Server, which enables the fast deployment of TabPFN without the need for a local GPU. Their efforts have greatly enhanced the accessibility and usability of TabPFN.</p> <p>David Schnurr and Kai Helli for their work on visualization, and David Schnurr for his specific contributions related to handling missing values. Lukas Schweizer for his work on exploring the Random Forest preprocessing for TabPFN further.</p> <p>Andreas M\u00fcller for the insightful discussions related to TabPFN training and for his guidance on identifying and mitigating biases in the prior. His expertise has been invaluable in refining the TabPFN methodology.</p> <p>Stefan St\u00e4glich for his diligent maintenance of the cluster infrastructure.</p> <p>We also extend our thanks to Claudia Langenberg and Maik Pietzner for providing insights on medical applications.</p> <p>We are grateful for the computational resources that were available for this research. Specifically, we acknowledge support by the state of Baden-W\u00fcrttemberg through bwHPC and the German Research Foundation (DFG) through grant no INST 39/963-1 FUGG (bwForCluster NEMO), and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828.</p> <p>We gratefully acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under SFB 1597 (SmallData), grant number 499552394, and by the European Union (via ERC Consolidator Grant DeepLearning 2.0, grant no.~101045765). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. </p>"},{"location":"example_competitions/","title":"Cheat Sheet / Best practices","text":"<p>Look at Autogluon</p>"},{"location":"getting_started/","title":"Installation","text":"<p>To install TabPFN, please use the notebooks provided in this review. After review we are going to release our models as a python package on the python repository pypi.</p>"},{"location":"getting_started/#example","title":"Example","text":"<p>A simple way to get started with TabPFN using our sklearn interface is demonstrated below. This example shows how to train a classifier on the breast cancer dataset and evaluate its accuracy.</p> <p><pre><code>from sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nfrom tabpfn import TabPFNClassifier\n\n# Load data\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Initialize classifier\nclassifier = TabPFNClassifier(device='cpu', N_ensemble_configurations=32)\n\n# Train classifier\nclassifier.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = classifier.predict(X_test)\nprint('Accuracy:', accuracy_score(y_test, y_pred))\n</code></pre> This example demonstrates the basic workflow of training and predicting with TabPFN models. For more advanced usage, including handling of categorical data, please refer to the Advanced Usage section.</p>"},{"location":"intended_use/","title":"Usage tips","text":""},{"location":"intended_use/#when-to-use-tabpfn","title":"When to use TabPFN","text":"<p>TabPFN excels in handling small to medium-sized datasets with up to 10,000 samples and 500 features. For larger datasets, approaches such as CatBoost, XGB, or AutoGluon are likely to outperform TabPFN.</p>"},{"location":"intended_use/#intended-use-of-tabpfn","title":"Intended Use of TabPFN","text":"<p>While TabPFN provides a powerful drop-in replacement for traditional tabular data models, achieving top performance on real-world problems often requires domain expertise and the ingenuity of data scientists. Data scientists should continue to apply their skills in feature engineering, data cleaning, and problem framing to get the most out of TabPFN.</p>"},{"location":"intended_use/#limitations-of-tabpfn","title":"Limitations of TabPFN","text":"<ol> <li>TabPFN's inference speed may be slower than highly optimized approaches like CatBoost.</li> <li>TabPFN's memory usage scales linearly with dataset size, which can be prohibitive for very large datasets.</li> <li>Our evaluation focused on datasets with up to 10,000 samples and 500 features; scalability to larger datasets requires further study.</li> </ol>"},{"location":"intended_use/#computational-and-time-requirements","title":"Computational and Time Requirements","text":"<p>TabPFN is computationally efficient and can run on consumer hardware for most datasets. Training on a new dataset is recommended to run on a GPU as this speeds it up significantly. However, TabPFN is not optimized for real-time inference tasks.</p>"},{"location":"intended_use/#data-preparation","title":"Data Preparation","text":"<p>TabPFN can handle raw data with minimal preprocessing. Provide the data in a tabular format, and TabPFN will automatically handle missing values, encode categorical variables, and normalize features. While TabPFN works well out-of-the-box, performance can further be improved using dataset-specific preprocessings.</p>"},{"location":"intended_use/#interpreting-results","title":"Interpreting Results","text":"<p>TabPFN's predictions come with uncertainty estimates, allowing you to assess the reliability of the results. You can use SHAP to interpret TabPFN's predictions and identify the most important features driving the model's decisions.</p>"},{"location":"intended_use/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>TabPFN provides strong performance out-of-the-box without extensive hyperparameter tuning. If you have additional computational resources, you can further optimize TabPFN's performance using random hyperparameter tuning or the Post-Hoc Ensembling (PHE) technique.</p>"},{"location":"regression/","title":"Regression","text":"<p>TabPFN can also be applied to regression tasks using the <code>TabPFNRegressor</code> class. This allows for predictive modeling of continuous outcomes.</p>"},{"location":"regression/#example","title":"Example","text":"<p>An example usage of <code>TabPFNRegressor</code> is shown below:</p> <p><pre><code>from tabpfn import TabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = TabPFNRegressor(device='auto')\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre> This example demonstrates how to train and evaluate a regression model. For more details on TabPFNRegressor and its parameters, refer to the API Reference section.</p>"},{"location":"regression/#example-with-autotabpfnregressor","title":"Example with AutoTabPFNRegressor","text":"<pre><code>from tabpfn.scripts.estimator.post_hoc_ensembles import AutoTabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = AutoTabPFNRegressor(device='auto\u2019, max_time=30)\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre>"},{"location":"release_notes/","title":"Release notes","text":"<pre><code># Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [1.0.0] - 2017-06-20\n</code></pre>"},{"location":"reproduction/","title":"Experimental Reproduction","text":"<p>Our code is currently stored in a private repository on GitHub, to avoid indexing at this point. We do not share this links on this public website. To access our code and any example notebooks, please use the notebook provided in the links to our submission via the \u201clinktree\u201d in our main paper or the code submission checklist. </p>"},{"location":"unsupervised/","title":"Unsupervised functionalities","text":"<pre><code>from tabpfn.scripts.estimator import TabPFNUnsupervisedModel, TabPFNClassifier, TabPFNRegressor\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nmodel_unsupervised = TabPFNUnsupervisedModel(TabPFNClassifier(), TabPFNRegressor())\n\n# Load the Iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n\nmodel_unsupervised.fit(X_train, y_train)\nembeddings = model_unsupervised.get_embeddings(X_test)\nX_outliers = model_unsupervised.outliers(X_test)\nX_synthetic = model_unsupervised.generate_synthetic_data(n_samples=100)\n</code></pre>"},{"location":"api/AutoTabPFNClassifier/","title":"AutoTabPFNClassifier","text":""},{"location":"api/AutoTabPFNClassifier/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier","title":"AutoTabPFNClassifier","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Automatic Post Hoc Ensemble Classifier for TabPFN models.</p> <p>Attributes:</p> Name Type Description <code>predictor_</code> <p>AutoPostHocEnsemblePredictor The predictor interface used to make predictions, see post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more.</p> <code>phe_init_args_</code> <p>dict The optional initialization arguments used for the post hoc ensemble predictor.</p>"},{"location":"api/AutoTabPFNClassifier/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier.__init__","title":"__init__","text":"<pre><code>__init__(\n    max_time: int | None = 30,\n    preset: Literal[\n        \"default\", \"custom_hps\", \"avoid_overfitting\"\n    ] = \"default\",\n    ges_scoring_string: str = \"roc\",\n    device: Literal[\"cpu\", \"cuda\"] = \"cpu\",\n    random_state: int | None | RandomState = None,\n    categorical_feature_indices: list[int] | None = None,\n    phe_init_args: dict | None = None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>max_time</code> <p>int | None, default=None The maximum time to spend on fitting the post hoc ensemble.</p> <code>30</code> <code>preset</code> <code>Literal['default', 'custom_hps', 'avoid_overfitting']</code> <p>{\"default\", \"custom_hps\", \"avoid_overfitting\"}, default=\"default\" The preset to use for the post hoc ensemble.</p> <code>'default'</code> <code>ges_scoring_string</code> <p>str, default=\"roc\" The scoring string to use for the greedy ensemble search. Allowed values are: {\"accuracy\", \"roc\" / \"auroc\", \"f1\", \"log_loss\"}.</p> <code>'roc'</code> <code>device</code> <p>{\"cpu\", \"cuda\"}, default=\"cuda\" The device to use for training and prediction.</p> <code>'cpu'</code> <code>random_state</code> <p>int, RandomState instance or None, default=None Controls both the randomness base models and the post hoc ensembling method.</p> <code>None</code> <code>categorical_feature_indices</code> <code>list[int] | None</code> <p>list[int] or None, default=None The indices of the categorical features in the input data. Can also be passed to <code>fit()</code>.</p> <code>None</code> <code>phe_init_args</code> <p>dict | None, default=None The initialization arguments for the post hoc ensemble predictor. See post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more options and all details.</p> <code>None</code>"},{"location":"api/AutoTabPFNClassifier/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier.fit","title":"fit","text":"<pre><code>fit(\n    X,\n    y,\n    categorical_feature_indices: list[int] | None = None,\n)\n</code></pre>"},{"location":"api/AutoTabPFNClassifier/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre>"},{"location":"api/AutoTabPFNClassifier/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre>"},{"location":"api/AutoTabPFNRegressor/","title":"AutoTabPFNRegressor","text":""},{"location":"api/AutoTabPFNRegressor/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor","title":"AutoTabPFNRegressor","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Automatic Post Hoc Ensemble Regressor for TabPFN models. Attributes:     predictor_ : AutoPostHocEnsemblePredictor         The predictor interface used to make predictions, see post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more.     phe_init_args_ : dict         The optional initialization arguments used for the post hoc ensemble predictor.</p>"},{"location":"api/AutoTabPFNRegressor/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor.__init__","title":"__init__","text":"<pre><code>__init__(\n    max_time: int | None = 30,\n    preset: Literal[\n        \"default\", \"custom_hps\", \"avoid_overfitting\"\n    ] = \"default\",\n    ges_scoring_string: str = \"mse\",\n    device: Literal[\"cpu\", \"cuda\"] = \"cpu\",\n    random_state: int | None | RandomState = None,\n    categorical_feature_indices: list[int] | None = None,\n    phe_init_args: dict | None = None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>max_time</code> <p>int | None, default=None The maximum time to spend on fitting the post hoc ensemble.</p> <code>30</code> <code>preset</code> <code>Literal['default', 'custom_hps', 'avoid_overfitting']</code> <p>{\"default\", \"custom_hps\", \"avoid_overfitting\"}, default=\"default\" The preset to use for the post hoc ensemble.</p> <code>'default'</code> <code>ges_scoring_string</code> <p>str, default=\"mse\" The scoring string to use for the greedy ensemble search. Allowed values are: {\"rmse\", \"mse\", \"mae\"}.</p> <code>'mse'</code> <code>device</code> <p>{\"cpu\", \"cuda\"}, default=\"cuda\" The device to use for training and prediction.</p> <code>'cpu'</code> <code>random_state</code> <p>int, RandomState instance or None, default=None Controls both the randomness base models and the post hoc ensembling method.</p> <code>None</code> <code>categorical_feature_indices</code> <code>list[int] | None</code> <p>list[int] or None, default=None The indices of the categorical features in the input data. Can also be passed to <code>fit()</code>.</p> <code>None</code> <code>phe_init_args</code> <p>dict | None, default=None The initialization arguments for the post hoc ensemble predictor. See post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more options and all details.</p> <code>None</code>"},{"location":"api/AutoTabPFNRegressor/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor.fit","title":"fit","text":"<pre><code>fit(\n    X,\n    y,\n    categorical_feature_indices: list[int] | None = None,\n)\n</code></pre>"},{"location":"api/AutoTabPFNRegressor/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre>"},{"location":"api/ClassifierAsRegressor/","title":"ClassifierAsRegressor","text":""},{"location":"api/ClassifierAsRegressor/#scripts.estimator.ClassifierAsRegressor","title":"ClassifierAsRegressor","text":"<p>             Bases: <code>RegressorMixin</code></p> <p>Wrapper class to use a classifier as a regressor.</p> <p>This class takes a classifier estimator and converts it into a regressor by encoding the target labels and treating the regression problem as a classification task.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>object Classifier estimator to be used as a regressor.</p> required <p>Attributes:</p> Name Type Description <code>label_encoder_</code> <p>LabelEncoder Label encoder used to transform target regression labels to classes.</p> <code>y_train_</code> <p>array-like of shape (n_samples,) Transformed target labels used for training.</p> <code>categorical_features</code> <p>list List of categorical feature indices.</p> Example<pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from tabpfn.scripts.estimator import ManyClassClassifier, TabPFNClassifier, ClassifierAsRegressor\n&gt;&gt;&gt; x, y = load_diabetes(return_X_y=True)\n&gt;&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n&gt;&gt;&gt; clf = TabPFNClassifier()\n&gt;&gt;&gt; clf = ManyClassClassifier(clf, n_estimators=10, alphabet_size=clf.max_num_classes_)\n&gt;&gt;&gt; reg = ClassifierAsRegressor(clf)\n&gt;&gt;&gt; reg.fit(x_train, y_train)\n&gt;&gt;&gt; y_pred = reg.predict(x_test)\n</code></pre>"},{"location":"api/ClassifierAsRegressor/#scripts.estimator.ClassifierAsRegressor.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre> <p>Fit the classifier as a regressor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Training data.</p> required <code>y</code> <p>array-like of shape (n_samples,) Target labels.</p> required <p>Returns:</p> Name Type Description <code>self</code> <p>object Fitted estimator.</p>"},{"location":"api/ClassifierAsRegressor/#scripts.estimator.ClassifierAsRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict the target values for the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Input data.</p> required <p>Returns:</p> Name Type Description <code>y_pred</code> <p>array-like of shape (n_samples,) Predicted target values.</p>"},{"location":"api/ManyClassClassifier/","title":"ManyClassClassifier","text":""},{"location":"api/ManyClassClassifier/#scripts.estimator.ManyClassClassifier","title":"ManyClassClassifier","text":"<p>             Bases: <code>OutputCodeClassifier</code></p> <p>Output-Code multiclass strategy with deciary codebook.</p> <p>This class extends the original OutputCodeClassifier to support n-ary codebooks (with n=alphabet_size), allowing for handling more classes.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>estimator object An estimator object implementing :term:<code>fit</code> and one of :term:<code>decision_function</code> or :term:<code>predict_proba</code>. The base classifier should be able to handle up to <code>alphabet_size</code> classes.</p> required <code>random_state</code> <p>int, RandomState instance, default=None The generator used to initialize the codebook. Pass an int for reproducible output across multiple function calls. See :term:<code>Glossary &lt;random_state&gt;</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimators_</code> <p>list of <code>int(n_classes * code_size)</code> estimators Estimators used for predictions.</p> <code>classes_</code> <p>ndarray of shape (n_classes,) Array containing labels.</p> <code>code_book_</code> <p>ndarray of shape (n_classes, <code>len(estimators_)</code>) Deciary array containing the code of each class.</p> Example<pre><code>&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from tabpfn.scripts.estimator import ManyClassClassifier, TabPFNClassifier\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; x, y = load_iris(return_X_y=True)\n&gt;&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n&gt;&gt;&gt; clf = TabPFNClassifier()\n&gt;&gt;&gt; clf = ManyClassClassifier(clf, alphabet_size=clf.max_num_classes_)\n&gt;&gt;&gt; clf.fit(x_train, y_train)\n&gt;&gt;&gt; clf.predict(x_test)\n</code></pre>"},{"location":"api/ManyClassClassifier/#scripts.estimator.ManyClassClassifier.fit","title":"fit","text":"<pre><code>fit(X, y, **fit_params)\n</code></pre> <p>Fit underlying estimators.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>{array-like, sparse matrix} of shape (n_samples, n_features) Data.</p> required <code>y</code> <p>array-like of shape (n_samples,) Multi-class targets.</p> required <code>**fit_params</code> <p>dict Parameters passed to the <code>estimator.fit</code> method of each sub-estimator.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <p>object Returns a fitted instance of self.</p>"},{"location":"api/ManyClassClassifier/#scripts.estimator.ManyClassClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict probabilities using the underlying estimators.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>{array-like, sparse matrix} of shape (n_samples, n_features) Data.</p> required <p>Returns:</p> Name Type Description <code>p</code> <p>ndarray of shape (n_samples, n_classes) Returns the probability of the samples for each class in the model, where classes are ordered as they are in <code>self.classes_</code>.</p>"},{"location":"api/PreprocessorConfig/","title":"PreprocessorConfig","text":""},{"location":"api/PreprocessorConfig/#scripts.estimator.PreprocessorConfig","title":"PreprocessorConfig  <code>dataclass</code>","text":"<p>Configuration for data preprocessors.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal</code> <p>Name of the preprocessor.</p> <code>categorical_name</code> <code>Literal</code> <p>Name of the categorical encoding method. Valid options are \"none\", \"numeric\",                     \"onehot\", \"ordinal\", \"ordinal_shuffled\". Default is \"none\".</p> <code>append_original</code> <code>bool</code> <p>Whether to append the original features to the transformed features. Default is False.</p> <code>subsample_features</code> <code>float</code> <p>Fraction of features to subsample. -1 means no subsampling. Default is -1.</p> <code>global_transformer_name</code> <code>str</code> <p>Name of the global transformer to use. Default is None.</p>"},{"location":"api/TabPFNUnsupervisedModel/","title":"TabPFNUnsupervisedModel","text":""},{"location":"api/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel","title":"TabPFNUnsupervisedModel","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>TabPFN unsupervised model for imputation, outlier detection, and synthetic data generation.</p> <p>This model combines a TabPFNClassifier for categorical features and a TabPFNRegressor for numerical features to perform various unsupervised learning tasks on tabular data.</p> <p>Parameters:</p> Name Type Description Default <code>tabpfn_clf</code> <p>TabPFNClassifier, optional TabPFNClassifier instance for handling categorical features. If not provided, the model assumes that there are no categorical features in the data.</p> <code>None</code> <code>tabpfn_reg</code> <p>TabPFNRegressor, optional TabPFNRegressor instance for handling numerical features. If not provided, the model assumes that there are no numerical features in the data.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>categorical_features</code> <p>list List of indices of categorical features in the input data.</p> Example<pre><code>&gt;&gt;&gt; tabpfn_clf = TabPFNClassifier()\n&gt;&gt;&gt; tabpfn_reg = TabPFNRegressor()\n&gt;&gt;&gt; model = TabPFNUnsupervisedModel(tabpfn_clf, tabpfn_reg)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\n&gt;&gt;&gt; model.fit(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_imputed = model.impute(X)\n&gt;&gt;&gt; X_outliers = model.outliers(X)\n&gt;&gt;&gt; X_synthetic = model.generate_synthetic_data(n_samples=100)\n</code></pre>"},{"location":"api/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.__init__","title":"__init__","text":"<pre><code>__init__(\n    tabpfn_clf: Optional[TabPFNClassifier] = None,\n    tabpfn_reg: Optional[TabPFNRegressor] = None,\n) -&gt; None\n</code></pre> <p>Initialize the TabPFNUnsupervisedModel.</p> <p>Parameters:</p> Name Type Description Default <code>tabpfn_clf</code> <p>TabPFNClassifier, optional TabPFNClassifier instance for handling categorical features. If not provided, the model assumes that there are no categorical features in the data.</p> <code>None</code> <code>tabpfn_reg</code> <p>TabPFNRegressor, optional TabPFNRegressor instance for handling numerical features. If not provided, the model assumes that there are no numerical features in the data.</p> <code>None</code>"},{"location":"api/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.fit","title":"fit","text":"<pre><code>fit(X: ndarray, y: Optional[ndarray] = None) -&gt; None\n</code></pre> <p>Fit the model to the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Input data to fit the model.</p> required <code>y</code> <p>array-like of shape (n_samples,), optional Target values.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>None</code> <p>TabPFNUnsupervisedModel Fitted model.</p>"},{"location":"api/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features)\n</code></pre>"},{"location":"api/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.impute","title":"impute","text":"<pre><code>impute(\n    X: tensor, t: float = 1e-09, n_permutations: int = 10\n) -&gt; tensor\n</code></pre> <p>Impute missing values in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>torch.Tensor of shape (n_samples, n_features) Input data with missing values encoded as np.nan.</p> required <code>t</code> <p>float, default=0.000000001 Temperature for sampling from the imputation distribution. Lower values result in more deterministic imputations.</p> <code>1e-09</code> <p>Returns:</p> Type Description <code>tensor</code> <p>torch.Tensor of shape (n_samples, n_features) Imputed data with missing values replaced.</p>"},{"location":"api/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    X: tensor, per_column: bool = False\n) -&gt; tensor\n</code></pre> <p>Get the transformer embeddings for the test data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>tensor</code> required <p>Returns:</p> Type Description <code>tensor</code> <p>torch.Tensor of shape (n_samples, embedding_dim)</p>"},{"location":"api/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.outliers","title":"outliers","text":"<pre><code>outliers(X: tensor, n_permutations: int = 10) -&gt; tensor\n</code></pre> <p>Preferred implementation for outliers, where we calculate the sample probability for each sample in X by multiplying the probabilities of each feature according to chain rule of probability. The first feature is estimated by using a zero feature as input.</p> <p>Args     X: Samples to calculate the sample probability for, shape (n_samples, n_features)</p> <p>Returns:</p> Type Description <code>tensor</code> <p>Sample unnormalized probability for each sample in X, shape (n_samples,)</p>"},{"location":"api/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.generate_synthetic_data","title":"generate_synthetic_data","text":"<pre><code>generate_synthetic_data(\n    n_samples=100, t=1.0, n_permutations=3\n)\n</code></pre> <p>Generate synthetic data using the trained models. Uses imputation method to generate synthetic data, passed with a matrix of nans. Samples are generated feature by feature in one pass, so samples are not dependent on each other per feature.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <p>int, default=100 Number of synthetic samples to generate.</p> <code>100</code> <code>t</code> <p>float, default=1.0 Temperature for sampling from the imputation distribution. Lower values result in more deterministic samples.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>torch.Tensor of shape (n_samples, n_features) Generated synthetic data.</p>"},{"location":"api/model.encoders/","title":"Encoders","text":""},{"location":"api/model.encoders/#model.encoders.InputEncoder","title":"InputEncoder","text":"<p>             Bases: <code>Module</code></p> <p>Base class for input encoders.</p> <p>All input encoders should subclass this class and implement the <code>forward</code> method.</p>"},{"location":"api/model.encoders/#model.encoders.InputEncoder.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre>"},{"location":"api/model.encoders/#model.encoders.InputEncoder.forward","title":"forward","text":"<pre><code>forward(x: Tensor, single_eval_pos: int) -&gt; Tensor\n</code></pre> <p>Encode the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to encode.</p> required <code>single_eval_pos</code> <code>int</code> <p>The position to use for single evaluation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The encoded tensor.</p>"},{"location":"api/model.encoders/#model.encoders.SequentialEncoder","title":"SequentialEncoder","text":"<p>             Bases: <code>Sequential</code>, <code>InputEncoder</code></p> <p>An encoder that applies a sequence of encoder steps.</p> <p>SequentialEncoder allows building an encoder from a sequence of EncoderSteps. The input is passed through each step in the provided order.</p>"},{"location":"api/model.encoders/#model.encoders.SequentialEncoder.__init__","title":"__init__","text":"<pre><code>__init__(*args, output_key: str = 'output', **kwargs)\n</code></pre> <p>Initialize the SequentialEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>A list of SeqEncStep instances to apply in order.</p> <code>()</code> <code>output_key</code> <code>str</code> <p>The key to use for the output of the encoder in the state dict.               Defaults to \"output\", i.e. <code>state[\"output\"]</code> will be returned.</p> <code>'output'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>torch.nn.Sequential</code>.</p> <code>{}</code>"},{"location":"api/model.encoders/#model.encoders.SequentialEncoder.forward","title":"forward","text":"<pre><code>forward(input: dict, **kwargs) -&gt; Tensor\n</code></pre> <p>Apply the sequence of encoder steps to the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>dict</code> <p>The input state dictionary.           If the input is not a dict and the first layer expects one input key,           the input tensor is mapped to the key expected by the first layer.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to each encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output of the final encoder step.</p>"},{"location":"api/model.encoders/#model.encoders.LinearInputEncoderStep","title":"LinearInputEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>A simple linear input encoder step.</p>"},{"location":"api/model.encoders/#model.encoders.LinearInputEncoderStep.__init__","title":"__init__","text":"<pre><code>__init__(\n    num_features: int,\n    emsize: int,\n    replace_nan_by_zero: bool = False,\n    bias: bool = True,\n    in_keys: tuple[str] = (\"main\"),\n    out_keys: tuple[str] = (\"output\"),\n)\n</code></pre> <p>Initialize the LinearInputEncoderStep.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>The number of input features.</p> required <code>emsize</code> <code>int</code> <p>The embedding size, i.e. the number of output features.</p> required <code>replace_nan_by_zero</code> <code>bool</code> <p>Whether to replace NaN values in the input by zero. Defaults to False.</p> <code>False</code> <code>bias</code> <code>bool</code> <p>Whether to use a bias term in the linear layer. Defaults to True.</p> <code>True</code> <code>in_keys</code> <code>tuple[str]</code> <p>The keys of the input tensors. Defaults to (\"main\",).</p> <code>('main')</code> <code>out_keys</code> <code>tuple[str]</code> <p>The keys to assign the output tensors to. Defaults to (\"output\",).</p> <code>('output')</code>"},{"location":"api/model.encoders/#model.encoders.LinearInputEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs\n)\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation.                                   Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"api/model.encoders/#model.encoders.NanHandlingEncoderStep","title":"NanHandlingEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to handle NaN and infinite values in the input.</p>"},{"location":"api/model.encoders/#model.encoders.NanHandlingEncoderStep.__init__","title":"__init__","text":"<pre><code>__init__(\n    keep_nans: bool = True,\n    in_keys: tuple[str] = (\"main\"),\n    out_keys: tuple[str] = (\"main\", \"nan_indicators\"),\n)\n</code></pre> <p>Initialize the NanHandlingEncoderStep.</p> <p>Parameters:</p> Name Type Description Default <code>keep_nans</code> <code>bool</code> <p>Whether to keep NaN values as separate indicators. Defaults to True.</p> <code>True</code> <code>in_keys</code> <code>tuple[str]</code> <p>The keys of the input tensors. Must be a single key.</p> <code>('main')</code> <code>out_keys</code> <code>tuple[str]</code> <p>The keys to assign the output tensors to.                    Defaults to (\"main\", \"nan_indicators\").</p> <code>('main', 'nan_indicators')</code>"},{"location":"api/model.encoders/#model.encoders.NanHandlingEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs\n)\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation.                                   Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"api/model.mlp/","title":"MLP","text":""},{"location":"api/model.mlp/#model.mlp.MLP","title":"MLP","text":"<p>             Bases: <code>Module</code></p> <p>Multi-Layer Perceptron (MLP) module.</p> <p>This module consists of two linear layers with an activation function in between. It supports various configurations such as the hidden size, activation function, initializing the output to zero, and recomputing the forward pass during backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The input and output size of the MLP.</p> required <code>hidden_size</code> <code>int</code> <p>The size of the hidden layer.</p> required <code>activation</code> <code>Union[Activation, str]</code> <p>The activation function to use. Can be either an Activation enum or a string representing the activation name.</p> required <code>device</code> <code>device</code> <p>The device to use for the linear layers.</p> required <code>dtype</code> <code>dtype</code> <p>The data type to use for the linear layers.</p> required <code>initialize_output_to_zero</code> <code>bool</code> <p>Whether to initialize the output layer weights to zero. Default is False.</p> <code>False</code> <code>recompute</code> <code>bool</code> <p>Whether to recompute the forward pass during backpropagation. This can save memory but increase computation time. Default is False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>linear1</code> <code>Linear</code> <p>The first linear layer.</p> <code>linear2</code> <code>Linear</code> <p>The second linear layer.</p> <code>activation</code> <code>Activation</code> <p>The activation function to use.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Performs the forward pass of the MLP. - x (torch.Tensor): The input tensor. - add_input (bool): Whether to add the input to the output. Default is False. - allow_inplace (bool): Indicates that 'x' is not used after the call and its buffer     can be reused for the output. The operation is not guaranteed to be inplace.     Default is False. - save_peak_mem_factor (Optional[int]): If provided, enables a memory-saving technique     that reduces peak memory usage during the forward pass. This requires 'add_input'     and 'allow_inplace' to be True. See the documentation of the decorator     'support_save_peak_mem_factor' for details. Default is None.</p> Example<pre><code>&gt;&gt;&gt; mlp = MLP(size=128, hidden_size=256, activation='gelu', device='cuda', dtype=torch.float32)\n&gt;&gt;&gt; x = torch.randn(32, 128, device='cuda', dtype=torch.float32)\n&gt;&gt;&gt; output = mlp(x)\n</code></pre>"},{"location":"api/model.mlp/#model.mlp.MLP.__init__","title":"__init__","text":"<pre><code>__init__(\n    size: int,\n    hidden_size: int,\n    activation: Union[Activation, str],\n    device,\n    dtype,\n    initialize_output_to_zero: bool = False,\n    recompute: bool = False,\n)\n</code></pre>"},{"location":"api/model.mlp/#model.mlp.MLP.forward","title":"forward","text":"<pre><code>forward(\n    x: Tensor,\n    add_input: bool = False,\n    allow_inplace: bool = False,\n    save_peak_mem_factor: Optional[int] = None,\n) -&gt; Tensor\n</code></pre>"},{"location":"api/model.transformer/","title":"Transformer","text":""},{"location":"api/model.transformer/#model.transformer.PerFeatureTransformer","title":"PerFeatureTransformer","text":"<p>             Bases: <code>Module</code></p> <p>A Transformer model processes a token per feature and sample.</p> <p>This model extends the standard Transformer architecture to operate on a per-feature basis. It allows for processing each feature separately while still leveraging the power of self-attention.</p> <p>The model consists of an encoder, decoder, and optional components such as a feature positional embedding and a separate decoder for each feature.</p>"},{"location":"api/model.transformer/#model.transformer.PerFeatureTransformer.__init__","title":"__init__","text":"<pre><code>__init__(\n    encoder: Module = encoders.SequentialEncoder(\n        encoders.LinearInputEncoderStep(\n            1,\n            DEFAULT_EMSIZE,\n            in_keys=[\"main\"],\n            out_keys=[\"output\"],\n        )\n    ),\n    ninp: int = DEFAULT_EMSIZE,\n    nhead: int = 4,\n    nhid: int = DEFAULT_EMSIZE * 4,\n    nlayers: int = 10,\n    y_encoder: Module = encoders.SequentialEncoder(\n        encoders.NanHandlingEncoderStep(),\n        encoders.LinearInputEncoderStep(\n            2,\n            DEFAULT_EMSIZE,\n            out_keys=[\"output\"],\n            in_keys=[\"main\", \"nan_indicators\"],\n        ),\n    ),\n    decoder_dict: Dict[\n        str, Tuple[Optional[Type[Module]], int]\n    ] = {\"standard\": (None, 1)},\n    init_method: Optional[str] = None,\n    activation: str = \"gelu\",\n    recompute_layer: bool = False,\n    min_num_layers_layer_dropout: Optional[int] = None,\n    repeat_same_layer: bool = False,\n    dag_pos_enc_dim: int = 0,\n    features_per_group: int = 1,\n    feature_positional_embedding: Optional[str] = None,\n    zero_init: bool = True,\n    use_separate_decoder: bool = False,\n    nlayers_decoder: Optional[int] = None,\n    use_encoder_compression_layer: bool = False,\n    precomputed_kv: Optional[\n        List[Union[Tensor, Tuple[Tensor, Tensor]]]\n    ] = None,\n    cache_trainset_representation: bool = False,\n    **layer_kwargs: Any\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Module</code> <p>Pass a nn.Module that takes in a batch of sequences of inputs and returns something of the shape (seq_len, batch_size, ninp)</p> <code>SequentialEncoder(LinearInputEncoderStep(1, DEFAULT_EMSIZE, in_keys=['main'], out_keys=['output']))</code> <code>ninp</code> <code>int</code> <p>Input dimension, also called the embedding dimension</p> <code>DEFAULT_EMSIZE</code> <code>nhead</code> <code>int</code> <p>Number of attention heads</p> <code>4</code> <code>nhid</code> <code>int</code> <p>Hidden dimension in the MLP layers</p> <code>DEFAULT_EMSIZE * 4</code> <code>nlayers</code> <code>int</code> <p>Number of layers, each consisting of a multi-head attention layer and an MLP layer</p> <code>10</code> <code>y_encoder</code> <code>Module</code> <p>A nn.Module that takes in a batch of sequences of outputs and returns something of the shape (seq_len, batch_size, ninp)</p> <code>SequentialEncoder(NanHandlingEncoderStep(), LinearInputEncoderStep(2, DEFAULT_EMSIZE, out_keys=['output'], in_keys=['main', 'nan_indicators']))</code> <code>decoder_dict</code> <code>Dict[str, Tuple[Optional[Type[Module]], int]]</code> <code>{'standard': (None, 1)}</code> <code>activation</code> <code>str</code> <p>An activation function, e.g. \"gelu\" or \"relu\"</p> <code>'gelu'</code> <code>recompute_layer</code> <code>bool</code> <p>If True, the transformer layers will be recomputed on each forward pass in training. This is useful to save memory.</p> <code>False</code> <code>min_num_layers_layer_dropout</code> <code>Optional[int]</code> <p>if this is set, it enables to drop the last layers randomly during training up to this number.</p> <code>None</code> <code>repeat_same_layer</code> <code>bool</code> <p>If True, the same layer will be used for all layers. This is useful to save memory on weights.</p> <code>False</code> <code>features_per_group</code> <code>int</code> <p>If &gt; 1, the features will be grouped into groups of this size and the attention is across groups.</p> <code>1</code> <code>feature_positional_embedding</code> <code>Optional[str]</code> <p>There is a risk that our models confuse features with each other. This positional embedding is added to the features to help the model distinguish them. We recommend setting this to \"subspace\".</p> <code>None</code> <code>zero_init</code> <code>bool</code> <p>If True, the last sublayer of each attention and MLP layer will be initialized with zeros. Thus, the layers will start out as identity functions.</p> <code>True</code> <code>use_separate_decoder</code> <code>bool</code> <p>If True, the decoder will be separate from the encoder.</p> <code>False</code> <code>nlayers_decoder</code> <code>Optional[int]</code> <p>If use_separate_decoder is True, this is the number of layers in the decoder. The default is to use \u2153 of the layers for the decoder and \u2154 for the encoder.</p> <code>None</code> <code>use_encoder_compression_layer</code> <code>bool</code> <p>Experimental</p> <code>False</code> <code>precomputed_kv</code> <code>Optional[List[Union[Tensor, Tuple[Tensor, Tensor]]]]</code> <p>Experimental</p> <code>None</code> <code>layer_kwargs</code> <code>Any</code> <code>{}</code>"},{"location":"api/model.transformer/#model.transformer.PerFeatureTransformer.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Performs a forward pass through the model.</p> <p>This method supports multiple calling conventions: - model(train_x, train_y, test_x, **kwargs) - model((x,y), **kwargs) - model((style,x,y), **kwargs)</p> <p>Parameters:</p> Name Type Description Default <code>train_x</code> <p>The input data for the training set.</p> required <code>train_y</code> <p>The target data for the training set.</p> required <code>test_x</code> <p>The input data for the test set.</p> required <code>x</code> <p>The input data.</p> required <code>y</code> <p>The target data.</p> required <code>style</code> <p>The style vector.</p> required <code>single_eval_pos</code> <p>The position to evaluate at.</p> required <code>only_return_standard_out</code> <p>Whether to only return the standard output.</p> required <code>data_dags</code> <p>The data DAGs for each example.</p> required <code>categorical_inds</code> <p>The indices of categorical features.</p> required <code>freeze_kv</code> <p>Whether to freeze the key and value weights.</p> required <p>Returns:</p> Type Description <p>The output of the model, which can be a tensor or a dictionary of tensors.</p>"},{"location":"api/model.transformer/#model.layer.PerFeatureEncoderLayer","title":"PerFeatureEncoderLayer","text":"<p>             Bases: <code>Module</code></p> <p>Transformer encoder layer that processes each feature block separately.</p> <p>This layer consists of multi-head attention between features, multi-head attention between items, and feedforward neural networks (MLPs). It supports various configurations and optimization options.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The dimensionality of the input and output embeddings.</p> required <code>nhead</code> <code>int</code> <p>The number of attention heads.</p> required <code>dim_feedforward</code> <code>Optional[int]</code> <p>The dimensionality of the feedforward network. Default is None (2 * d_model).</p> <code>None</code> <code>activation</code> <code>str</code> <p>The activation function to use in the MLPs. Default is \"relu\".</p> <code>'relu'</code> <code>layer_norm_eps</code> <code>float</code> <p>The epsilon value for layer normalization. Default is 1e-5.</p> <code>1e-05</code> <code>pre_norm</code> <code>bool</code> <p>Whether to apply layer normalization before or after the attention and MLPs. Default is False.</p> <code>False</code> <code>device</code> <code>Optional[device]</code> <p>The device to use for the layer parameters. Default is None.</p> <code>None</code> <code>dtype</code> <code>Optional[dtype]</code> <p>The data type to use for the layer parameters. Default is None.</p> <code>None</code> <code>recompute_attn</code> <code>bool</code> <p>Whether to recompute attention during backpropagation. Default is False.</p> <code>False</code> <code>second_mlp</code> <code>bool</code> <p>Whether to include a second MLP in the layer. Default is False.</p> <code>False</code> <code>layer_norm_with_elementwise_affine</code> <code>bool</code> <p>Whether to use elementwise affine parameters in layer normalization. Default is False.</p> <code>False</code> <code>zero_init</code> <code>bool</code> <p>Whether to initialize the output of the MLPs to zero. Default is False.</p> <code>False</code> <code>save_peak_mem_factor</code> <code>Optional[int]</code> <p>The factor to save peak memory, only effective with post-norm. Default is None.</p> <code>None</code> <code>attention_between_features</code> <code>bool</code> <p>Whether to apply attention between feature blocks. Default is True.</p> <code>True</code> <code>multiquery_item_attention</code> <code>bool</code> <p>Whether to use multiquery attention for items. Default is False.</p> <code>False</code> <code>multiquery_item_attention_for_test_set</code> <code>bool</code> <p>Whether to use multiquery attention for the test set. Default is False.</p> <code>False</code> <code>attention_init_gain</code> <code>float</code> <p>The gain value for initializing attention parameters. Default is 1.0.</p> <code>1.0</code> <code>d_k</code> <code>Optional[int]</code> <p>The dimensionality of the query and key vectors. Default is None (d_model // nhead).</p> <code>None</code> <code>d_v</code> <code>Optional[int]</code> <p>The dimensionality of the value vectors. Default is None (d_model // nhead).</p> <code>None</code> <code>precomputed_kv</code> <code>Union[None, Tensor, Tuple[Tensor, Tensor]]</code> <p>Precomputed key-value pairs for attention. Default is None.</p> <code>None</code>"},{"location":"api/model.transformer/#model.layer.PerFeatureEncoderLayer.__init__","title":"__init__","text":"<pre><code>__init__(\n    d_model: int,\n    nhead: int,\n    dim_feedforward: Optional[int] = None,\n    activation: str = \"relu\",\n    layer_norm_eps: float = 1e-05,\n    pre_norm: bool = False,\n    device: Optional[device] = None,\n    dtype: Optional[dtype] = None,\n    recompute_attn: bool = False,\n    second_mlp: bool = False,\n    layer_norm_with_elementwise_affine: bool = False,\n    zero_init: bool = False,\n    save_peak_mem_factor: Optional[int] = None,\n    attention_between_features: bool = True,\n    multiquery_item_attention: bool = False,\n    multiquery_item_attention_for_test_set: bool = False,\n    two_sets_of_queries: bool = False,\n    attention_init_gain: float = 1.0,\n    d_k: Optional[int] = None,\n    d_v: Optional[int] = None,\n    precomputed_kv: Union[\n        None, Tensor, Tuple[Tensor, Tensor]\n    ] = None,\n) -&gt; None\n</code></pre>"},{"location":"api/model.transformer/#model.layer.PerFeatureEncoderLayer.forward","title":"forward","text":"<pre><code>forward(\n    state: Tensor,\n    single_eval_pos: Optional[int] = None,\n    cache_trainset_representation: bool = False,\n    att_src: Optional[Tensor] = None,\n) -&gt; Tensor\n</code></pre> <p>Pass the input through the encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Tensor</code> <p>The transformer state passed as input to the layer of shape (batch_size, num_items, num_feature_blocks, d_model).</p> required <code>single_eval_pos</code> <code>Optional[int]</code> <p>The position from which on everything is treated as test set. Default is None.</p> <code>None</code> <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the trainset representation. If single_eval_pos is set (&gt; 0 and not None), create a cache of the trainset KV. This may require a lot of memory. Otherwise, use cached KV representations for inference. Default is False.</p> <code>False</code> <code>att_src</code> <code>Optional[Tensor]</code> <p>The tensor to attend to from the final layer of the encoder. It has a shape of (batch_size, num_train_items, num_feature_blocks, d_model). This does not work with multiquery_item_attention_for_test_set and cache_trainset_representation at this point. Combining would be possible, however. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The transformer state passed through the encoder layer.</p>"},{"location":"api/tabpfn_classifier/","title":"TabPFNClassifier","text":""},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier","title":"TabPFNClassifier","text":"<p>             Bases: <code>TabPFNBaseModel</code>, <code>ClassifierMixin</code></p>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_path: str | Path = Path(local_model_path)\n    / \"model_hans_classification.ckpt\",\n    n_estimators: int = 4,\n    preprocess_transforms: Tuple[\n        PreprocessorConfig, ...\n    ] = (\n        PreprocessorConfig(\n            \"quantile_uni_coarse\",\n            append_original=True,\n            categorical_name=\"ordinal_very_common_categories_shuffled\",\n            global_transformer_name=\"svd\",\n            subsample_features=-1,\n        ),\n        PreprocessorConfig(\n            \"none\",\n            categorical_name=\"numeric\",\n            subsample_features=-1,\n        ),\n    ),\n    feature_shift_decoder: str = \"shuffle\",\n    normalize_with_test: bool = False,\n    average_logits: bool = False,\n    optimize_metric: ClassificationOptimizationMetricType = \"roc\",\n    transformer_predict_kwargs: Optional[Dict] = None,\n    multiclass_decoder=\"shuffle\",\n    softmax_temperature: Optional[float] = math.exp(-0.1),\n    use_poly_features: bool = False,\n    max_poly_features: int = 50,\n    transductive: bool = False,\n    remove_outliers: float = 12.0,\n    add_fingerprint_features: bool = True,\n    subsample_samples: float = -1,\n    model: Optional[Module] = None,\n    model_config: Optional[Dict] = None,\n    fit_at_predict_time: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n    seed: Optional[int] = 0,\n    show_progress: bool = True,\n    batch_size_inference: int = 1,\n    fp16_inference: bool = True,\n    save_peak_memory: Literal[\n        \"True\", \"False\", \"auto\"\n    ] = \"True\",\n    maximum_free_memory_in_gb: Optional[float] = None,\n    split_test_samples: float | str = 1,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str | Path</code> <p>The model string is the path to the model.</p> <code>Path(local_model_path) / 'model_hans_classification.ckpt'</code> <code>n_estimators</code> <code>int</code> <p>The number of ensemble configurations to use, the most important setting.</p> <code>4</code> <code>preprocess_transforms</code> <code>Tuple[PreprocessorConfig, ...]</code> <p>A tuple of strings, specifying the preprocessing steps to use. You can use the following strings as elements '(none|power|quantile|robust)_all', where the first part specifies the preprocessing step and the second part specifies the features to apply it to and finally '_and_none' specifies that the original features should be added back to the features in plain. Finally, you can combine all strings without <code>_all</code> with <code>_onehot</code> to apply one-hot encoding to the categorical features specified with <code>self.fit(..., categorical_features=...)</code>.</p> <code>(PreprocessorConfig('quantile_uni_coarse', append_original=True, categorical_name='ordinal_very_common_categories_shuffled', global_transformer_name='svd', subsample_features=-1), PreprocessorConfig('none', categorical_name='numeric', subsample_features=-1))</code> <code>feature_shift_decoder</code> <code>str</code> <p>[\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.</p> <code>'shuffle'</code> <code>normalize_with_test</code> <code>bool</code> <p>If True, the test set is used to normalize the data, otherwise the training set is used only.</p> <code>False</code> <code>average_logits</code> <code>bool</code> <p>Whether to average logits or probabilities for ensemble members.</p> <code>False</code> <code>optimize_metric</code> <code>ClassificationOptimizationMetricType</code> <p>The optimization metric to use.</p> <code>'roc'</code> <code>transformer_predict_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments to pass to the transformer predict method.</p> <code>None</code> <code>multiclass_decoder</code> <p>The multiclass decoder to use.</p> <code>'shuffle'</code> <code>softmax_temperature</code> <code>Optional[float]</code> <p>A log spaced temperature, it will be applied as logits &lt;- logits/softmax_temperature.</p> <code>exp(-0.1)</code> <code>use_poly_features</code> <code>bool</code> <p>Whether to use polynomial features as the last preprocessing step.</p> <code>False</code> <code>max_poly_features</code> <code>int</code> <p>Maximum number of polynomial features to use, None means unlimited.</p> <code>50</code> <code>transductive</code> <code>bool</code> <p>Whether to use transductive learning.</p> <code>False</code> <code>remove_outliers</code> <code>float</code> <p>If not 0.0, will remove outliers from the input features, where values with a standard deviation larger than remove_outliers will be removed.</p> <code>12.0</code> <code>add_fingerprint_features</code> <code>bool</code> <p>If True, will add one feature of random values, that will be added to the input features. This helps discern duplicated samples in the transformer model.</p> <code>True</code> <code>subsample_samples</code> <code>float</code> <p>If not None, will use a random subset of the samples for training in each ensemble configuration. If 1 or above, this will subsample to the specified number of samples. If in 0 to 1, the value is viewed as a fraction of the training set size.</p> <code>-1</code> <code>model</code> <code>Optional[Module]</code> <p>The model, if you want to specify it directly, this is used in combination with model_config.</p> <code>None</code> <code>model_config</code> <code>Optional[Dict]</code> <p>The config, if you want to specify it directly, this is used in combination with model.</p> <code>None</code> <code>fit_at_predict_time</code> <code>bool</code> <p>Whether to train the model lazily, i.e. only when it is needed for inference in predict[_proba].</p> <code>True</code> <code>device</code> <code>Literal['cuda', 'cpu', 'auto']</code> <p>The device to use for inference, \"auto\" means that it will use cuda if available, otherwise cpu.</p> <code>'auto'</code> <code>seed</code> <code>Optional[int]</code> <p>The default seed to use for the order of the ensemble configurations, a seed of None will not.</p> <code>0</code> <code>show_progress</code> <code>bool</code> <p>Whether to show progress bars during training and inference.</p> <code>True</code> <code>batch_size_inference</code> <code>int</code> <p>The batch size to use for inference, this does not affect the results, just the memory usage and speed. A higher batch size is faster but uses more memory. Setting the batch size to None means that the batch size is automatically determined based on the memory usage and the maximum free memory specified with <code>maximum_free_memory_in_gb</code>.</p> <code>1</code> <code>fp16_inference</code> <code>bool</code> <p>Whether to use fp16 for inference on GPU, does not affect CPU inference.</p> <code>True</code> <code>save_peak_memory</code> <code>Literal['True', 'False', 'auto']</code> <p>Whether to save the peak memory usage of the model, can enable up to 8 times larger datasets to fit into memory. \"True\", means always enabled, \"False\", means always disabled, \"auto\" means that it will be set based on the memory usage.</p> <code>'True'</code>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.fit","title":"fit","text":"<pre><code>fit(\n    X: Union[ndarray, Tensor],\n    y: Union[ndarray, Tensor],\n    additional_y: Optional[Dict[str, Tensor]] = None,\n) -&gt; TabPFNClassifier\n</code></pre> <p>Fits the TabPFNClassifier model to the input data <code>X</code> and <code>y</code>.</p> <p>The actual training logic is delegated to the <code>_fit</code> method, which should be implemented by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, Tensor]</code> <p>The input feature matrix of shape (n_samples, n_features).</p> required <code>y</code> <code>Union[ndarray, Tensor]</code> <p>The target labels of shape (n_samples,).</p> required <code>additional_y</code> <code>Optional[Dict[str, Tensor]]</code> <p>Additional labels to use during training.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TabPFNClassifier</code> <code>TabPFNClassifier</code> <p>The fitted model object (self).</p>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X, return_winning_probability=False)\n</code></pre> <p>Predict the class labels for the input samples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The input samples.</p> required <code>return_winning_probability</code> <code>bool</code> <p>Whether to return the winning probability.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>array</code> <p>The predicted class labels.</p>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X, additional_y=None)\n</code></pre> <p>Calls the transformer to predict the probabilities of the classes of the X test inputs given the previous set training dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>test datapoints</p> required"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.predict_y_proba","title":"predict_y_proba","text":"<pre><code>predict_y_proba(X, y)\n</code></pre> <p>Predict the probability of the target labels <code>y</code> given the input samples <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The input samples.</p> required <code>y</code> <code>array - like</code> <p>The target labels.</p> required <p>Returns:</p> Name Type Description <code>array</code> <p>The predicted probabilities of the target labels.</p>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features: List[int])\n</code></pre> <p>Set the categorical features to use for the model.</p> <p>These categorical features might be overridden by the preprocessing steps. This is controlled by i) <code>max_unique_values_as_categorical_feature</code>, the maximum number of unique values a feature can have to be considered a categorical feature. Features with more unique values are considered numerical features. ii) <code>min_unique_values_as_numerical_feature</code> the minimum number of unique values a feature can have to be considered a numerical feature. Features with less unique values are considered categorical features.</p> <p>:param categorical_features: The feature indices of the categorical features</p>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.score","title":"score","text":"<pre><code>score(X, y, sample_weight=None)\n</code></pre> <p>Compute the score of the model on the given test data and labels.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The input samples.</p> required <code>y</code> <code>array - like</code> <p>The true labels for <code>X</code>.</p> required <code>sample_weight</code> <code>array - like</code> <p>Sample weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The computed score.</p>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.estimate_memory_usage","title":"estimate_memory_usage","text":"<pre><code>estimate_memory_usage(\n    X: ndarray | tensor,\n    unit: Literal[\"b\", \"mb\", \"gb\"] = \"gb\",\n    eval_position: int = -1,\n    **overwrite_params\n) -&gt; float | None\n</code></pre> <p>Estimates the memory usage of the model.</p> <p>Peak memory usage is accurate for \u00b4save_peak_mem_factor\u00b4 in O(n_feats, n_samples) on average but with significant outliers (2x). Also this calculation does not include baseline usage and constant offsets. Baseline memory usage can be ignored if we set the maximum memory usage to the default None which uses the free memory of the system. The constant offsets are not significant for large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if <code>self.fit_at_predict_time</code> and train only otherwise. If you add a batch dimension at position 1 to the table this is used as the batch size used during inference, otherwise this depends on the <code>batch_size_inference</code> and <code>n_estimators</code>.</p> required <code>unit</code> <code>Literal['b', 'mb', 'gb']</code> <p>The unit to return the memory usage in (bytes, megabytes, or gigabytes).</p> <code>'gb'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated memory usage in bytes.</p>"},{"location":"api/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.estimate_computation_usage","title":"estimate_computation_usage","text":"<pre><code>estimate_computation_usage(\n    X: ndarray,\n    unit: Literal[\n        \"sequential_flops\", \"s\"\n    ] = \"sequential_flops\",\n    eval_position: int = -1,\n    **overwrite_params\n) -&gt; float | None\n</code></pre> <p>Estimates the sequential computation usage of the model. Those are the operations that are not parallelizable and are the main bottleneck for the computation time.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if</p> required <code>unit</code> <code>str</code> <p>The unit to return the computation usage in.</p> <code>'sequential_flops'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated computation usage in unit of choice.</p>"},{"location":"api/tabpfn_regressor/","title":"TabPFNRegressor","text":""},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor","title":"TabPFNRegressor","text":"<p>             Bases: <code>TabPFNBaseModel</code>, <code>RegressorMixin</code></p>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_path: str | Path = str(\n        Path(local_model_path).resolve()\n        / \"model_hans_regression.ckpt\"\n    ),\n    n_estimators: int = 8,\n    preprocess_transforms: Tuple[\n        PreprocessorConfig, ...\n    ] = (\n        PreprocessorConfig(\n            \"quantile_uni\",\n            append_original=True,\n            categorical_name=\"ordinal_very_common_categories_shuffled\",\n            global_transformer_name=\"svd\",\n        ),\n        PreprocessorConfig(\n            \"safepower\", categorical_name=\"onehot\"\n        ),\n    ),\n    feature_shift_decoder: str = \"shuffle\",\n    normalize_with_test: bool = False,\n    average_logits: bool = False,\n    optimize_metric: RegressionOptimizationMetricType = \"rmse\",\n    transformer_predict_kwargs: Optional[Dict] = None,\n    softmax_temperature: Optional[float] = math.exp(-0.1),\n    use_poly_features: bool = False,\n    max_poly_features: int = 50,\n    transductive: bool = False,\n    remove_outliers=-1,\n    regression_y_preprocess_transforms: Optional[\n        Tuple[\n            None\n            | Literal[\n                \"safepower\", \"power\", \"quantile_norm\"\n            ],\n            ...,\n        ]\n    ] = (None, \"safepower\"),\n    add_fingerprint_features: bool = True,\n    cancel_nan_borders: bool = True,\n    super_bar_dist_averaging: bool = False,\n    subsample_samples: float = -1,\n    model: Optional[Module] = None,\n    model_config: Optional[Dict] = None,\n    fit_at_predict_time: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n    seed: Optional[int] = 0,\n    show_progress: bool = True,\n    batch_size_inference: int = 1,\n    fp16_inference: bool = True,\n    save_peak_memory: Literal[\n        \"True\", \"False\", \"auto\"\n    ] = \"True\",\n    maximum_free_memory_in_gb: Optional[float] = None,\n    split_test_samples: float | str = 1,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str | Path</code> <p>The model string is the path to the model.</p> <code>str(resolve() / 'model_hans_regression.ckpt')</code> <code>n_estimators</code> <code>int</code> <p>The number of ensemble configurations to use, the most important setting.</p> <code>8</code> <code>preprocess_transforms</code> <code>Tuple[PreprocessorConfig, ...]</code> <p>A tuple of strings, specifying the preprocessing steps to use. You can use the following strings as elements '(none|power|quantile_norm|quantile_uni|quantile_uni_coarse|robust...)_all', where the first part specifies the preprocessing step (see <code>.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()</code>) and the second part specifies the features to apply it to and finally '_and_none' specifies that the original features should be added back to the features in plain. Finally, you can combine all strings without <code>_all</code> with <code>_onehot</code> to apply one-hot encoding to the categorical features specified with <code>self.fit(..., categorical_features=...)</code>.</p> <code>(PreprocessorConfig('quantile_uni', append_original=True, categorical_name='ordinal_very_common_categories_shuffled', global_transformer_name='svd'), PreprocessorConfig('safepower', categorical_name='onehot'))</code> <code>feature_shift_decoder</code> <code>str</code> <p>[\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.</p> <code>'shuffle'</code> <code>normalize_with_test</code> <code>bool</code> <p>If True, the test set is used to normalize the data, otherwise the training set is used only.</p> <code>False</code> <code>average_logits</code> <code>bool</code> <p>Whether to average logits or probabilities for ensemble members.</p> <code>False</code> <code>optimize_metric</code> <code>RegressionOptimizationMetricType</code> <p>The optimization metric to use.</p> <code>'rmse'</code> <code>transformer_predict_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments to pass to the transformer predict method.</p> <code>None</code> <code>softmax_temperature</code> <code>Optional[float]</code> <p>A log spaced temperature, it will be applied as logits &lt;- logits/softmax_temperature.</p> <code>exp(-0.1)</code> <code>use_poly_features</code> <code>bool</code> <p>Whether to use polynomial features as the last preprocessing step.</p> <code>False</code> <code>max_poly_features</code> <code>int</code> <p>Maximum number of polynomial features to use, None means unlimited.</p> <code>50</code> <code>transductive</code> <code>bool</code> <p>Whether to use transductive learning.</p> <code>False</code> <code>remove_outliers</code> <p>If not 0.0, will remove outliers from the input features, where values with a standard deviation larger than remove_outliers will be removed.</p> <code>-1</code> <code>regression_y_preprocess_transforms</code> <code>Optional[Tuple[None | Literal['safepower', 'power', 'quantile_norm'], ...]]</code> <p>Preprocessing transforms for the target variable. This can be one from <code>.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()</code>, e.g. \"power\". This can also be None to not transform the targets, beside a simple mean/variance normalization.</p> <code>(None, 'safepower')</code> <code>add_fingerprint_features</code> <code>bool</code> <p>If True, will add one feature of random values, that will be added to the input features. This helps discern duplicated samples in the transformer model.</p> <code>True</code> <code>cancel_nan_borders</code> <code>bool</code> <p>Whether to ignore buckets that are tranformed to nan values by inverting a <code>regression_y_preprocess_transform</code>. This should be set to True, only set this to False if you know what you are doing.</p> <code>True</code> <code>super_bar_dist_averaging</code> <code>bool</code> <p>If we use <code>regression_y_preprocess_transforms</code> we need to average the predictions over the different configurations. The different configurations all come with different bar_distributions (Riemann distributions), though. The default is for us to aggregate all bar distributions using simply scaled borders in the bar distribution, scaled by the mean and std of the target variable. If you set this to True, a new bar distribution will be built using all the borders generated in the different configurations.</p> <code>False</code> <code>subsample_samples</code> <code>float</code> <p>If not None, will use a random subset of the samples for training in each ensemble configuration. If 1 or above, this will subsample to the specified number of samples. If in 0 to 1, the value is viewed as a fraction of the training set size.</p> <code>-1</code> <code>model</code> <code>Optional[Module]</code> <p>The model, if you want to specify it directly, this is used in combination with model_config.</p> <code>None</code> <code>model_config</code> <code>Optional[Dict]</code> <p>The config, if you want to specify it directly, this is used in combination with model.</p> <code>None</code> <code>fit_at_predict_time</code> <code>bool</code> <p>Whether to train the model lazily, i.e. only when it is needed for inference in predict[_proba].</p> <code>True</code> <code>device</code> <code>Literal['cuda', 'cpu', 'auto']</code> <p>The device to use for inference, \"auto\" means that it will use cuda if available, otherwise cpu.</p> <code>'auto'</code> <code>seed</code> <code>Optional[int]</code> <p>The default seed to use for the order of the ensemble configurations, a seed of None will not.</p> <code>0</code> <code>show_progress</code> <code>bool</code> <p>Whether to show progress bars during training and inference.</p> <code>True</code> <code>batch_size_inference</code> <code>int</code> <p>The batch size to use for inference, this does not affect the results, just the memory usage and speed. A higher batch size is faster but uses more memory. Setting the batch size to None means that the batch size is automatically determined based on the memory usage and the maximum free memory specified with <code>maximum_free_memory_in_gb</code>.</p> <code>1</code> <code>fp16_inference</code> <code>bool</code> <p>Whether to use fp16 for inference on GPU, does not affect CPU inference.</p> <code>True</code> <code>save_peak_memory</code> <code>Literal['True', 'False', 'auto']</code> <p>Whether to save the peak memory usage of the model, can enable up to 8 times larger datasets to fit into memory. \"True\", means always enabled, \"False\", means always disabled, \"auto\" means that it will be set based on the memory usage.</p> <code>'True'</code>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.fit","title":"fit","text":"<pre><code>fit(X, y, additional_y=None) -&gt; TabPFNBaseModel\n</code></pre> <p>Fits the model to the input data <code>X</code> and <code>y</code>.</p> <p>The actual training logic is delegated to the <code>_fit</code> method, which should be implemented by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, Tensor]</code> <p>The input feature matrix of shape (n_samples, n_features).</p> required <code>y</code> <code>Union[ndarray, Tensor]</code> <p>The target labels of shape (n_samples,).</p> required <code>additional_y</code> <code>Optional[Dict[str, Tensor]]</code> <p>Additional labels to use during training.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TabPFNBaseModel</code> <code>TabPFNBaseModel</code> <p>The fitted model object (self).</p>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(X, additional_y=None) -&gt; ndarray\n</code></pre>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.predict_y_proba","title":"predict_y_proba","text":"<pre><code>predict_y_proba(\n    X: ndarray | Tensor, y: ndarray | Tensor\n) -&gt; ndarray\n</code></pre> <p>Predicts the probability of the target y given the input X.</p>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features: List[int])\n</code></pre> <p>Set the categorical features to use for the model.</p> <p>These categorical features might be overridden by the preprocessing steps. This is controlled by i) <code>max_unique_values_as_categorical_feature</code>, the maximum number of unique values a feature can have to be considered a categorical feature. Features with more unique values are considered numerical features. ii) <code>min_unique_values_as_numerical_feature</code> the minimum number of unique values a feature can have to be considered a numerical feature. Features with less unique values are considered categorical features.</p> <p>:param categorical_features: The feature indices of the categorical features</p>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.score","title":"score","text":"<pre><code>score(X, y, sample_weight=None)\n</code></pre>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.estimate_memory_usage","title":"estimate_memory_usage","text":"<pre><code>estimate_memory_usage(\n    X: ndarray | tensor,\n    unit: Literal[\"b\", \"mb\", \"gb\"] = \"gb\",\n    eval_position: int = -1,\n    **overwrite_params\n) -&gt; float | None\n</code></pre> <p>Estimates the memory usage of the model.</p> <p>Peak memory usage is accurate for \u00b4save_peak_mem_factor\u00b4 in O(n_feats, n_samples) on average but with significant outliers (2x). Also this calculation does not include baseline usage and constant offsets. Baseline memory usage can be ignored if we set the maximum memory usage to the default None which uses the free memory of the system. The constant offsets are not significant for large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if <code>self.fit_at_predict_time</code> and train only otherwise. If you add a batch dimension at position 1 to the table this is used as the batch size used during inference, otherwise this depends on the <code>batch_size_inference</code> and <code>n_estimators</code>.</p> required <code>unit</code> <code>Literal['b', 'mb', 'gb']</code> <p>The unit to return the memory usage in (bytes, megabytes, or gigabytes).</p> <code>'gb'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated memory usage in bytes.</p>"},{"location":"api/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.estimate_computation_usage","title":"estimate_computation_usage","text":"<pre><code>estimate_computation_usage(\n    X: ndarray,\n    unit: Literal[\n        \"sequential_flops\", \"s\"\n    ] = \"sequential_flops\",\n    eval_position: int = -1,\n    **overwrite_params\n) -&gt; float | None\n</code></pre> <p>Estimates the sequential computation usage of the model. Those are the operations that are not parallelizable and are the main bottleneck for the computation time.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if</p> required <code>unit</code> <code>str</code> <p>The unit to return the computation usage in.</p> <code>'sequential_flops'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated computation usage in unit of choice.</p>"}]}