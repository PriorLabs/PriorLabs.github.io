{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"","text":"<p>PriorLabs is building breakthrough foundation models that understand spreadsheets and databases. While foundation models have transformed text and images, tabular data has remained largely untouched. We're tackling this opportunity with technology that could revolutionize how we approach scientific discovery, medical research, financial modeling, and business intelligence.</p>"},{"location":"#why-tabpfn","title":"Why TabPFN","text":"<ul> <li> <p> Rapid Training</p> <p>TabPFN significantly reduces training time, outperforming traditional models tuned for hours in just a few seconds. For instance, it surpasses an ensemble of the strongest baselines in 2.8 seconds compared to 4 hours of tuning.</p> </li> <li> <p> Superior Accuracy</p> <p>TabPFN consistently outperforms state-of-the-art methods like gradient-boosted decision trees (GBDTs) on datasets with up to 10,000 samples. It achieves higher accuracy and better performance metrics across a range of classification and regression tasks.</p> </li> <li> <p> Robustness</p> <p>The model demonstrates robustness to various dataset characteristics, including uninformative features, outliers, and missing values, maintaining high performance where other methods struggle.</p> </li> <li> <p> Generative Capabilities</p> <p>As a generative transformer-based model, TabPFN can be fine-tuned for specific tasks, generate synthetic data, estimate densities, and learn reusable embeddings. This makes it versatile for various applications beyond standard prediction tasks.</p> </li> <li> <p> Sklearn Interface</p> <p>TabPFN follows the interfaces provided by scikit-learn, making it easy to integrate into existing workflows and utilize familiar functions for fitting, predicting, and evaluating models.</p> </li> <li> <p> Minimal Preprocessing</p> <p>The model handles various types of raw data, including missing values and categorical variables, with minimal preprocessing. This reduces the burden on users to perform extensive data preparation.</p> </li> </ul>"},{"location":"#tabpfn-integrations","title":"TabPFN Integrations","text":"<ul> <li> <p> API Client</p> <p>The fastest way to get started with TabPFN. Access our models through the cloud without requiring local GPU resources.</p> </li> <li> <p> User Interface</p> <p>Visual interface for no-code interaction with TabPFN. Perfect for quick experimentation and visualization.</p> <p> Access GUI</p> </li> <li> <p> Python Package</p> <p>Coming soon! Local installation with GPU support and scikit-learn compatible interface.</p> </li> <li> <p> R Integration</p> <p>Currently in development. Bringing TabPFN's capabilities to the R ecosystem for data scientists and researchers.</p> </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>You can access our models through our API (https://github.com/automl/tabpfn-client) or via our user interface built on top of the API (https://ux.priorlabs.ai/). We will release open weights models soon, currently we are available via api and via our user interface built on top of the API.</p> Python API Client (No GPU, Online)Python Local (GPU) <pre><code>pip install tabpfn-client\n</code></pre> <p>Warning</p> <p>Not released yet</p> <p> </p>"},{"location":"api%20copy/","title":"Contribute","text":"<p>Put out project that people could contribute to and provide instructions for contributing</p>"},{"location":"api/","title":"Contribute","text":"<p>Put out project that people could contribute to and provide instructions for contributing</p>"},{"location":"cla/","title":"Contributor Agreement","text":""},{"location":"cla/#individual-contributor-exclusive-license-agreement","title":"Individual Contributor Exclusive License Agreement","text":""},{"location":"cla/#including-the-traditional-patent-license-option","title":"(including the Traditional Patent License OPTION)","text":"<p>Thank you for your interest in contributing to PriorLabs's TabPFN (\"We\" or \"Us\").</p> <p>The purpose of this contributor agreement (\"Agreement\") is to clarify and document the rights granted by contributors to Us. To make this document effective, please follow the instructions at https://www.priorlabs.ai/sign-cla.</p>"},{"location":"cla/#how-to-use-this-contributor-agreement","title":"How to use this Contributor Agreement","text":"<p>If You are an employee and have created the Contribution as part of your employment, You need to have Your employer approve this Agreement or sign the Entity version of this document. If You do not own the Copyright in the entire work of authorship, any other author of the Contribution should also sign this \u2013 in any event, please contact Us at noah.homa@gmail.com</p>"},{"location":"cla/#1-definitions","title":"1. Definitions","text":"<p>\"You\" means the individual Copyright owner who Submits a Contribution to Us.</p> <p>\"Contribution\" means any original work of authorship, including any original modifications or additions to an existing work of authorship, Submitted by You to Us, in which You own the Copyright.</p> <p>\"Copyright\" means all rights protecting works of authorship, including copyright, moral and neighboring rights, as appropriate, for the full term of their existence.</p> <p>\"Material\" means the software or documentation made available by Us to third parties. When this Agreement covers more than one software project, the Material means the software or documentation to which the Contribution was Submitted. After You Submit the Contribution, it may be included in the Material.</p> <p>\"Submit\" means any act by which a Contribution is transferred to Us by You by means of tangible or intangible media, including but not limited to electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Us, but excluding any transfer that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\"</p> <p>\"Documentation\" means any non-software portion of a Contribution.</p>"},{"location":"cla/#2-license-grant","title":"2. License grant","text":""},{"location":"cla/#21-copyright-license-to-us","title":"2.1 Copyright license to Us","text":"<p>Subject to the terms and conditions of this Agreement, You hereby grant to Us a worldwide, royalty-free, Exclusive, perpetual and irrevocable (except as stated in Section 8.2) license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul>"},{"location":"cla/#22-moral-rights","title":"2.2 Moral rights","text":"<p>Moral Rights remain unaffected to the extent they are recognized and not waivable by applicable law. Notwithstanding, You may add your name to the attribution mechanism customary used in the Materials you Contribute to, such as the header of the source code files of Your Contribution, and We will respect this attribution when using Your Contribution.</p>"},{"location":"cla/#23-copyright-license-back-to-you","title":"2.3 Copyright license back to You","text":"<p>Upon such grant of rights to Us, We immediately grant to You a worldwide, royalty-free, non-exclusive, perpetual and irrevocable license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, under the Copyright covering the Contribution to use the Contribution by all means, including, but not limited to:</p> <ul> <li>publish the Contribution,</li> <li>modify the Contribution,</li> <li>prepare derivative works based upon or containing the Contribution and/or to combine the Contribution with other Materials,</li> <li>reproduce the Contribution in original or modified form,</li> <li>distribute, to make the Contribution available to the public, display and publicly perform the Contribution in original or modified form.</li> </ul> <p>This license back is limited to the Contribution and does not provide any rights to the Material.</p>"},{"location":"cla/#3-patents","title":"3. Patents","text":""},{"location":"cla/#31-patent-license","title":"3.1 Patent license","text":"<p>Subject to the terms and conditions of this Agreement You hereby grant to Us and to recipients of Materials distributed by Us a worldwide, royalty-free, non-exclusive, perpetual and irrevocable (except as stated in Section 3.2) patent license, with the right to transfer an unlimited number of non-exclusive licenses or to grant sublicenses to third parties, to make, have made, use, sell, offer for sale, import and otherwise transfer the Contribution and the Contribution in combination with any Material (and portions of such combination). This license applies to all patents owned or controlled by You, whether already acquired or hereafter acquired, that would be infringed by making, having made, using, selling, offering for sale, importing or otherwise transferring of Your Contribution(s) alone or by combination of Your Contribution(s) with any Material.</p>"},{"location":"cla/#32-revocation-of-patent-license","title":"3.2 Revocation of patent license","text":"<p>You reserve the right to revoke the patent license stated in section 3.1 if We make any infringement claim that is targeted at your Contribution and not asserted for a Defensive Purpose. An assertion of claims of the Patents shall be considered for a \"Defensive Purpose\" if the claims are asserted against an entity that has filed, maintained, threatened, or voluntarily participated in a patent infringement lawsuit against Us or any of Our licensees.</p>"},{"location":"cla/#4-license-obligations-by-us","title":"4. License obligations by Us","text":"<p>We agree to license the Contribution only under the terms of the license or licenses that We are using on the Submission Date for the Material (including any rights to adopt any future version of a license).</p> <p>In addition, We may use the following licenses for Documentation in the Contribution: CC-BY-4.0, CC-BY-ND-4.0, CC-BY-NC-4.0, CC-BY-NC-ND-4.0, CC-BY-NC-SA-4.0, CC-BY-SA-4.0, CC0-1.0, MIT License, Apache License, GNU General Public License (GPL) v2.0, GNU General Public License (GPL) v3.0, GNU Affero General Public License v3.0, GNU Lesser General Public License (LGPL) v2.1, GNU Lesser General Public License (LGPL) v3.0, Mozilla Public License 2.0, Eclipse Public License 2.0, Microsoft Public License (Ms-PL), Microsoft Reciprocal License (Ms-RL), BSD 2-Clause \"Simplified\" or \"FreeBSD\" license, BSD 3-Clause \"New\" or \"Revised\" license (including any right to adopt any future version of a license).</p> <p>We agree to license patents owned or controlled by You only to the extent necessary to (sub)license Your Contribution(s) and the combination of Your Contribution(s) with the Material under the terms of the license or licenses that We are using on the Submission Date.</p>"},{"location":"cla/#5-disclaimer","title":"5. Disclaimer","text":"<p>THE CONTRIBUTION IS PROVIDED \"AS IS\". MORE PARTICULARLY, ALL EXPRESS OR IMPLIED WARRANTIES INCLUDING, WITHOUT LIMITATION, ANY IMPLIED WARRANTY OF SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT ARE EXPRESSLY DISCLAIMED BY YOU TO US AND BY US TO YOU. TO THE EXTENT THAT ANY SUCH WARRANTIES CANNOT BE DISCLAIMED, SUCH WARRANTY IS LIMITED IN DURATION AND EXTENT TO THE MINIMUM PERIOD AND EXTENT PERMITTED BY LAW.</p>"},{"location":"cla/#6-consequential-damage-waiver","title":"6. Consequential damage waiver","text":"<p>TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL YOU OR WE BE LIABLE FOR ANY LOSS OF PROFITS, LOSS OF ANTICIPATED SAVINGS, LOSS OF DATA, INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL AND EXEMPLARY DAMAGES ARISING OUT OF THIS AGREEMENT REGARDLESS OF THE LEGAL OR EQUITABLE THEORY (CONTRACT, TORT OR OTHERWISE) UPON WHICH THE CLAIM IS BASED.</p>"},{"location":"cla/#7-approximation-of-disclaimer-and-damage-waiver","title":"7. Approximation of disclaimer and damage waiver","text":"<p>IF THE DISCLAIMER AND DAMAGE WAIVER MENTIONED IN SECTION 5. AND SECTION 6. CANNOT BE GIVEN LEGAL EFFECT UNDER APPLICABLE LOCAL LAW, REVIEWING COURTS SHALL APPLY LOCAL LAW THAT MOST CLOSELY APPROXIMATES AN ABSOLUTE WAIVER OF ALL CIVIL OR CONTRACTUAL LIABILITY IN CONNECTION WITH THE CONTRIBUTION.</p>"},{"location":"cla/#8-term","title":"8. Term","text":"<p>8.1 This Agreement shall come into effect upon Your acceptance of the terms and conditions.</p> <p>8.2 This Agreement shall apply for the term of the copyright and patents licensed here. However, You shall have the right to terminate the Agreement if We do not fulfill the obligations as set forth in Section 4. Such termination must be made in writing.</p> <p>8.3 In the event of a termination of this Agreement Sections 5, 6, 7, 8 and 9 shall survive such termination and shall remain in full force thereafter. For the avoidance of doubt, Free and Open Source Software (sub)licenses that have already been granted for Contributions at the date of the termination shall remain in full force after the termination of this Agreement.</p>"},{"location":"cla/#9-miscellaneous","title":"9. Miscellaneous","text":"<p>9.1 This Agreement and all disputes, claims, actions, suits or other proceedings arising out of this agreement or relating in any way to it shall be governed by the laws of Germany excluding its private international law provisions.</p> <p>9.2 This Agreement sets out the entire agreement between You and Us for Your Contributions to Us and overrides all other agreements or understandings.</p> <p>9.3 In case of Your death, this agreement shall continue with Your heirs. In case of more than one heir, all heirs must exercise their rights through a commonly authorized person.</p> <p>9.4 If any provision of this Agreement is found void and unenforceable, such provision will be replaced to the extent possible with a provision that comes closest to the meaning of the original provision and that is enforceable. The terms and conditions set forth in this Agreement shall apply notwithstanding any failure of essential purpose of this Agreement or any limited remedy to the maximum extent possible under law.</p> <p>9.5 You agree to notify Us of any facts or circumstances of which you become aware that would make this Agreement inaccurate in any respect.</p>"},{"location":"code_download/","title":"Code download","text":"<p>Our code is currently stored in a private repository on GitHub, to avoid indexing at this point. We do not share this links on this public website. To access our code and any example notebooks, please use the notebook provided in the links to our submission via the \u201clinktree\u201d in our main paper or the code submission checklist. </p>"},{"location":"contribute/","title":"Contribute","text":"<p>Put out project that people could contribute to and provide instructions for contributing</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#citing","title":"Citing","text":""},{"location":"credits/#acknowledgments","title":"Acknowledgments","text":"<p>We express our gratitude to the following individuals for their valuable contributions and support:</p> <p>Eddie Bergman for his assistance with the evaluation of TabPFN and for his efforts in improving the code quality and documentation. His contributions were instrumental in benchmarking TabPFN and ensuring the reproducibility of our results.</p> <p>Liam Shi Bin Hoo, Anshul Gupta, and David Otte for their work on the Inference Server, which enables the fast deployment of TabPFN without the need for a local GPU. Their efforts have greatly enhanced the accessibility and usability of TabPFN.</p> <p>David Schnurr and Kai Helli for their work on visualization, and David Schnurr for his specific contributions related to handling missing values. Lukas Schweizer for his work on exploring the Random Forest preprocessing for TabPFN further.</p> <p>Andreas M\u00fcller for the insightful discussions related to TabPFN training and for his guidance on identifying and mitigating biases in the prior. His expertise has been invaluable in refining the TabPFN methodology.</p> <p>Stefan St\u00e4glich for his diligent maintenance of the cluster infrastructure.</p> <p>We also extend our thanks to Claudia Langenberg and Maik Pietzner for providing insights on medical applications.</p> <p>We are grateful for the computational resources that were available for this research. Specifically, we acknowledge support by the state of Baden-W\u00fcrttemberg through bwHPC and the German Research Foundation (DFG) through grant no INST 39/963-1 FUGG (bwForCluster NEMO), and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828.</p> <p>We gratefully acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under SFB 1597 (SmallData), grant number 499552394, and by the European Union (via ERC Consolidator Grant DeepLearning 2.0, grant no.~101045765). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. </p>"},{"location":"release_notes/","title":"Release notes","text":"<pre><code># Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [1.0.0] - 2017-06-20\n</code></pre>"},{"location":"tabpfn-nature/","title":"Highly Accurate Predictions for Small Data With the Tabular Foundation Model TabPFN","text":"<p>Warning</p> <p>This is a beta version of our documentation created for the review phase. Please do not share this review version.</p> <p>Our code is currently stored in a private repository on GitHub We do not share this links on this public website. To access our code and any example notebooks, please use the notebook provided in the links to our submission via the link in our main paper or the code submission checklist. </p> <p>This page contains usage examples and installation instructions of TabPFN. Please find additional instructions on our Classifiers and Regressors on the respective subpages. An in-depth technical documentation of our software interfaces can be found in the API Reference</p>"},{"location":"tabpfn-nature/#installation","title":"Installation","text":"<p>To install our software, we use pip the python package installer in combination with Git for code-management. Please find the code for installation via the private link shared with you, that also contains the private access tokens to the code. An installation typically takes 5 minutes in a setup python environment. </p> <p>Tip</p> <p>The easiest way to install and run our code is via the Colab Notebooks shared in the link in our submission.</p>"},{"location":"tabpfn-nature/#software-dependencies-and-operating-systems","title":"Software Dependencies and Operating Systems","text":"<p>Python: Version &gt;= 3.9</p> <p>Operating Systems: The software has been tested on major operating systems including:</p> <ul> <li> <p>Ubuntu 20.04, 22.04</p> </li> <li> <p>Windows 10, 11</p> </li> <li> <p>macOS 11.0 (Big Sur) and later</p> </li> </ul> <p>Git Version 2 or later (https://git-scm.com/)</p>"},{"location":"tabpfn-nature/#software-dependencies-as-specified-in-requirementstxt","title":"Software Dependencies (as specified in <code>requirements.txt</code>):","text":"TabPFNTabPFN and Baselines <pre><code>torch&gt;=2.1 (Includes CUDA support in version 2.1 and later)\nscikit-learn&gt;=1.4.2\ntqdm&gt;=4.66.\nnumpy&gt;=1.21.2\nhyperopt==0.2.7 (Note: Earlier versions fail with numpy number generator change)\npre-commit&gt;=3.3.3\neinops&gt;=0.6.0\nscipy&gt;=1.8.0\ntorchmetrics==1.2.0\npytest&gt;=7.1.3\npandas[plot,output_formatting]&gt;=2.0.3,&lt;2.2 (Note: Version 2.2 has a bug with multi-index tables (https://github.com/pandas-dev/pandas/issues/57663), recheck when fixed)\npyyaml&gt;=6.0.1\nkditransform&gt;=0.2.0\n</code></pre> <pre><code>torch&gt;=2.1 (Includes CUDA support in version 2.1 and later)\nscikit-learn&gt;=1.4.2\ntqdm&gt;=4.66.\nnumpy&gt;=1.21.2\nhyperopt==0.2.7 (Note: Earlier versions fail with numpy number generator change)\npre-commit&gt;=3.3.3\neinops&gt;=0.6.0\nscipy&gt;=1.8.0\ntorchmetrics==1.2.0\npytest&gt;=7.1.3\npandas[plot,output_formatting]&gt;=2.0.3,&lt;2.2 (Note: Version 2.2 has a bug with multi-index tables (https://github.com/pandas-dev/pandas/issues/57663), recheck when fixed)\npyyaml&gt;=6.0.1\nkditransform&gt;=0.2.0\nseaborn==0.12.2\nopenml==0.14.1\nnumba&gt;=0.58.1\nshap&gt;=0.44.1\n\n# Baselines\nlightgbm==3.3.5\nxgboost&gt;=2.0.0\ncatboost&gt;=1.1.1\n#auto-sklearn==0.14.5\n#autogluon==0.4.0\n\n# -- Quantile Baseline\nquantile-forest==1.2.4\n</code></pre> <p>For GPU usage CUDA 12.1 has been tested.</p>"},{"location":"tabpfn-nature/#non-standard-hardware","title":"Non-Standard Hardware","text":"<p>GPU: A CUDA-enabled GPU is recommended for optimal performance, though the software can also run on a CPU.</p>"},{"location":"tabpfn-nature/#example-usage","title":"Example usage","text":"ClassificationRegression <pre><code>import numpy as np\nimport sklearn\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nfrom tabpfn import TabPFNClassifier\n\n# Create a classifier\nclf = TabPFNClassifier(fit_at_predict_time=True)\n\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nclf.fit(X_train, y_train)\npreds = clf.predict_proba(X_test)\ny_eval = np.argmax(preds, axis=1)\n\nprint('ROC AUC: ', sklearn.metrics.roc_auc_score(y_test, preds, multi_class='ovr'), 'Accuracy', sklearn.metrics.accuracy_score(y_test, y_eval))\n</code></pre> <pre><code>from tabpfn import TabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = TabPFNRegressor(device='auto')\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre>"},{"location":"tabpfn-nature/#expected-output","title":"Expected Output","text":"<p>Our models follow the interfaces provided by sklearn, so you can expect the same output as you would from sklearn models. TabPFNClassifier will return a numpy array of shape <code>(n_samples, n_classes)</code> with the probabilities of each class, while TabPFNRegressor will return a numpy array of shape <code>(n_samples,)</code> with the predicted values. For more detailed documentation please check the technical documentation of scripts.estimator.TabPFNClassifier.predict_proba.</p>"},{"location":"tabpfn-nature/#expected-runtime","title":"Expected Runtime","text":"<p>The runtime of the model is dependent on the number of estimators and the size of the dataset. For a dataset of 1000 samples and 4 features, the runtime on GPU is typically less than 1 second. For a dataset of 10000 samples and 4 features, the runtime on GPU is typically less than 10 seconds.</p>"},{"location":"terms-eu-en/","title":"Terms of Service Agreement","text":"<p>This Agreement outlines the terms governing your use of TabPFN and our associated services, software, and websites (\"Services\"). By accepting these Terms or using the Services, you enter into a contractual agreement with us.</p>"},{"location":"terms-eu-en/#about-us","title":"About Us","text":"<p>We are PriorLabs, a team from the Machine Learning Lab at the University of Freiburg, committed to democratizing data science.</p>"},{"location":"terms-eu-en/#specific-terms-for-certain-services","title":"Specific Terms for Certain Services","text":"<p>Your use of our Services may be subject to additional terms depending on the specific Services or features you utilize. These include:</p> <ul> <li>Usage Guidelines: outlining permissible uses of our Services and Content</li> <li>Specific Service Terms: applicable when engaging with certain Services or features</li> <li>Content Sharing and Publication Rules: governing the sharing of Content</li> <li>Terms for Service Credits: relating to the acquisition and utilization of service credits</li> <li>Brand Usage Norms: detailing permissible uses of our trademarks and logos</li> </ul>"},{"location":"terms-eu-en/#account-creation-and-use","title":"Account Creation and Use","text":""},{"location":"terms-eu-en/#age-requirement","title":"Age Requirement","text":"<p>Users must be at least 13 years old or the minimum legal age in their country, with parental or guardian consent if under 18.</p>"},{"location":"terms-eu-en/#account-registration","title":"Account Registration","text":"<p>Accurate and complete information is required for account registration. Your account is personal and should not be shared. You are accountable for all activities under your account. If you are registering on behalf of another individual or entity, you must be authorized to accept these Terms on their behalf.</p>"},{"location":"terms-eu-en/#beta-services","title":"Beta Services","text":"<p>Beta Services, offered in alpha, preview, early access, or beta stages, are provided \"as-is\" for testing purposes.</p> <p>We offer no guarantees regarding these Beta Services, including their up-time, availability, uninterrupted functioning, security of Content, or error-free operation. All warranties, explicit or implied, are disclaimed for Beta Services to the extent allowable by law.</p>"},{"location":"terms-eu-en/#usage-of-our-services","title":"Usage of Our Services","text":""},{"location":"terms-eu-en/#permitted-uses","title":"Permitted Uses","text":"<p>You may access and use our Services in compliance with these Terms and all relevant laws, along with the specific service terms and policies mentioned above.</p>"},{"location":"terms-eu-en/#prohibited-uses","title":"Prohibited Uses","text":"<p>Our Services must not be used for any illegal or harmful activities. This includes:</p> <ul> <li>Violating rights or intellectual property</li> <li>Altering, replicating, leasing, selling, or distributing our Services</li> <li>Engaging in or aiding reverse engineering or extraction of code, models, algorithms, or systems</li> <li>Data extraction or Output use in an unauthorized manner</li> <li>Disrupting our Services or bypassing security measures</li> <li>Utilizing Output to create competing models</li> </ul>"},{"location":"terms-eu-en/#software-use","title":"Software Use","text":"<p>Software provided through our Services may update automatically. This includes open-source components governed by their own licenses.</p>"},{"location":"terms-eu-en/#third-party-services","title":"Third-Party Services","text":"<p>We include third-party software or services, which are subject to their own terms. We are not responsible for these third-party elements.</p>"},{"location":"terms-eu-en/#feedback","title":"Feedback","text":"<p>Your input about our Services is valuable, and we may use it to enhance our offerings without any obligation for compensation.</p>"},{"location":"terms-eu-en/#content-responsibilities","title":"Content Responsibilities","text":""},{"location":"terms-eu-en/#your-content","title":"Your Content","text":"<p>You are responsible for any input (\"Input\") and the resultant output (\"Output\") from our Services (\"Content\"). You must ensure that your Content complies with laws and these Terms. You warrant that you have all necessary rights and permissions for the Input.</p> <p>You are obliged to observe all legal provisions for the collection, processing and use of data that is transmitted to and processed in connection with the Services. In particular, you shall promptly enter into a data processing agreement with PriorLabs (which we will provide) if you intend to transmit personal data to PriorLabs when using the Services. You warrant that you will not collect, process or use any personal data in connection with the use of the Services without the express consent of the data subject or another legal basis.</p>"},{"location":"terms-eu-en/#our-content-usage","title":"Our Content Usage","text":"<p>We may use your Content globally to enhance, maintain, and develop our Services and comply with legal obligations.</p>"},{"location":"terms-eu-en/#opt-out-option","title":"Opt-Out Option","text":"<p>You can opt-out of having your Content used to train our models by adjusting your account settings. Opting out may affect the Services' performance for your use case.</p>"},{"location":"terms-eu-en/#accuracy-and-reliability","title":"Accuracy and Reliability","text":"<p>We constantly strive to improve our Services, but due to the nature of machine learning, results may not always be accurate. You should not solely rely on the Output and are responsible for assessing its accuracy and suitability.</p>"},{"location":"terms-eu-en/#intellectual-property-rights","title":"Intellectual Property Rights","text":"<p>All intellectual property rights in the Services are owned by us and our affiliates.</p>"},{"location":"terms-eu-en/#termination-and-suspension","title":"Termination and Suspension","text":""},{"location":"terms-eu-en/#our-rights","title":"Our Rights","text":"<p>The Services are provided to you free of charge. Therefore, we may cease, suspend, or limit the Services at any time.</p> <p>We reserve the right to suspend or terminate access to our Services or close accounts if:</p> <ul> <li>There is a breach of these Terms or Usage Policies</li> <li>Compliance with legal requirements necessitates it</li> <li>Your use poses a risk to us, our users, or others</li> <li>Your account is inactive for over a year</li> </ul>"},{"location":"terms-eu-en/#limitation-of-liability","title":"Limitation of Liability","text":"<p>The Services are provided to you free of charge. Therefore, our liability is in any cases limited to acts of intent or gross negligence.</p> <p>The strict liability for damages for defects of the Service already existing at the beginning of the Service Term in terms of sec. 536a German Civil Code is excluded. We provide the Services on an \"as is\" basis, which refers in particular to the marketability, availability and security aspects of the contractual software.</p>"},{"location":"terms-eu-en/#final-provisions","title":"Final Provisions","text":"<p>Should individual provisions of the contract be or become invalid in whole or in part, this shall not affect the validity of the remaining provisions.</p> <p>The exclusive place of jurisdiction for all disputes arising from or in connection with the contract is Berlin. PriorLabs shall also be entitled to bring an action at your place of business or at any other competent court.</p> <p>The law of the Federal Republic of Germany shall apply with the exception of its provisions on the choice of law which would lead to the application of another legal system. The validity of the CISG (\"UN Sales Convention\") is excluded.</p>"},{"location":"terms/","title":"Terms of Service Agreement","text":"<p>This Agreement outlines the terms governing your use of TabPFN and our associated services, software, and websites (\"Services\"). By accepting these Terms or using the Services, you enter into a contractual agreement with us.</p>"},{"location":"terms/#about-us","title":"About Us","text":"<p>We are PriorLabs, a team from the Machine Learning Lab at the University of Freiburg, committed to democratizing data science.</p>"},{"location":"terms/#specific-terms-for-certain-services","title":"Specific Terms for Certain Services","text":"<p>Your use of our Services may be subject to additional terms depending on the specific Services or features you utilize. These include:</p> <ul> <li>Usage Guidelines: outlining permissible uses of our Services and Content</li> <li>Specific Service Terms: applicable when engaging with certain Services or features</li> <li>Content Sharing and Publication Rules: governing the sharing of Content</li> <li>Terms for Service Credits: relating to the acquisition and utilization of service credits</li> <li>Brand Usage Norms: detailing permissible uses of our trademarks and logos</li> </ul>"},{"location":"terms/#account-creation-and-use","title":"Account Creation and Use","text":""},{"location":"terms/#age-requirement","title":"Age Requirement","text":"<p>Users must be at least 13 years old or the minimum legal age in their country, with parental or guardian consent if under 18.</p>"},{"location":"terms/#account-registration","title":"Account Registration","text":"<p>Accurate and complete information is required for account registration. Your account is personal and should not be shared. You are accountable for all activities under your account. If you are registering on behalf of another individual or entity, you must be authorized to accept these Terms on their behalf.</p>"},{"location":"terms/#beta-services","title":"Beta Services","text":"<p>Beta Services, offered in alpha, preview, early access, or beta stages, are provided \"as-is\" for testing purposes.</p> <p>We offer no guarantees regarding these Beta Services, including their up-time, availability, uninterrupted functioning, security of Content, or error-free operation. All warranties, explicit or implied, are disclaimed for Beta Services to the extent allowable by law.</p>"},{"location":"terms/#usage-of-our-services","title":"Usage of Our Services","text":""},{"location":"terms/#permitted-uses","title":"Permitted Uses","text":"<p>You may access and use our Services in compliance with these Terms and all relevant laws, along with the specific service terms and policies mentioned above.</p>"},{"location":"terms/#prohibited-uses","title":"Prohibited Uses","text":"<p>Our Services must not be used for any illegal or harmful activities. This includes:</p> <ul> <li>Violating rights or intellectual property</li> <li>Altering, replicating, leasing, selling, or distributing our Services</li> <li>Engaging in or aiding reverse engineering or extraction of code, models, algorithms, or systems</li> <li>Data extraction or Output use in an unauthorized manner</li> <li>Disrupting our Services or bypassing security measures</li> <li>Utilizing Output to create competing models</li> </ul>"},{"location":"terms/#software-use","title":"Software Use","text":"<p>Software provided through our Services may update automatically. This includes open-source components governed by their own licenses.</p>"},{"location":"terms/#third-party-services","title":"Third-Party Services","text":"<p>We include third-party software or services, which are subject to their own terms. We are not responsible for these third-party elements.</p>"},{"location":"terms/#feedback","title":"Feedback","text":"<p>Your input about our Services is valuable, and we may use it to enhance our offerings without any obligation for compensation.</p>"},{"location":"terms/#content-responsibilities","title":"Content Responsibilities","text":""},{"location":"terms/#your-content","title":"Your Content","text":"<p>You are responsible for any input (\"Input\") and the resultant output (\"Output\") from our Services (\"Content\"). You must ensure that your Content complies with laws and these Terms. You warrant that you have all necessary rights and permissions for the Input.</p> <p>You are obliged to observe all legal provisions for the collection, processing and use of data that is transmitted to and processed in connection with the Services. In particular, you shall promptly enter into a data processing agreement with PriorLabs (which we will provide) if you intend to transmit personal data to PriorLabs when using the Services. You warrant that you will not collect, process or use any personal data in connection with the use of the Services without the express consent of the data subject or another legal basis.</p>"},{"location":"terms/#our-content-usage","title":"Our Content Usage","text":"<p>We may use your Content globally to enhance, maintain, and develop our Services and comply with legal obligations.</p>"},{"location":"terms/#opt-out-option","title":"Opt-Out Option","text":"<p>You can opt-out of having your Content used to train our models by adjusting your account settings. Opting out may affect the Services' performance for your use case.</p>"},{"location":"terms/#accuracy-and-reliability","title":"Accuracy and Reliability","text":"<p>We constantly strive to improve our Services, but due to the nature of machine learning, results may not always be accurate. You should not solely rely on the Output and are responsible for assessing its accuracy and suitability.</p>"},{"location":"terms/#intellectual-property-rights","title":"Intellectual Property Rights","text":"<p>All intellectual property rights in the Services are owned by us and our affiliates.</p>"},{"location":"terms/#termination-and-suspension","title":"Termination and Suspension","text":""},{"location":"terms/#our-rights","title":"Our Rights","text":"<p>The Services are provided to you free of charge. Therefore, we may cease, suspend, or limit the Services at any time.</p> <p>We reserve the right to suspend or terminate access to our Services or close accounts if:</p> <ul> <li>There is a breach of these Terms or Usage Policies</li> <li>Compliance with legal requirements necessitates it</li> <li>Your use poses a risk to us, our users, or others</li> <li>Your account is inactive for over a year</li> </ul>"},{"location":"terms/#limitation-of-liability","title":"Limitation of Liability","text":"<p>The Services are provided to you free of charge. Therefore, our liability is in any cases limited to acts of intent or gross negligence.</p> <p>The strict liability for damages for defects of the Service already existing at the beginning of the Service Term in terms of sec. 536a German Civil Code is excluded. We provide the Services on an \"as is\" basis, which refers in particular to the marketability, availability and security aspects of the contractual software.</p>"},{"location":"terms/#final-provisions","title":"Final Provisions","text":"<p>Should individual provisions of the contract be or become invalid in whole or in part, this shall not affect the validity of the remaining provisions.</p> <p>The exclusive place of jurisdiction for all disputes arising from or in connection with the contract is Berlin. PriorLabs shall also be entitled to bring an action at your place of business or at any other competent court.</p> <p>The law of the Federal Republic of Germany shall apply with the exception of its provisions on the choice of law which would lead to the application of another legal system. The validity of the CISG (\"UN Sales Convention\") is excluded.</p>"},{"location":"api/tabpfn/AutoTabPFNClassifier/","title":"AutoTabPFNClassifier","text":""},{"location":"api/tabpfn/AutoTabPFNClassifier/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier","title":"AutoTabPFNClassifier","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Automatic Post Hoc Ensemble Classifier for TabPFN models.</p> <p>Attributes:</p> Name Type Description <code>predictor_</code> <p>AutoPostHocEnsemblePredictor The predictor interface used to make predictions, see post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more.</p> <code>phe_init_args_</code> <p>dict The optional initialization arguments used for the post hoc ensemble predictor.</p>"},{"location":"api/tabpfn/AutoTabPFNClassifier/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier.__init__","title":"__init__","text":"<pre><code>__init__(\n    max_time: int | None = 30,\n    preset: Literal[\n        \"default\", \"custom_hps\", \"avoid_overfitting\"\n    ] = \"default\",\n    ges_scoring_string: str = \"roc\",\n    device: Literal[\"cpu\", \"cuda\"] = \"cpu\",\n    random_state: int | None | RandomState = None,\n    categorical_feature_indices: list[int] | None = None,\n    phe_init_args: dict | None = None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>max_time</code> <p>int | None, default=None The maximum time to spend on fitting the post hoc ensemble.</p> <code>30</code> <code>preset</code> <code>Literal['default', 'custom_hps', 'avoid_overfitting']</code> <p>{\"default\", \"custom_hps\", \"avoid_overfitting\"}, default=\"default\" The preset to use for the post hoc ensemble.</p> <code>'default'</code> <code>ges_scoring_string</code> <p>str, default=\"roc\" The scoring string to use for the greedy ensemble search. Allowed values are: {\"accuracy\", \"roc\" / \"auroc\", \"f1\", \"log_loss\"}.</p> <code>'roc'</code> <code>device</code> <p>{\"cpu\", \"cuda\"}, default=\"cuda\" The device to use for training and prediction.</p> <code>'cpu'</code> <code>random_state</code> <p>int, RandomState instance or None, default=None Controls both the randomness base models and the post hoc ensembling method.</p> <code>None</code> <code>categorical_feature_indices</code> <code>list[int] | None</code> <p>list[int] or None, default=None The indices of the categorical features in the input data. Can also be passed to <code>fit()</code>.</p> <code>None</code> <code>phe_init_args</code> <p>dict | None, default=None The initialization arguments for the post hoc ensemble predictor. See post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more options and all details.</p> <code>None</code>"},{"location":"api/tabpfn/AutoTabPFNClassifier/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier.fit","title":"fit","text":"<pre><code>fit(\n    X,\n    y,\n    categorical_feature_indices: list[int] | None = None,\n)\n</code></pre>"},{"location":"api/tabpfn/AutoTabPFNClassifier/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre>"},{"location":"api/tabpfn/AutoTabPFNClassifier/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre>"},{"location":"api/tabpfn/AutoTabPFNRegressor/","title":"AutoTabPFNRegressor","text":""},{"location":"api/tabpfn/AutoTabPFNRegressor/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor","title":"AutoTabPFNRegressor","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Automatic Post Hoc Ensemble Regressor for TabPFN models. Attributes:     predictor_ : AutoPostHocEnsemblePredictor         The predictor interface used to make predictions, see post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more.     phe_init_args_ : dict         The optional initialization arguments used for the post hoc ensemble predictor.</p>"},{"location":"api/tabpfn/AutoTabPFNRegressor/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor.__init__","title":"__init__","text":"<pre><code>__init__(\n    max_time: int | None = 30,\n    preset: Literal[\n        \"default\", \"custom_hps\", \"avoid_overfitting\"\n    ] = \"default\",\n    ges_scoring_string: str = \"mse\",\n    device: Literal[\"cpu\", \"cuda\"] = \"cpu\",\n    random_state: int | None | RandomState = None,\n    categorical_feature_indices: list[int] | None = None,\n    phe_init_args: dict | None = None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>max_time</code> <p>int | None, default=None The maximum time to spend on fitting the post hoc ensemble.</p> <code>30</code> <code>preset</code> <code>Literal['default', 'custom_hps', 'avoid_overfitting']</code> <p>{\"default\", \"custom_hps\", \"avoid_overfitting\"}, default=\"default\" The preset to use for the post hoc ensemble.</p> <code>'default'</code> <code>ges_scoring_string</code> <p>str, default=\"mse\" The scoring string to use for the greedy ensemble search. Allowed values are: {\"rmse\", \"mse\", \"mae\"}.</p> <code>'mse'</code> <code>device</code> <p>{\"cpu\", \"cuda\"}, default=\"cuda\" The device to use for training and prediction.</p> <code>'cpu'</code> <code>random_state</code> <p>int, RandomState instance or None, default=None Controls both the randomness base models and the post hoc ensembling method.</p> <code>None</code> <code>categorical_feature_indices</code> <code>list[int] | None</code> <p>list[int] or None, default=None The indices of the categorical features in the input data. Can also be passed to <code>fit()</code>.</p> <code>None</code> <code>phe_init_args</code> <p>dict | None, default=None The initialization arguments for the post hoc ensemble predictor. See post_hoc_ensembles.pfn_phe.AutoPostHocEnsemblePredictor for more options and all details.</p> <code>None</code>"},{"location":"api/tabpfn/AutoTabPFNRegressor/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor.fit","title":"fit","text":"<pre><code>fit(\n    X,\n    y,\n    categorical_feature_indices: list[int] | None = None,\n)\n</code></pre>"},{"location":"api/tabpfn/AutoTabPFNRegressor/#scripts.estimator.post_hoc_ensembles.sklearn_interface.AutoTabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre>"},{"location":"api/tabpfn/ClassifierAsRegressor/","title":"ClassifierAsRegressor","text":""},{"location":"api/tabpfn/ClassifierAsRegressor/#scripts.estimator.ClassifierAsRegressor","title":"ClassifierAsRegressor","text":"<p>             Bases: <code>RegressorMixin</code></p> <p>Wrapper class to use a classifier as a regressor.</p> <p>This class takes a classifier estimator and converts it into a regressor by encoding the target labels and treating the regression problem as a classification task.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>object Classifier estimator to be used as a regressor.</p> required <p>Attributes:</p> Name Type Description <code>label_encoder_</code> <p>LabelEncoder Label encoder used to transform target regression labels to classes.</p> <code>y_train_</code> <p>array-like of shape (n_samples,) Transformed target labels used for training.</p> <code>categorical_features</code> <p>list List of categorical feature indices.</p> Example<pre><code>&gt;&gt;&gt; from sklearn.datasets import load_diabetes\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; from tabpfn.scripts.estimator import ManyClassClassifier, TabPFNClassifier, ClassifierAsRegressor\n&gt;&gt;&gt; x, y = load_diabetes(return_X_y=True)\n&gt;&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n&gt;&gt;&gt; clf = TabPFNClassifier()\n&gt;&gt;&gt; clf = ManyClassClassifier(clf, n_estimators=10, alphabet_size=clf.max_num_classes_)\n&gt;&gt;&gt; reg = ClassifierAsRegressor(clf)\n&gt;&gt;&gt; reg.fit(x_train, y_train)\n&gt;&gt;&gt; y_pred = reg.predict(x_test)\n</code></pre>"},{"location":"api/tabpfn/ClassifierAsRegressor/#scripts.estimator.ClassifierAsRegressor.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre> <p>Fit the classifier as a regressor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Training data.</p> required <code>y</code> <p>array-like of shape (n_samples,) Target labels.</p> required <p>Returns:</p> Name Type Description <code>self</code> <p>object Fitted estimator.</p>"},{"location":"api/tabpfn/ClassifierAsRegressor/#scripts.estimator.ClassifierAsRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict the target values for the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Input data.</p> required <p>Returns:</p> Name Type Description <code>y_pred</code> <p>array-like of shape (n_samples,) Predicted target values.</p>"},{"location":"api/tabpfn/ManyClassClassifier/","title":"ManyClassClassifier","text":""},{"location":"api/tabpfn/ManyClassClassifier/#scripts.estimator.ManyClassClassifier","title":"ManyClassClassifier","text":"<p>             Bases: <code>OutputCodeClassifier</code></p> <p>Output-Code multiclass strategy with deciary codebook.</p> <p>This class extends the original OutputCodeClassifier to support n-ary codebooks (with n=alphabet_size), allowing for handling more classes.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <p>estimator object An estimator object implementing :term:<code>fit</code> and one of :term:<code>decision_function</code> or :term:<code>predict_proba</code>. The base classifier should be able to handle up to <code>alphabet_size</code> classes.</p> required <code>random_state</code> <p>int, RandomState instance, default=None The generator used to initialize the codebook. Pass an int for reproducible output across multiple function calls. See :term:<code>Glossary &lt;random_state&gt;</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>estimators_</code> <p>list of <code>int(n_classes * code_size)</code> estimators Estimators used for predictions.</p> <code>classes_</code> <p>ndarray of shape (n_classes,) Array containing labels.</p> <code>code_book_</code> <p>ndarray of shape (n_classes, <code>len(estimators_)</code>) Deciary array containing the code of each class.</p> Example<pre><code>&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from tabpfn.scripts.estimator import ManyClassClassifier, TabPFNClassifier\n&gt;&gt;&gt; from sklearn.model_selection import train_test_split\n&gt;&gt;&gt; x, y = load_iris(return_X_y=True)\n&gt;&gt;&gt; x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n&gt;&gt;&gt; clf = TabPFNClassifier()\n&gt;&gt;&gt; clf = ManyClassClassifier(clf, alphabet_size=clf.max_num_classes_)\n&gt;&gt;&gt; clf.fit(x_train, y_train)\n&gt;&gt;&gt; clf.predict(x_test)\n</code></pre>"},{"location":"api/tabpfn/ManyClassClassifier/#scripts.estimator.ManyClassClassifier.fit","title":"fit","text":"<pre><code>fit(X, y, **fit_params)\n</code></pre> <p>Fit underlying estimators.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>{array-like, sparse matrix} of shape (n_samples, n_features) Data.</p> required <code>y</code> <p>array-like of shape (n_samples,) Multi-class targets.</p> required <code>**fit_params</code> <p>dict Parameters passed to the <code>estimator.fit</code> method of each sub-estimator.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>self</code> <p>object Returns a fitted instance of self.</p>"},{"location":"api/tabpfn/ManyClassClassifier/#scripts.estimator.ManyClassClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre> <p>Predict probabilities using the underlying estimators.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>{array-like, sparse matrix} of shape (n_samples, n_features) Data.</p> required <p>Returns:</p> Name Type Description <code>p</code> <p>ndarray of shape (n_samples, n_classes) Returns the probability of the samples for each class in the model, where classes are ordered as they are in <code>self.classes_</code>.</p>"},{"location":"api/tabpfn/PreprocessorConfig/","title":"PreprocessorConfig","text":""},{"location":"api/tabpfn/PreprocessorConfig/#scripts.estimator.PreprocessorConfig","title":"PreprocessorConfig  <code>dataclass</code>","text":"<p>Configuration for data preprocessors.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal</code> <p>Name of the preprocessor.</p> <code>categorical_name</code> <code>Literal</code> <p>Name of the categorical encoding method. Valid options are \"none\", \"numeric\",                     \"onehot\", \"ordinal\", \"ordinal_shuffled\". Default is \"none\".</p> <code>append_original</code> <code>bool</code> <p>Whether to append the original features to the transformed features. Default is False.</p> <code>subsample_features</code> <code>float</code> <p>Fraction of features to subsample. -1 means no subsampling. Default is -1.</p> <code>global_transformer_name</code> <code>str</code> <p>Name of the global transformer to use. Default is None.</p>"},{"location":"api/tabpfn/TabPFNUnsupervisedModel/","title":"TabPFNUnsupervisedModel","text":""},{"location":"api/tabpfn/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel","title":"TabPFNUnsupervisedModel","text":"<p>             Bases: <code>BaseEstimator</code></p> <p>TabPFN unsupervised model for imputation, outlier detection, and synthetic data generation.</p> <p>This model combines a TabPFNClassifier for categorical features and a TabPFNRegressor for numerical features to perform various unsupervised learning tasks on tabular data.</p> <p>Parameters:</p> Name Type Description Default <code>tabpfn_clf</code> <p>TabPFNClassifier, optional TabPFNClassifier instance for handling categorical features. If not provided, the model assumes that there are no categorical features in the data.</p> <code>None</code> <code>tabpfn_reg</code> <p>TabPFNRegressor, optional TabPFNRegressor instance for handling numerical features. If not provided, the model assumes that there are no numerical features in the data.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>categorical_features</code> <p>list List of indices of categorical features in the input data.</p> Example<pre><code>&gt;&gt;&gt; tabpfn_clf = TabPFNClassifier()\n&gt;&gt;&gt; tabpfn_reg = TabPFNRegressor()\n&gt;&gt;&gt; model = TabPFNUnsupervisedModel(tabpfn_clf, tabpfn_reg)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\n&gt;&gt;&gt; model.fit(X)\n&gt;&gt;&gt;\n&gt;&gt;&gt; X_imputed = model.impute(X)\n&gt;&gt;&gt; X_outliers = model.outliers(X)\n&gt;&gt;&gt; X_synthetic = model.generate_synthetic_data(n_samples=100)\n</code></pre>"},{"location":"api/tabpfn/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.__init__","title":"__init__","text":"<pre><code>__init__(\n    tabpfn_clf: Optional[TabPFNClassifier] = None,\n    tabpfn_reg: Optional[TabPFNRegressor] = None,\n) -&gt; None\n</code></pre> <p>Initialize the TabPFNUnsupervisedModel.</p> <p>Parameters:</p> Name Type Description Default <code>tabpfn_clf</code> <p>TabPFNClassifier, optional TabPFNClassifier instance for handling categorical features. If not provided, the model assumes that there are no categorical features in the data.</p> <code>None</code> <code>tabpfn_reg</code> <p>TabPFNRegressor, optional TabPFNRegressor instance for handling numerical features. If not provided, the model assumes that there are no numerical features in the data.</p> <code>None</code>"},{"location":"api/tabpfn/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.fit","title":"fit","text":"<pre><code>fit(X: ndarray, y: Optional[ndarray] = None) -&gt; None\n</code></pre> <p>Fit the model to the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>array-like of shape (n_samples, n_features) Input data to fit the model.</p> required <code>y</code> <p>array-like of shape (n_samples,), optional Target values.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>None</code> <p>TabPFNUnsupervisedModel Fitted model.</p>"},{"location":"api/tabpfn/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features)\n</code></pre>"},{"location":"api/tabpfn/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.impute","title":"impute","text":"<pre><code>impute(\n    X: tensor, t: float = 1e-09, n_permutations: int = 10\n) -&gt; tensor\n</code></pre> <p>Impute missing values in the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>torch.Tensor of shape (n_samples, n_features) Input data with missing values encoded as np.nan.</p> required <code>t</code> <p>float, default=0.000000001 Temperature for sampling from the imputation distribution. Lower values result in more deterministic imputations.</p> <code>1e-09</code> <p>Returns:</p> Type Description <code>tensor</code> <p>torch.Tensor of shape (n_samples, n_features) Imputed data with missing values replaced.</p>"},{"location":"api/tabpfn/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    X: tensor, per_column: bool = False\n) -&gt; tensor\n</code></pre> <p>Get the transformer embeddings for the test data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>tensor</code> required <p>Returns:</p> Type Description <code>tensor</code> <p>torch.Tensor of shape (n_samples, embedding_dim)</p>"},{"location":"api/tabpfn/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.outliers","title":"outliers","text":"<pre><code>outliers(X: tensor, n_permutations: int = 10) -&gt; tensor\n</code></pre> <p>Preferred implementation for outliers, where we calculate the sample probability for each sample in X by multiplying the probabilities of each feature according to chain rule of probability. The first feature is estimated by using a zero feature as input.</p> <p>Args     X: Samples to calculate the sample probability for, shape (n_samples, n_features)</p> <p>Returns:</p> Type Description <code>tensor</code> <p>Sample unnormalized probability for each sample in X, shape (n_samples,)</p>"},{"location":"api/tabpfn/TabPFNUnsupervisedModel/#scripts.estimator.unsupervised.TabPFNUnsupervisedModel.generate_synthetic_data","title":"generate_synthetic_data","text":"<pre><code>generate_synthetic_data(\n    n_samples=100, t=1.0, n_permutations=3\n)\n</code></pre> <p>Generate synthetic data using the trained models. Uses imputation method to generate synthetic data, passed with a matrix of nans. Samples are generated feature by feature in one pass, so samples are not dependent on each other per feature.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <p>int, default=100 Number of synthetic samples to generate.</p> <code>100</code> <code>t</code> <p>float, default=1.0 Temperature for sampling from the imputation distribution. Lower values result in more deterministic samples.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>torch.Tensor of shape (n_samples, n_features) Generated synthetic data.</p>"},{"location":"api/tabpfn/model.encoders/","title":"Model.encoders","text":""},{"location":"api/tabpfn/model.encoders/#model.encoders.InputEncoder","title":"InputEncoder","text":"<p>             Bases: <code>Module</code></p> <p>Base class for input encoders.</p> <p>All input encoders should subclass this class and implement the <code>forward</code> method.</p>"},{"location":"api/tabpfn/model.encoders/#model.encoders.InputEncoder.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre>"},{"location":"api/tabpfn/model.encoders/#model.encoders.InputEncoder.forward","title":"forward","text":"<pre><code>forward(x: Tensor, single_eval_pos: int) -&gt; Tensor\n</code></pre> <p>Encode the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to encode.</p> required <code>single_eval_pos</code> <code>int</code> <p>The position to use for single evaluation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The encoded tensor.</p>"},{"location":"api/tabpfn/model.encoders/#model.encoders.SequentialEncoder","title":"SequentialEncoder","text":"<p>             Bases: <code>Sequential</code>, <code>InputEncoder</code></p> <p>An encoder that applies a sequence of encoder steps.</p> <p>SequentialEncoder allows building an encoder from a sequence of EncoderSteps. The input is passed through each step in the provided order.</p>"},{"location":"api/tabpfn/model.encoders/#model.encoders.SequentialEncoder.__init__","title":"__init__","text":"<pre><code>__init__(*args, output_key: str = 'output', **kwargs)\n</code></pre> <p>Initialize the SequentialEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>A list of SeqEncStep instances to apply in order.</p> <code>()</code> <code>output_key</code> <code>str</code> <p>The key to use for the output of the encoder in the state dict.               Defaults to \"output\", i.e. <code>state[\"output\"]</code> will be returned.</p> <code>'output'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>torch.nn.Sequential</code>.</p> <code>{}</code>"},{"location":"api/tabpfn/model.encoders/#model.encoders.SequentialEncoder.forward","title":"forward","text":"<pre><code>forward(input: dict, **kwargs) -&gt; Tensor\n</code></pre> <p>Apply the sequence of encoder steps to the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>dict</code> <p>The input state dictionary.           If the input is not a dict and the first layer expects one input key,           the input tensor is mapped to the key expected by the first layer.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to each encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output of the final encoder step.</p>"},{"location":"api/tabpfn/model.encoders/#model.encoders.LinearInputEncoderStep","title":"LinearInputEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>A simple linear input encoder step.</p>"},{"location":"api/tabpfn/model.encoders/#model.encoders.LinearInputEncoderStep.__init__","title":"__init__","text":"<pre><code>__init__(\n    num_features: int,\n    emsize: int,\n    replace_nan_by_zero: bool = False,\n    bias: bool = True,\n    in_keys: tuple[str] = (\"main\"),\n    out_keys: tuple[str] = (\"output\"),\n)\n</code></pre> <p>Initialize the LinearInputEncoderStep.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>The number of input features.</p> required <code>emsize</code> <code>int</code> <p>The embedding size, i.e. the number of output features.</p> required <code>replace_nan_by_zero</code> <code>bool</code> <p>Whether to replace NaN values in the input by zero. Defaults to False.</p> <code>False</code> <code>bias</code> <code>bool</code> <p>Whether to use a bias term in the linear layer. Defaults to True.</p> <code>True</code> <code>in_keys</code> <code>tuple[str]</code> <p>The keys of the input tensors. Defaults to (\"main\",).</p> <code>('main')</code> <code>out_keys</code> <code>tuple[str]</code> <p>The keys to assign the output tensors to. Defaults to (\"output\",).</p> <code>('output')</code>"},{"location":"api/tabpfn/model.encoders/#model.encoders.LinearInputEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs\n)\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation.                                   Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"api/tabpfn/model.encoders/#model.encoders.NanHandlingEncoderStep","title":"NanHandlingEncoderStep","text":"<p>             Bases: <code>SeqEncStep</code></p> <p>Encoder step to handle NaN and infinite values in the input.</p>"},{"location":"api/tabpfn/model.encoders/#model.encoders.NanHandlingEncoderStep.__init__","title":"__init__","text":"<pre><code>__init__(\n    keep_nans: bool = True,\n    in_keys: tuple[str] = (\"main\"),\n    out_keys: tuple[str] = (\"main\", \"nan_indicators\"),\n)\n</code></pre> <p>Initialize the NanHandlingEncoderStep.</p> <p>Parameters:</p> Name Type Description Default <code>keep_nans</code> <code>bool</code> <p>Whether to keep NaN values as separate indicators. Defaults to True.</p> <code>True</code> <code>in_keys</code> <code>tuple[str]</code> <p>The keys of the input tensors. Must be a single key.</p> <code>('main')</code> <code>out_keys</code> <code>tuple[str]</code> <p>The keys to assign the output tensors to.                    Defaults to (\"main\", \"nan_indicators\").</p> <code>('main', 'nan_indicators')</code>"},{"location":"api/tabpfn/model.encoders/#model.encoders.NanHandlingEncoderStep.forward","title":"forward","text":"<pre><code>forward(\n    state: dict,\n    cache_trainset_representation: bool = False,\n    **kwargs\n)\n</code></pre> <p>Perform the forward pass of the encoder step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>dict</code> <p>The input state dictionary containing the input tensors.</p> required <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the training set representation.                                   Only supported for _fit and _transform (not _forward).</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the encoder step.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The updated state dictionary with the output tensors assigned to the output keys.</p>"},{"location":"api/tabpfn/model.mlp/","title":"Model.mlp","text":""},{"location":"api/tabpfn/model.mlp/#model.mlp.MLP","title":"MLP","text":"<p>             Bases: <code>Module</code></p> <p>Multi-Layer Perceptron (MLP) module.</p> <p>This module consists of two linear layers with an activation function in between. It supports various configurations such as the hidden size, activation function, initializing the output to zero, and recomputing the forward pass during backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The input and output size of the MLP.</p> required <code>hidden_size</code> <code>int</code> <p>The size of the hidden layer.</p> required <code>activation</code> <code>Union[Activation, str]</code> <p>The activation function to use. Can be either an Activation enum or a string representing the activation name.</p> required <code>device</code> <code>device</code> <p>The device to use for the linear layers.</p> required <code>dtype</code> <code>dtype</code> <p>The data type to use for the linear layers.</p> required <code>initialize_output_to_zero</code> <code>bool</code> <p>Whether to initialize the output layer weights to zero. Default is False.</p> <code>False</code> <code>recompute</code> <code>bool</code> <p>Whether to recompute the forward pass during backpropagation. This can save memory but increase computation time. Default is False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>linear1</code> <code>Linear</code> <p>The first linear layer.</p> <code>linear2</code> <code>Linear</code> <p>The second linear layer.</p> <code>activation</code> <code>Activation</code> <p>The activation function to use.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Performs the forward pass of the MLP. - x (torch.Tensor): The input tensor. - add_input (bool): Whether to add the input to the output. Default is False. - allow_inplace (bool): Indicates that 'x' is not used after the call and its buffer     can be reused for the output. The operation is not guaranteed to be inplace.     Default is False. - save_peak_mem_factor (Optional[int]): If provided, enables a memory-saving technique     that reduces peak memory usage during the forward pass. This requires 'add_input'     and 'allow_inplace' to be True. See the documentation of the decorator     'support_save_peak_mem_factor' for details. Default is None.</p> Example<pre><code>&gt;&gt;&gt; mlp = MLP(size=128, hidden_size=256, activation='gelu', device='cuda', dtype=torch.float32)\n&gt;&gt;&gt; x = torch.randn(32, 128, device='cuda', dtype=torch.float32)\n&gt;&gt;&gt; output = mlp(x)\n</code></pre>"},{"location":"api/tabpfn/model.mlp/#model.mlp.MLP.__init__","title":"__init__","text":"<pre><code>__init__(\n    size: int,\n    hidden_size: int,\n    activation: Union[Activation, str],\n    device,\n    dtype,\n    initialize_output_to_zero: bool = False,\n    recompute: bool = False,\n)\n</code></pre>"},{"location":"api/tabpfn/model.mlp/#model.mlp.MLP.forward","title":"forward","text":"<pre><code>forward(\n    x: Tensor,\n    add_input: bool = False,\n    allow_inplace: bool = False,\n    save_peak_mem_factor: Optional[int] = None,\n) -&gt; Tensor\n</code></pre>"},{"location":"api/tabpfn/model.transformer/","title":"Model.transformer","text":""},{"location":"api/tabpfn/model.transformer/#model.transformer.PerFeatureTransformer","title":"PerFeatureTransformer","text":"<p>             Bases: <code>Module</code></p> <p>A Transformer model processes a token per feature and sample.</p> <p>This model extends the standard Transformer architecture to operate on a per-feature basis. It allows for processing each feature separately while still leveraging the power of self-attention.</p> <p>The model consists of an encoder, decoder, and optional components such as a feature positional embedding and a separate decoder for each feature.</p>"},{"location":"api/tabpfn/model.transformer/#model.transformer.PerFeatureTransformer.__init__","title":"__init__","text":"<pre><code>__init__(\n    encoder: Module = encoders.SequentialEncoder(\n        encoders.LinearInputEncoderStep(\n            1,\n            DEFAULT_EMSIZE,\n            in_keys=[\"main\"],\n            out_keys=[\"output\"],\n        )\n    ),\n    ninp: int = DEFAULT_EMSIZE,\n    nhead: int = 4,\n    nhid: int = DEFAULT_EMSIZE * 4,\n    nlayers: int = 10,\n    y_encoder: Module = encoders.SequentialEncoder(\n        encoders.NanHandlingEncoderStep(),\n        encoders.LinearInputEncoderStep(\n            2,\n            DEFAULT_EMSIZE,\n            out_keys=[\"output\"],\n            in_keys=[\"main\", \"nan_indicators\"],\n        ),\n    ),\n    decoder_dict: Dict[\n        str, Tuple[Optional[Type[Module]], int]\n    ] = {\"standard\": (None, 1)},\n    init_method: Optional[str] = None,\n    activation: str = \"gelu\",\n    recompute_layer: bool = False,\n    min_num_layers_layer_dropout: Optional[int] = None,\n    repeat_same_layer: bool = False,\n    dag_pos_enc_dim: int = 0,\n    features_per_group: int = 1,\n    feature_positional_embedding: Optional[str] = None,\n    zero_init: bool = True,\n    use_separate_decoder: bool = False,\n    nlayers_decoder: Optional[int] = None,\n    use_encoder_compression_layer: bool = False,\n    precomputed_kv: Optional[\n        List[Union[Tensor, Tuple[Tensor, Tensor]]]\n    ] = None,\n    cache_trainset_representation: bool = False,\n    **layer_kwargs: Any\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Module</code> <p>Pass a nn.Module that takes in a batch of sequences of inputs and returns something of the shape (seq_len, batch_size, ninp)</p> <code>SequentialEncoder(LinearInputEncoderStep(1, DEFAULT_EMSIZE, in_keys=['main'], out_keys=['output']))</code> <code>ninp</code> <code>int</code> <p>Input dimension, also called the embedding dimension</p> <code>DEFAULT_EMSIZE</code> <code>nhead</code> <code>int</code> <p>Number of attention heads</p> <code>4</code> <code>nhid</code> <code>int</code> <p>Hidden dimension in the MLP layers</p> <code>DEFAULT_EMSIZE * 4</code> <code>nlayers</code> <code>int</code> <p>Number of layers, each consisting of a multi-head attention layer and an MLP layer</p> <code>10</code> <code>y_encoder</code> <code>Module</code> <p>A nn.Module that takes in a batch of sequences of outputs and returns something of the shape (seq_len, batch_size, ninp)</p> <code>SequentialEncoder(NanHandlingEncoderStep(), LinearInputEncoderStep(2, DEFAULT_EMSIZE, out_keys=['output'], in_keys=['main', 'nan_indicators']))</code> <code>decoder_dict</code> <code>Dict[str, Tuple[Optional[Type[Module]], int]]</code> <code>{'standard': (None, 1)}</code> <code>activation</code> <code>str</code> <p>An activation function, e.g. \"gelu\" or \"relu\"</p> <code>'gelu'</code> <code>recompute_layer</code> <code>bool</code> <p>If True, the transformer layers will be recomputed on each forward pass in training. This is useful to save memory.</p> <code>False</code> <code>min_num_layers_layer_dropout</code> <code>Optional[int]</code> <p>if this is set, it enables to drop the last layers randomly during training up to this number.</p> <code>None</code> <code>repeat_same_layer</code> <code>bool</code> <p>If True, the same layer will be used for all layers. This is useful to save memory on weights.</p> <code>False</code> <code>features_per_group</code> <code>int</code> <p>If &gt; 1, the features will be grouped into groups of this size and the attention is across groups.</p> <code>1</code> <code>feature_positional_embedding</code> <code>Optional[str]</code> <p>There is a risk that our models confuse features with each other. This positional embedding is added to the features to help the model distinguish them. We recommend setting this to \"subspace\".</p> <code>None</code> <code>zero_init</code> <code>bool</code> <p>If True, the last sublayer of each attention and MLP layer will be initialized with zeros. Thus, the layers will start out as identity functions.</p> <code>True</code> <code>use_separate_decoder</code> <code>bool</code> <p>If True, the decoder will be separate from the encoder.</p> <code>False</code> <code>nlayers_decoder</code> <code>Optional[int]</code> <p>If use_separate_decoder is True, this is the number of layers in the decoder. The default is to use \u2153 of the layers for the decoder and \u2154 for the encoder.</p> <code>None</code> <code>use_encoder_compression_layer</code> <code>bool</code> <p>Experimental</p> <code>False</code> <code>precomputed_kv</code> <code>Optional[List[Union[Tensor, Tuple[Tensor, Tensor]]]]</code> <p>Experimental</p> <code>None</code> <code>layer_kwargs</code> <code>Any</code> <code>{}</code>"},{"location":"api/tabpfn/model.transformer/#model.transformer.PerFeatureTransformer.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Performs a forward pass through the model.</p> <p>This method supports multiple calling conventions: - model(train_x, train_y, test_x, **kwargs) - model((x,y), **kwargs) - model((style,x,y), **kwargs)</p> <p>Parameters:</p> Name Type Description Default <code>train_x</code> <p>The input data for the training set.</p> required <code>train_y</code> <p>The target data for the training set.</p> required <code>test_x</code> <p>The input data for the test set.</p> required <code>x</code> <p>The input data.</p> required <code>y</code> <p>The target data.</p> required <code>style</code> <p>The style vector.</p> required <code>single_eval_pos</code> <p>The position to evaluate at.</p> required <code>only_return_standard_out</code> <p>Whether to only return the standard output.</p> required <code>data_dags</code> <p>The data DAGs for each example.</p> required <code>categorical_inds</code> <p>The indices of categorical features.</p> required <code>freeze_kv</code> <p>Whether to freeze the key and value weights.</p> required <p>Returns:</p> Type Description <p>The output of the model, which can be a tensor or a dictionary of tensors.</p>"},{"location":"api/tabpfn/model.transformer/#model.layer.PerFeatureEncoderLayer","title":"PerFeatureEncoderLayer","text":"<p>             Bases: <code>Module</code></p> <p>Transformer encoder layer that processes each feature block separately.</p> <p>This layer consists of multi-head attention between features, multi-head attention between items, and feedforward neural networks (MLPs). It supports various configurations and optimization options.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>The dimensionality of the input and output embeddings.</p> required <code>nhead</code> <code>int</code> <p>The number of attention heads.</p> required <code>dim_feedforward</code> <code>Optional[int]</code> <p>The dimensionality of the feedforward network. Default is None (2 * d_model).</p> <code>None</code> <code>activation</code> <code>str</code> <p>The activation function to use in the MLPs. Default is \"relu\".</p> <code>'relu'</code> <code>layer_norm_eps</code> <code>float</code> <p>The epsilon value for layer normalization. Default is 1e-5.</p> <code>1e-05</code> <code>pre_norm</code> <code>bool</code> <p>Whether to apply layer normalization before or after the attention and MLPs. Default is False.</p> <code>False</code> <code>device</code> <code>Optional[device]</code> <p>The device to use for the layer parameters. Default is None.</p> <code>None</code> <code>dtype</code> <code>Optional[dtype]</code> <p>The data type to use for the layer parameters. Default is None.</p> <code>None</code> <code>recompute_attn</code> <code>bool</code> <p>Whether to recompute attention during backpropagation. Default is False.</p> <code>False</code> <code>second_mlp</code> <code>bool</code> <p>Whether to include a second MLP in the layer. Default is False.</p> <code>False</code> <code>layer_norm_with_elementwise_affine</code> <code>bool</code> <p>Whether to use elementwise affine parameters in layer normalization. Default is False.</p> <code>False</code> <code>zero_init</code> <code>bool</code> <p>Whether to initialize the output of the MLPs to zero. Default is False.</p> <code>False</code> <code>save_peak_mem_factor</code> <code>Optional[int]</code> <p>The factor to save peak memory, only effective with post-norm. Default is None.</p> <code>None</code> <code>attention_between_features</code> <code>bool</code> <p>Whether to apply attention between feature blocks. Default is True.</p> <code>True</code> <code>multiquery_item_attention</code> <code>bool</code> <p>Whether to use multiquery attention for items. Default is False.</p> <code>False</code> <code>multiquery_item_attention_for_test_set</code> <code>bool</code> <p>Whether to use multiquery attention for the test set. Default is False.</p> <code>False</code> <code>attention_init_gain</code> <code>float</code> <p>The gain value for initializing attention parameters. Default is 1.0.</p> <code>1.0</code> <code>d_k</code> <code>Optional[int]</code> <p>The dimensionality of the query and key vectors. Default is None (d_model // nhead).</p> <code>None</code> <code>d_v</code> <code>Optional[int]</code> <p>The dimensionality of the value vectors. Default is None (d_model // nhead).</p> <code>None</code> <code>precomputed_kv</code> <code>Union[None, Tensor, Tuple[Tensor, Tensor]]</code> <p>Precomputed key-value pairs for attention. Default is None.</p> <code>None</code>"},{"location":"api/tabpfn/model.transformer/#model.layer.PerFeatureEncoderLayer.__init__","title":"__init__","text":"<pre><code>__init__(\n    d_model: int,\n    nhead: int,\n    dim_feedforward: Optional[int] = None,\n    activation: str = \"relu\",\n    layer_norm_eps: float = 1e-05,\n    pre_norm: bool = False,\n    device: Optional[device] = None,\n    dtype: Optional[dtype] = None,\n    recompute_attn: bool = False,\n    second_mlp: bool = False,\n    layer_norm_with_elementwise_affine: bool = False,\n    zero_init: bool = False,\n    save_peak_mem_factor: Optional[int] = None,\n    attention_between_features: bool = True,\n    multiquery_item_attention: bool = False,\n    multiquery_item_attention_for_test_set: bool = False,\n    two_sets_of_queries: bool = False,\n    attention_init_gain: float = 1.0,\n    d_k: Optional[int] = None,\n    d_v: Optional[int] = None,\n    precomputed_kv: Union[\n        None, Tensor, Tuple[Tensor, Tensor]\n    ] = None,\n) -&gt; None\n</code></pre>"},{"location":"api/tabpfn/model.transformer/#model.layer.PerFeatureEncoderLayer.forward","title":"forward","text":"<pre><code>forward(\n    state: Tensor,\n    single_eval_pos: Optional[int] = None,\n    cache_trainset_representation: bool = False,\n    att_src: Optional[Tensor] = None,\n) -&gt; Tensor\n</code></pre> <p>Pass the input through the encoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Tensor</code> <p>The transformer state passed as input to the layer of shape (batch_size, num_items, num_feature_blocks, d_model).</p> required <code>single_eval_pos</code> <code>Optional[int]</code> <p>The position from which on everything is treated as test set. Default is None.</p> <code>None</code> <code>cache_trainset_representation</code> <code>bool</code> <p>Whether to cache the trainset representation. If single_eval_pos is set (&gt; 0 and not None), create a cache of the trainset KV. This may require a lot of memory. Otherwise, use cached KV representations for inference. Default is False.</p> <code>False</code> <code>att_src</code> <code>Optional[Tensor]</code> <p>The tensor to attend to from the final layer of the encoder. It has a shape of (batch_size, num_train_items, num_feature_blocks, d_model). This does not work with multiquery_item_attention_for_test_set and cache_trainset_representation at this point. Combining would be possible, however. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The transformer state passed through the encoder layer.</p>"},{"location":"api/tabpfn/tabpfn_classifier/","title":"Tabpfn classifier","text":""},{"location":"api/tabpfn/tabpfn_classifier/#scripts.estimator.TabPFNClassifier","title":"TabPFNClassifier","text":"<p>             Bases: <code>TabPFNBaseModel</code>, <code>ClassifierMixin</code></p>"},{"location":"api/tabpfn/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_path: str | Path = Path(local_model_path)\n    / \"model_hans_classification.ckpt\",\n    n_estimators: int = 4,\n    preprocess_transforms: Tuple[\n        PreprocessorConfig, ...\n    ] = (\n        PreprocessorConfig(\n            \"quantile_uni_coarse\",\n            append_original=True,\n            categorical_name=\"ordinal_very_common_categories_shuffled\",\n            global_transformer_name=\"svd\",\n            subsample_features=-1,\n        ),\n        PreprocessorConfig(\n            \"none\",\n            categorical_name=\"numeric\",\n            subsample_features=-1,\n        ),\n    ),\n    feature_shift_decoder: str = \"shuffle\",\n    normalize_with_test: bool = False,\n    average_logits: bool = False,\n    optimize_metric: ClassificationOptimizationMetricType = \"roc\",\n    transformer_predict_kwargs: Optional[Dict] = None,\n    multiclass_decoder=\"shuffle\",\n    softmax_temperature: Optional[float] = math.exp(-0.1),\n    use_poly_features: bool = False,\n    max_poly_features: int = 50,\n    transductive: bool = False,\n    remove_outliers: float = 12.0,\n    add_fingerprint_features: bool = True,\n    subsample_samples: float = -1,\n    model: Optional[Module] = None,\n    model_config: Optional[Dict] = None,\n    fit_at_predict_time: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n    seed: Optional[int] = 0,\n    show_progress: bool = True,\n    batch_size_inference: int = 1,\n    fp16_inference: bool = True,\n    save_peak_memory: Literal[\n        \"True\", \"False\", \"auto\"\n    ] = \"True\",\n    maximum_free_memory_in_gb: Optional[float] = None,\n    split_test_samples: float | str = 1,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str | Path</code> <p>The model string is the path to the model.</p> <code>Path(local_model_path) / 'model_hans_classification.ckpt'</code> <code>n_estimators</code> <code>int</code> <p>The number of ensemble configurations to use, the most important setting.</p> <code>4</code> <code>preprocess_transforms</code> <code>Tuple[PreprocessorConfig, ...]</code> <p>A tuple of strings, specifying the preprocessing steps to use. You can use the following strings as elements '(none|power|quantile|robust)_all', where the first part specifies the preprocessing step and the second part specifies the features to apply it to and finally '_and_none' specifies that the original features should be added back to the features in plain. Finally, you can combine all strings without <code>_all</code> with <code>_onehot</code> to apply one-hot encoding to the categorical features specified with <code>self.fit(..., categorical_features=...)</code>.</p> <code>(PreprocessorConfig('quantile_uni_coarse', append_original=True, categorical_name='ordinal_very_common_categories_shuffled', global_transformer_name='svd', subsample_features=-1), PreprocessorConfig('none', categorical_name='numeric', subsample_features=-1))</code> <code>feature_shift_decoder</code> <code>str</code> <p>[\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.</p> <code>'shuffle'</code> <code>normalize_with_test</code> <code>bool</code> <p>If True, the test set is used to normalize the data, otherwise the training set is used only.</p> <code>False</code> <code>average_logits</code> <code>bool</code> <p>Whether to average logits or probabilities for ensemble members.</p> <code>False</code> <code>optimize_metric</code> <code>ClassificationOptimizationMetricType</code> <p>The optimization metric to use.</p> <code>'roc'</code> <code>transformer_predict_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments to pass to the transformer predict method.</p> <code>None</code> <code>multiclass_decoder</code> <p>The multiclass decoder to use.</p> <code>'shuffle'</code> <code>softmax_temperature</code> <code>Optional[float]</code> <p>A log spaced temperature, it will be applied as logits &lt;- logits/softmax_temperature.</p> <code>exp(-0.1)</code> <code>use_poly_features</code> <code>bool</code> <p>Whether to use polynomial features as the last preprocessing step.</p> <code>False</code> <code>max_poly_features</code> <code>int</code> <p>Maximum number of polynomial features to use, None means unlimited.</p> <code>50</code> <code>transductive</code> <code>bool</code> <p>Whether to use transductive learning.</p> <code>False</code> <code>remove_outliers</code> <code>float</code> <p>If not 0.0, will remove outliers from the input features, where values with a standard deviation larger than remove_outliers will be removed.</p> <code>12.0</code> <code>add_fingerprint_features</code> <code>bool</code> <p>If True, will add one feature of random values, that will be added to the input features. This helps discern duplicated samples in the transformer model.</p> <code>True</code> <code>subsample_samples</code> <code>float</code> <p>If not None, will use a random subset of the samples for training in each ensemble configuration. If 1 or above, this will subsample to the specified number of samples. If in 0 to 1, the value is viewed as a fraction of the training set size.</p> <code>-1</code> <code>model</code> <code>Optional[Module]</code> <p>The model, if you want to specify it directly, this is used in combination with model_config.</p> <code>None</code> <code>model_config</code> <code>Optional[Dict]</code> <p>The config, if you want to specify it directly, this is used in combination with model.</p> <code>None</code> <code>fit_at_predict_time</code> <code>bool</code> <p>Whether to train the model lazily, i.e. only when it is needed for inference in predict[_proba].</p> <code>True</code> <code>device</code> <code>Literal['cuda', 'cpu', 'auto']</code> <p>The device to use for inference, \"auto\" means that it will use cuda if available, otherwise cpu.</p> <code>'auto'</code> <code>seed</code> <code>Optional[int]</code> <p>The default seed to use for the order of the ensemble configurations, a seed of None will not.</p> <code>0</code> <code>show_progress</code> <code>bool</code> <p>Whether to show progress bars during training and inference.</p> <code>True</code> <code>batch_size_inference</code> <code>int</code> <p>The batch size to use for inference, this does not affect the results, just the memory usage and speed. A higher batch size is faster but uses more memory. Setting the batch size to None means that the batch size is automatically determined based on the memory usage and the maximum free memory specified with <code>maximum_free_memory_in_gb</code>.</p> <code>1</code> <code>fp16_inference</code> <code>bool</code> <p>Whether to use fp16 for inference on GPU, does not affect CPU inference.</p> <code>True</code> <code>save_peak_memory</code> <code>Literal['True', 'False', 'auto']</code> <p>Whether to save the peak memory usage of the model, can enable up to 8 times larger datasets to fit into memory. \"True\", means always enabled, \"False\", means always disabled, \"auto\" means that it will be set based on the memory usage.</p> <code>'True'</code>"},{"location":"api/tabpfn/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.fit","title":"fit","text":"<pre><code>fit(\n    X: Union[ndarray, Tensor],\n    y: Union[ndarray, Tensor],\n    additional_y: Optional[Dict[str, Tensor]] = None,\n) -&gt; TabPFNClassifier\n</code></pre> <p>Fits the TabPFNClassifier model to the input data <code>X</code> and <code>y</code>.</p> <p>The actual training logic is delegated to the <code>_fit</code> method, which should be implemented by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, Tensor]</code> <p>The input feature matrix of shape (n_samples, n_features).</p> required <code>y</code> <code>Union[ndarray, Tensor]</code> <p>The target labels of shape (n_samples,).</p> required <code>additional_y</code> <code>Optional[Dict[str, Tensor]]</code> <p>Additional labels to use during training.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TabPFNClassifier</code> <code>TabPFNClassifier</code> <p>The fitted model object (self).</p>"},{"location":"api/tabpfn/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X, return_winning_probability=False)\n</code></pre> <p>Predict the class labels for the input samples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The input samples.</p> required <code>return_winning_probability</code> <code>bool</code> <p>Whether to return the winning probability.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>array</code> <p>The predicted class labels.</p>"},{"location":"api/tabpfn/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X, additional_y=None)\n</code></pre> <p>Calls the transformer to predict the probabilities of the classes of the X test inputs given the previous set training dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>test datapoints</p> required"},{"location":"api/tabpfn/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.predict_y_proba","title":"predict_y_proba","text":"<pre><code>predict_y_proba(X, y)\n</code></pre> <p>Predict the probability of the target labels <code>y</code> given the input samples <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The input samples.</p> required <code>y</code> <code>array - like</code> <p>The target labels.</p> required <p>Returns:</p> Name Type Description <code>array</code> <p>The predicted probabilities of the target labels.</p>"},{"location":"api/tabpfn/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features: List[int])\n</code></pre> <p>Set the categorical features to use for the model.</p> <p>These categorical features might be overridden by the preprocessing steps. This is controlled by i) <code>max_unique_values_as_categorical_feature</code>, the maximum number of unique values a feature can have to be considered a categorical feature. Features with more unique values are considered numerical features. ii) <code>min_unique_values_as_numerical_feature</code> the minimum number of unique values a feature can have to be considered a numerical feature. Features with less unique values are considered categorical features.</p> <p>:param categorical_features: The feature indices of the categorical features</p>"},{"location":"api/tabpfn/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.score","title":"score","text":"<pre><code>score(X, y, sample_weight=None)\n</code></pre> <p>Compute the score of the model on the given test data and labels.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>The input samples.</p> required <code>y</code> <code>array - like</code> <p>The true labels for <code>X</code>.</p> required <code>sample_weight</code> <code>array - like</code> <p>Sample weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The computed score.</p>"},{"location":"api/tabpfn/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.estimate_memory_usage","title":"estimate_memory_usage","text":"<pre><code>estimate_memory_usage(\n    X: ndarray | tensor,\n    unit: Literal[\"b\", \"mb\", \"gb\"] = \"gb\",\n    eval_position: int = -1,\n    **overwrite_params\n) -&gt; float | None\n</code></pre> <p>Estimates the memory usage of the model.</p> <p>Peak memory usage is accurate for \u00b4save_peak_mem_factor\u00b4 in O(n_feats, n_samples) on average but with significant outliers (2x). Also this calculation does not include baseline usage and constant offsets. Baseline memory usage can be ignored if we set the maximum memory usage to the default None which uses the free memory of the system. The constant offsets are not significant for large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if <code>self.fit_at_predict_time</code> and train only otherwise. If you add a batch dimension at position 1 to the table this is used as the batch size used during inference, otherwise this depends on the <code>batch_size_inference</code> and <code>n_estimators</code>.</p> required <code>unit</code> <code>Literal['b', 'mb', 'gb']</code> <p>The unit to return the memory usage in (bytes, megabytes, or gigabytes).</p> <code>'gb'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated memory usage in bytes.</p>"},{"location":"api/tabpfn/tabpfn_classifier/#scripts.estimator.TabPFNClassifier.estimate_computation_usage","title":"estimate_computation_usage","text":"<pre><code>estimate_computation_usage(\n    X: ndarray,\n    unit: Literal[\n        \"sequential_flops\", \"s\"\n    ] = \"sequential_flops\",\n    eval_position: int = -1,\n    **overwrite_params\n) -&gt; float | None\n</code></pre> <p>Estimates the sequential computation usage of the model. Those are the operations that are not parallelizable and are the main bottleneck for the computation time.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if</p> required <code>unit</code> <code>str</code> <p>The unit to return the computation usage in.</p> <code>'sequential_flops'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated computation usage in unit of choice.</p>"},{"location":"api/tabpfn/tabpfn_regressor/","title":"Tabpfn regressor","text":""},{"location":"api/tabpfn/tabpfn_regressor/#scripts.estimator.TabPFNRegressor","title":"TabPFNRegressor","text":"<p>             Bases: <code>TabPFNBaseModel</code>, <code>RegressorMixin</code></p>"},{"location":"api/tabpfn/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_path: str | Path = str(\n        Path(local_model_path).resolve()\n        / \"model_hans_regression.ckpt\"\n    ),\n    n_estimators: int = 8,\n    preprocess_transforms: Tuple[\n        PreprocessorConfig, ...\n    ] = (\n        PreprocessorConfig(\n            \"quantile_uni\",\n            append_original=True,\n            categorical_name=\"ordinal_very_common_categories_shuffled\",\n            global_transformer_name=\"svd\",\n        ),\n        PreprocessorConfig(\n            \"safepower\", categorical_name=\"onehot\"\n        ),\n    ),\n    feature_shift_decoder: str = \"shuffle\",\n    normalize_with_test: bool = False,\n    average_logits: bool = False,\n    optimize_metric: RegressionOptimizationMetricType = \"rmse\",\n    transformer_predict_kwargs: Optional[Dict] = None,\n    softmax_temperature: Optional[float] = math.exp(-0.1),\n    use_poly_features: bool = False,\n    max_poly_features: int = 50,\n    transductive: bool = False,\n    remove_outliers=-1,\n    regression_y_preprocess_transforms: Optional[\n        Tuple[\n            None\n            | Literal[\n                \"safepower\", \"power\", \"quantile_norm\"\n            ],\n            ...,\n        ]\n    ] = (None, \"safepower\"),\n    add_fingerprint_features: bool = True,\n    cancel_nan_borders: bool = True,\n    super_bar_dist_averaging: bool = False,\n    subsample_samples: float = -1,\n    model: Optional[Module] = None,\n    model_config: Optional[Dict] = None,\n    fit_at_predict_time: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n    seed: Optional[int] = 0,\n    show_progress: bool = True,\n    batch_size_inference: int = 1,\n    fp16_inference: bool = True,\n    save_peak_memory: Literal[\n        \"True\", \"False\", \"auto\"\n    ] = \"True\",\n    maximum_free_memory_in_gb: Optional[float] = None,\n    split_test_samples: float | str = 1,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str | Path</code> <p>The model string is the path to the model.</p> <code>str(resolve() / 'model_hans_regression.ckpt')</code> <code>n_estimators</code> <code>int</code> <p>The number of ensemble configurations to use, the most important setting.</p> <code>8</code> <code>preprocess_transforms</code> <code>Tuple[PreprocessorConfig, ...]</code> <p>A tuple of strings, specifying the preprocessing steps to use. You can use the following strings as elements '(none|power|quantile_norm|quantile_uni|quantile_uni_coarse|robust...)_all', where the first part specifies the preprocessing step (see <code>.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()</code>) and the second part specifies the features to apply it to and finally '_and_none' specifies that the original features should be added back to the features in plain. Finally, you can combine all strings without <code>_all</code> with <code>_onehot</code> to apply one-hot encoding to the categorical features specified with <code>self.fit(..., categorical_features=...)</code>.</p> <code>(PreprocessorConfig('quantile_uni', append_original=True, categorical_name='ordinal_very_common_categories_shuffled', global_transformer_name='svd'), PreprocessorConfig('safepower', categorical_name='onehot'))</code> <code>feature_shift_decoder</code> <code>str</code> <p>[\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.</p> <code>'shuffle'</code> <code>normalize_with_test</code> <code>bool</code> <p>If True, the test set is used to normalize the data, otherwise the training set is used only.</p> <code>False</code> <code>average_logits</code> <code>bool</code> <p>Whether to average logits or probabilities for ensemble members.</p> <code>False</code> <code>optimize_metric</code> <code>RegressionOptimizationMetricType</code> <p>The optimization metric to use.</p> <code>'rmse'</code> <code>transformer_predict_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments to pass to the transformer predict method.</p> <code>None</code> <code>softmax_temperature</code> <code>Optional[float]</code> <p>A log spaced temperature, it will be applied as logits &lt;- logits/softmax_temperature.</p> <code>exp(-0.1)</code> <code>use_poly_features</code> <code>bool</code> <p>Whether to use polynomial features as the last preprocessing step.</p> <code>False</code> <code>max_poly_features</code> <code>int</code> <p>Maximum number of polynomial features to use, None means unlimited.</p> <code>50</code> <code>transductive</code> <code>bool</code> <p>Whether to use transductive learning.</p> <code>False</code> <code>remove_outliers</code> <p>If not 0.0, will remove outliers from the input features, where values with a standard deviation larger than remove_outliers will be removed.</p> <code>-1</code> <code>regression_y_preprocess_transforms</code> <code>Optional[Tuple[None | Literal['safepower', 'power', 'quantile_norm'], ...]]</code> <p>Preprocessing transforms for the target variable. This can be one from <code>.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()</code>, e.g. \"power\". This can also be None to not transform the targets, beside a simple mean/variance normalization.</p> <code>(None, 'safepower')</code> <code>add_fingerprint_features</code> <code>bool</code> <p>If True, will add one feature of random values, that will be added to the input features. This helps discern duplicated samples in the transformer model.</p> <code>True</code> <code>cancel_nan_borders</code> <code>bool</code> <p>Whether to ignore buckets that are tranformed to nan values by inverting a <code>regression_y_preprocess_transform</code>. This should be set to True, only set this to False if you know what you are doing.</p> <code>True</code> <code>super_bar_dist_averaging</code> <code>bool</code> <p>If we use <code>regression_y_preprocess_transforms</code> we need to average the predictions over the different configurations. The different configurations all come with different bar_distributions (Riemann distributions), though. The default is for us to aggregate all bar distributions using simply scaled borders in the bar distribution, scaled by the mean and std of the target variable. If you set this to True, a new bar distribution will be built using all the borders generated in the different configurations.</p> <code>False</code> <code>subsample_samples</code> <code>float</code> <p>If not None, will use a random subset of the samples for training in each ensemble configuration. If 1 or above, this will subsample to the specified number of samples. If in 0 to 1, the value is viewed as a fraction of the training set size.</p> <code>-1</code> <code>model</code> <code>Optional[Module]</code> <p>The model, if you want to specify it directly, this is used in combination with model_config.</p> <code>None</code> <code>model_config</code> <code>Optional[Dict]</code> <p>The config, if you want to specify it directly, this is used in combination with model.</p> <code>None</code> <code>fit_at_predict_time</code> <code>bool</code> <p>Whether to train the model lazily, i.e. only when it is needed for inference in predict[_proba].</p> <code>True</code> <code>device</code> <code>Literal['cuda', 'cpu', 'auto']</code> <p>The device to use for inference, \"auto\" means that it will use cuda if available, otherwise cpu.</p> <code>'auto'</code> <code>seed</code> <code>Optional[int]</code> <p>The default seed to use for the order of the ensemble configurations, a seed of None will not.</p> <code>0</code> <code>show_progress</code> <code>bool</code> <p>Whether to show progress bars during training and inference.</p> <code>True</code> <code>batch_size_inference</code> <code>int</code> <p>The batch size to use for inference, this does not affect the results, just the memory usage and speed. A higher batch size is faster but uses more memory. Setting the batch size to None means that the batch size is automatically determined based on the memory usage and the maximum free memory specified with <code>maximum_free_memory_in_gb</code>.</p> <code>1</code> <code>fp16_inference</code> <code>bool</code> <p>Whether to use fp16 for inference on GPU, does not affect CPU inference.</p> <code>True</code> <code>save_peak_memory</code> <code>Literal['True', 'False', 'auto']</code> <p>Whether to save the peak memory usage of the model, can enable up to 8 times larger datasets to fit into memory. \"True\", means always enabled, \"False\", means always disabled, \"auto\" means that it will be set based on the memory usage.</p> <code>'True'</code>"},{"location":"api/tabpfn/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.fit","title":"fit","text":"<pre><code>fit(X, y, additional_y=None) -&gt; TabPFNBaseModel\n</code></pre> <p>Fits the model to the input data <code>X</code> and <code>y</code>.</p> <p>The actual training logic is delegated to the <code>_fit</code> method, which should be implemented by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, Tensor]</code> <p>The input feature matrix of shape (n_samples, n_features).</p> required <code>y</code> <code>Union[ndarray, Tensor]</code> <p>The target labels of shape (n_samples,).</p> required <code>additional_y</code> <code>Optional[Dict[str, Tensor]]</code> <p>Additional labels to use during training.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>TabPFNBaseModel</code> <code>TabPFNBaseModel</code> <p>The fitted model object (self).</p>"},{"location":"api/tabpfn/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(X, additional_y=None) -&gt; ndarray\n</code></pre>"},{"location":"api/tabpfn/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.predict_y_proba","title":"predict_y_proba","text":"<pre><code>predict_y_proba(\n    X: ndarray | Tensor, y: ndarray | Tensor\n) -&gt; ndarray\n</code></pre> <p>Predicts the probability of the target y given the input X.</p>"},{"location":"api/tabpfn/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.set_categorical_features","title":"set_categorical_features","text":"<pre><code>set_categorical_features(categorical_features: List[int])\n</code></pre> <p>Set the categorical features to use for the model.</p> <p>These categorical features might be overridden by the preprocessing steps. This is controlled by i) <code>max_unique_values_as_categorical_feature</code>, the maximum number of unique values a feature can have to be considered a categorical feature. Features with more unique values are considered numerical features. ii) <code>min_unique_values_as_numerical_feature</code> the minimum number of unique values a feature can have to be considered a numerical feature. Features with less unique values are considered categorical features.</p> <p>:param categorical_features: The feature indices of the categorical features</p>"},{"location":"api/tabpfn/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.score","title":"score","text":"<pre><code>score(X, y, sample_weight=None)\n</code></pre>"},{"location":"api/tabpfn/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.estimate_memory_usage","title":"estimate_memory_usage","text":"<pre><code>estimate_memory_usage(\n    X: ndarray | tensor,\n    unit: Literal[\"b\", \"mb\", \"gb\"] = \"gb\",\n    eval_position: int = -1,\n    **overwrite_params\n) -&gt; float | None\n</code></pre> <p>Estimates the memory usage of the model.</p> <p>Peak memory usage is accurate for \u00b4save_peak_mem_factor\u00b4 in O(n_feats, n_samples) on average but with significant outliers (2x). Also this calculation does not include baseline usage and constant offsets. Baseline memory usage can be ignored if we set the maximum memory usage to the default None which uses the free memory of the system. The constant offsets are not significant for large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if <code>self.fit_at_predict_time</code> and train only otherwise. If you add a batch dimension at position 1 to the table this is used as the batch size used during inference, otherwise this depends on the <code>batch_size_inference</code> and <code>n_estimators</code>.</p> required <code>unit</code> <code>Literal['b', 'mb', 'gb']</code> <p>The unit to return the memory usage in (bytes, megabytes, or gigabytes).</p> <code>'gb'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated memory usage in bytes.</p>"},{"location":"api/tabpfn/tabpfn_regressor/#scripts.estimator.TabPFNRegressor.estimate_computation_usage","title":"estimate_computation_usage","text":"<pre><code>estimate_computation_usage(\n    X: ndarray,\n    unit: Literal[\n        \"sequential_flops\", \"s\"\n    ] = \"sequential_flops\",\n    eval_position: int = -1,\n    **overwrite_params\n) -&gt; float | None\n</code></pre> <p>Estimates the sequential computation usage of the model. Those are the operations that are not parallelizable and are the main bottleneck for the computation time.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. X should represent the concat of train and test in if</p> required <code>unit</code> <code>str</code> <p>The unit to return the computation usage in.</p> <code>'sequential_flops'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>float | None</code> <p>The estimated computation usage in unit of choice.</p>"},{"location":"api/tabpfn-client/DatasetUIDCacheManager/","title":"DatasetUIDCacheManager","text":""},{"location":"api/tabpfn-client/DatasetUIDCacheManager/#tabpfn_client.client.DatasetUIDCacheManager","title":"DatasetUIDCacheManager","text":"<p>Manages a cache of the last 50 uploaded datasets, tracking dataset hashes and their UIDs.</p>"},{"location":"api/tabpfn-client/DatasetUIDCacheManager/#tabpfn_client.client.DatasetUIDCacheManager.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre>"},{"location":"api/tabpfn-client/DatasetUIDCacheManager/#tabpfn_client.client.DatasetUIDCacheManager.load_cache","title":"load_cache","text":"<pre><code>load_cache()\n</code></pre> <p>Loads the cache from disk if it exists, otherwise initializes an empty cache.</p>"},{"location":"api/tabpfn-client/DatasetUIDCacheManager/#tabpfn_client.client.DatasetUIDCacheManager.get_dataset_uid","title":"get_dataset_uid","text":"<pre><code>get_dataset_uid(*args)\n</code></pre> <p>Generates hash by all received arguments and returns cached dataset uid if in cache, otherwise None.</p>"},{"location":"api/tabpfn-client/DatasetUIDCacheManager/#tabpfn_client.client.DatasetUIDCacheManager.add_dataset_uid","title":"add_dataset_uid","text":"<pre><code>add_dataset_uid(hash: str, dataset_uid: str)\n</code></pre> <p>Adds a new dataset to the cache, removing the oldest item if the cache exceeds 50 entries. Assumes the dataset is not already in the cache.</p>"},{"location":"api/tabpfn-client/DatasetUIDCacheManager/#tabpfn_client.client.DatasetUIDCacheManager.save_cache","title":"save_cache","text":"<pre><code>save_cache()\n</code></pre> <p>Saves the current cache to disk.</p>"},{"location":"api/tabpfn-client/DatasetUIDCacheManager/#tabpfn_client.client.DatasetUIDCacheManager.delete_uid","title":"delete_uid","text":"<pre><code>delete_uid(dataset_uid: str) -&gt; Optional[str]\n</code></pre> <p>Deletes an entry from the cache based on the dataset UID.</p>"},{"location":"api/tabpfn-client/InferenceClient/","title":"InferenceClient","text":""},{"location":"api/tabpfn-client/InferenceClient/#tabpfn_client.service_wrapper.InferenceClient","title":"InferenceClient","text":"<p>             Bases: <code>ServiceClientWrapper</code></p> <p>Wrapper of ServiceClient to handle inference, including: - fitting - prediction</p>"},{"location":"api/tabpfn-client/InferenceClient/#tabpfn_client.service_wrapper.InferenceClient.__init__","title":"__init__","text":"<pre><code>__init__(service_client=ServiceClient())\n</code></pre>"},{"location":"api/tabpfn-client/InferenceClient/#tabpfn_client.service_wrapper.InferenceClient.fit","title":"fit","text":"<pre><code>fit(X, y) -&gt; str\n</code></pre>"},{"location":"api/tabpfn-client/InferenceClient/#tabpfn_client.service_wrapper.InferenceClient.predict","title":"predict","text":"<pre><code>predict(\n    X,\n    task: Literal[\"classification\", \"regression\"],\n    train_set_uid: str,\n    config=None,\n    X_train=None,\n    y_train=None,\n)\n</code></pre>"},{"location":"api/tabpfn-client/PreprocessorConfig/","title":"PreprocessorConfig","text":""},{"location":"api/tabpfn-client/PreprocessorConfig/#tabpfn_client.estimator.PreprocessorConfig","title":"PreprocessorConfig  <code>dataclass</code>","text":"<p>Configuration for data preprocessors.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal</code> <p>Name of the preprocessor.</p> <code>categorical_name</code> <code>Literal</code> <p>Name of the categorical encoding method. Valid options are \"none\", \"numeric\",                     \"onehot\", \"ordinal\", \"ordinal_shuffled\". Default is \"none\".</p> <code>append_original</code> <code>bool</code> <p>Whether to append the original features to the transformed features. Default is False.</p> <code>subsample_features</code> <code>float</code> <p>Fraction of features to subsample. -1 means no subsampling. Default is -1.</p> <code>global_transformer_name</code> <code>str</code> <p>Name of the global transformer to use. Default is None.</p>"},{"location":"api/tabpfn-client/PreprocessorConfig/#tabpfn_client.estimator.PreprocessorConfig.can_be_cached","title":"can_be_cached","text":"<pre><code>can_be_cached()\n</code></pre>"},{"location":"api/tabpfn-client/PreprocessorConfig/#tabpfn_client.estimator.PreprocessorConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre>"},{"location":"api/tabpfn-client/PromptAgent/","title":"PromptAgent","text":""},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent","title":"PromptAgent","text":""},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent.indent","title":"indent  <code>staticmethod</code>","text":"<pre><code>indent(text: str)\n</code></pre>"},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent.password_req_to_policy","title":"password_req_to_policy  <code>staticmethod</code>","text":"<pre><code>password_req_to_policy(password_req: list[str])\n</code></pre> <p>Small function that receives password requirements as a list of strings like \"Length(8)\" and returns a corresponding PasswordPolicy object.</p>"},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent.prompt_welcome","title":"prompt_welcome  <code>classmethod</code>","text":"<pre><code>prompt_welcome()\n</code></pre>"},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent.prompt_and_set_token","title":"prompt_and_set_token  <code>classmethod</code>","text":"<pre><code>prompt_and_set_token(\n    user_auth_handler: UserAuthenticationClient,\n)\n</code></pre>"},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent.prompt_terms_and_cond","title":"prompt_terms_and_cond  <code>classmethod</code>","text":"<pre><code>prompt_terms_and_cond() -&gt; bool\n</code></pre>"},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent.prompt_add_user_information","title":"prompt_add_user_information  <code>classmethod</code>","text":"<pre><code>prompt_add_user_information() -&gt; dict\n</code></pre>"},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent.prompt_reusing_existing_token","title":"prompt_reusing_existing_token  <code>classmethod</code>","text":"<pre><code>prompt_reusing_existing_token()\n</code></pre>"},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent.reverify_email","title":"reverify_email  <code>classmethod</code>","text":"<pre><code>reverify_email(\n    access_token,\n    user_auth_handler: UserAuthenticationClient,\n)\n</code></pre>"},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent.prompt_retrieved_greeting_messages","title":"prompt_retrieved_greeting_messages  <code>classmethod</code>","text":"<pre><code>prompt_retrieved_greeting_messages(\n    greeting_messages: list[str],\n)\n</code></pre>"},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent.prompt_confirm_password_for_user_account_deletion","title":"prompt_confirm_password_for_user_account_deletion  <code>classmethod</code>","text":"<pre><code>prompt_confirm_password_for_user_account_deletion() -&gt; str\n</code></pre>"},{"location":"api/tabpfn-client/PromptAgent/#tabpfn_client.prompt_agent.PromptAgent.prompt_account_deleted","title":"prompt_account_deleted  <code>classmethod</code>","text":"<pre><code>prompt_account_deleted()\n</code></pre>"},{"location":"api/tabpfn-client/SensitiveDataFilter/","title":"SensitiveDataFilter","text":""},{"location":"api/tabpfn-client/SensitiveDataFilter/#tabpfn_client.client.SensitiveDataFilter","title":"SensitiveDataFilter","text":"<p>             Bases: <code>Filter</code></p>"},{"location":"api/tabpfn-client/SensitiveDataFilter/#tabpfn_client.client.SensitiveDataFilter.filter","title":"filter","text":"<pre><code>filter(record)\n</code></pre>"},{"location":"api/tabpfn-client/ServiceClient/","title":"ServiceClient","text":""},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient","title":"ServiceClient","text":"<p>Singleton class for handling communication with the server. It encapsulates all the API calls to the server.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.access_token","title":"access_token  <code>property</code>","text":"<pre><code>access_token\n</code></pre>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.is_initialized","title":"is_initialized  <code>property</code>","text":"<pre><code>is_initialized\n</code></pre>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.authorize","title":"authorize","text":"<pre><code>authorize(access_token: str)\n</code></pre>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.reset_authorization","title":"reset_authorization","text":"<pre><code>reset_authorization()\n</code></pre>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.upload_train_set","title":"upload_train_set","text":"<pre><code>upload_train_set(X, y) -&gt; str\n</code></pre> <p>Upload a train set to server and return the train set UID if successful.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.upload_train_set--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     The training input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs)     The target values.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.upload_train_set--returns","title":"Returns","text":"<p>train_set_uid : str     The unique ID of the train set in the server.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.predict","title":"predict","text":"<pre><code>predict(\n    train_set_uid: str,\n    x_test,\n    task: Literal[\"classification\", \"regression\"],\n    tabpfn_config: dict | None = None,\n    X_train=None,\n    y_train=None,\n) -&gt; dict[str, ndarray]\n</code></pre> <p>Predict the class labels for the provided data (test set).</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.predict--parameters","title":"Parameters","text":"<p>train_set_uid : str     The unique ID of the train set in the server. x_test : array-like of shape (n_samples, n_features)     The test input.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.predict--returns","title":"Returns","text":"<p>y_pred : array-like of shape (n_samples,)     The predicted class labels.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.try_connection","title":"try_connection","text":"<pre><code>try_connection() -&gt; bool\n</code></pre> <p>Check if server is reachable and accepts the connection.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.is_auth_token_outdated","title":"is_auth_token_outdated","text":"<pre><code>is_auth_token_outdated(access_token) -&gt; bool | None\n</code></pre> <p>Check if the provided access token is valid and return True if successful.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.validate_email","title":"validate_email","text":"<pre><code>validate_email(email: str) -&gt; tuple[bool, str]\n</code></pre> <p>Send entered email to server that checks if it is valid and not already in use.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.validate_email--parameters","title":"Parameters","text":"<p>email : str</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.validate_email--returns","title":"Returns","text":"<p>is_valid : bool     True if the email is valid. message : str     The message returned from the server.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.register","title":"register","text":"<pre><code>register(\n    email: str,\n    password: str,\n    password_confirm: str,\n    validation_link: str,\n    additional_info: dict,\n)\n</code></pre> <p>Register a new user with the provided credentials.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.register--parameters","title":"Parameters","text":"<p>email : str password : str password_confirm : str validation_link: str additional_info : dict</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.register--returns","title":"Returns","text":"<p>is_created : bool     True if the user is created successfully. message : str     The message returned from the server.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.login","title":"login","text":"<pre><code>login(email: str, password: str) -&gt; tuple[str, str]\n</code></pre> <p>Login with the provided credentials and return the access token if successful.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.login--parameters","title":"Parameters","text":"<p>email : str password : str</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.login--returns","title":"Returns","text":"<p>access_token : str | None     The access token returned from the server. Return None if login fails. message : str     The message returned from the server.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.get_password_policy","title":"get_password_policy","text":"<pre><code>get_password_policy() -&gt; {}\n</code></pre> <p>Get the password policy from the server.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.get_password_policy--returns","title":"Returns","text":"<p>password_policy : {}     The password policy returned from the server.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.send_reset_password_email","title":"send_reset_password_email","text":"<pre><code>send_reset_password_email(email: str) -&gt; tuple[bool, str]\n</code></pre> <p>Let the server send an email for resetting the password.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.send_verification_email","title":"send_verification_email","text":"<pre><code>send_verification_email(\n    access_token: str,\n) -&gt; tuple[bool, str]\n</code></pre> <p>Let the server send an email for verifying the email.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.retrieve_greeting_messages","title":"retrieve_greeting_messages","text":"<pre><code>retrieve_greeting_messages() -&gt; list[str]\n</code></pre> <p>Retrieve greeting messages that are new for the user.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.get_data_summary","title":"get_data_summary","text":"<pre><code>get_data_summary() -&gt; {}\n</code></pre> <p>Get the data summary of the user from the server.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.get_data_summary--returns","title":"Returns","text":"<p>data_summary : {}     The data summary returned from the server.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.download_all_data","title":"download_all_data","text":"<pre><code>download_all_data(save_dir: Path) -&gt; Path | None\n</code></pre> <p>Download all data uploaded by the user from the server.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.download_all_data--returns","title":"Returns","text":"<p>save_path : Path | None     The path to the downloaded file. Return None if download fails.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.delete_dataset","title":"delete_dataset","text":"<pre><code>delete_dataset(dataset_uid: str) -&gt; [str]\n</code></pre> <p>Delete the dataset with the provided UID from the server. Note that deleting a train set with lead to deleting all associated test sets.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.delete_dataset--parameters","title":"Parameters","text":"<p>dataset_uid : str     The UID of the dataset to be deleted.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.delete_dataset--returns","title":"Returns","text":"<p>deleted_dataset_uids : [str]     The list of deleted dataset UIDs.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.delete_all_datasets","title":"delete_all_datasets","text":"<pre><code>delete_all_datasets() -&gt; [str]\n</code></pre> <p>Delete all datasets uploaded by the user from the server.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.delete_all_datasets--returns","title":"Returns","text":"<p>deleted_dataset_uids : [str]     The list of deleted dataset UIDs.</p>"},{"location":"api/tabpfn-client/ServiceClient/#tabpfn_client.client.ServiceClient.delete_user_account","title":"delete_user_account","text":"<pre><code>delete_user_account(confirm_pass: str) -&gt; None\n</code></pre>"},{"location":"api/tabpfn-client/ServiceClientWrapper/","title":"ServiceClientWrapper","text":""},{"location":"api/tabpfn-client/ServiceClientWrapper/#tabpfn_client.service_wrapper.ServiceClientWrapper","title":"ServiceClientWrapper","text":""},{"location":"api/tabpfn-client/ServiceClientWrapper/#tabpfn_client.service_wrapper.ServiceClientWrapper.__init__","title":"__init__","text":"<pre><code>__init__(service_client: ServiceClient)\n</code></pre>"},{"location":"api/tabpfn-client/TabPFNClassifier/","title":"TabPFNClassifier","text":""},{"location":"api/tabpfn-client/TabPFNClassifier/#tabpfn_client.estimator.TabPFNClassifier","title":"TabPFNClassifier","text":"<p>             Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code>, <code>TabPFNModelSelection</code></p>"},{"location":"api/tabpfn-client/TabPFNClassifier/#tabpfn_client.estimator.TabPFNClassifier.__init__","title":"__init__","text":"<pre><code>__init__(\n    model=\"default\",\n    n_estimators: int = 4,\n    preprocess_transforms: Tuple[\n        PreprocessorConfig, ...\n    ] = (\n        PreprocessorConfig(\n            \"quantile_uni_coarse\",\n            append_original=True,\n            categorical_name=\"ordinal_very_common_categories_shuffled\",\n            global_transformer_name=\"svd\",\n            subsample_features=-1,\n        ),\n        PreprocessorConfig(\n            \"none\",\n            categorical_name=\"numeric\",\n            subsample_features=-1,\n        ),\n    ),\n    feature_shift_decoder: str = \"shuffle\",\n    normalize_with_test: bool = False,\n    average_logits: bool = False,\n    optimize_metric: Literal[\n        \"auroc\",\n        \"roc\",\n        \"auroc_ovo\",\n        \"balanced_acc\",\n        \"acc\",\n        \"log_loss\",\n        None,\n    ] = \"roc\",\n    transformer_predict_kwargs: Optional[dict] = None,\n    multiclass_decoder=\"shuffle\",\n    softmax_temperature: Optional[float] = -0.1,\n    use_poly_features=False,\n    max_poly_features=50,\n    remove_outliers=12.0,\n    add_fingerprint_features=True,\n    subsample_samples=-1,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model string is the path to the model.</p> <code>'default'</code> <code>n_estimators</code> <code>int</code> <p>The number of ensemble configurations to use, the most important setting.</p> <code>4</code> <code>preprocess_transforms</code> <code>Tuple[PreprocessorConfig, ...]</code> <p>A tuple of strings, specifying the preprocessing steps to use. You can use the following strings as elements '(none|power|quantile|robust)_all', where the first part specifies the preprocessing step and the second part specifies the features to apply it to and finally '_and_none' specifies that the original features should be added back to the features in plain. Finally, you can combine all strings without <code>_all</code> with <code>_onehot</code> to apply one-hot encoding to the categorical features specified with <code>self.fit(..., categorical_features=...)</code>.</p> <code>(PreprocessorConfig('quantile_uni_coarse', append_original=True, categorical_name='ordinal_very_common_categories_shuffled', global_transformer_name='svd', subsample_features=-1), PreprocessorConfig('none', categorical_name='numeric', subsample_features=-1))</code> <code>feature_shift_decoder</code> <code>str</code> <p>[\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.</p> <code>'shuffle'</code> <code>normalize_with_test</code> <code>bool</code> <p>If True, the test set is used to normalize the data, otherwise the training set is used only.</p> <code>False</code> <code>average_logits</code> <code>bool</code> <p>Whether to average logits or probabilities for ensemble members.</p> <code>False</code> <code>optimize_metric</code> <code>Literal['auroc', 'roc', 'auroc_ovo', 'balanced_acc', 'acc', 'log_loss', None]</code> <p>The optimization metric to use.</p> <code>'roc'</code> <code>transformer_predict_kwargs</code> <code>Optional[dict]</code> <p>Additional keyword arguments to pass to the transformer predict method.</p> <code>None</code> <code>multiclass_decoder</code> <p>The multiclass decoder to use.</p> <code>'shuffle'</code> <code>softmax_temperature</code> <code>Optional[float]</code> <p>A log spaced temperature, it will be applied as logits &lt;- logits/exp(softmax_temperature).</p> <code>-0.1</code> <code>use_poly_features</code> <p>Whether to use polynomial features as the last preprocessing step.</p> <code>False</code> <code>max_poly_features</code> <p>Maximum number of polynomial features to use.</p> <code>50</code> <code>remove_outliers</code> <p>If not 0.0, will remove outliers from the input features, where values with a standard deviation larger than remove_outliers will be removed.</p> <code>12.0</code> <code>add_fingerprint_features</code> <p>If True, will add one feature of random values, that will be added to the input features. This helps discern duplicated samples in the transformer model.</p> <code>True</code> <code>subsample_samples</code> <p>If not None, will use a random subset of the samples for training in each ensemble configuration. If 1 or above, this will subsample to the specified number of samples. If in 0 to 1, the value is viewed as a fraction of the training set size.</p> <code>-1</code>"},{"location":"api/tabpfn-client/TabPFNClassifier/#tabpfn_client.estimator.TabPFNClassifier.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre>"},{"location":"api/tabpfn-client/TabPFNClassifier/#tabpfn_client.estimator.TabPFNClassifier.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre>"},{"location":"api/tabpfn-client/TabPFNClassifier/#tabpfn_client.estimator.TabPFNClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X)\n</code></pre>"},{"location":"api/tabpfn-client/TabPFNConfig/","title":"TabPFNConfig","text":""},{"location":"api/tabpfn-client/TabPFNConfig/#tabpfn_client.config.TabPFNConfig","title":"TabPFNConfig","text":""},{"location":"api/tabpfn-client/TabPFNModelSelection/","title":"TabPFNModelSelection","text":""},{"location":"api/tabpfn-client/TabPFNModelSelection/#tabpfn_client.estimator.TabPFNModelSelection","title":"TabPFNModelSelection","text":"<p>Base class for TabPFN model selection and path handling.</p>"},{"location":"api/tabpfn-client/TabPFNModelSelection/#tabpfn_client.estimator.TabPFNModelSelection.list_available_models","title":"list_available_models  <code>classmethod</code>","text":"<pre><code>list_available_models() -&gt; list[str]\n</code></pre>"},{"location":"api/tabpfn-client/TabPFNRegressor/","title":"TabPFNRegressor","text":""},{"location":"api/tabpfn-client/TabPFNRegressor/#tabpfn_client.estimator.TabPFNRegressor","title":"TabPFNRegressor","text":"<p>             Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code>, <code>TabPFNModelSelection</code></p>"},{"location":"api/tabpfn-client/TabPFNRegressor/#tabpfn_client.estimator.TabPFNRegressor.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: str = \"default\",\n    n_estimators: int = 8,\n    preprocess_transforms: Tuple[\n        PreprocessorConfig, ...\n    ] = (\n        PreprocessorConfig(\n            \"quantile_uni\",\n            append_original=True,\n            categorical_name=\"ordinal_very_common_categories_shuffled\",\n            global_transformer_name=\"svd\",\n        ),\n        PreprocessorConfig(\n            \"safepower\", categorical_name=\"onehot\"\n        ),\n    ),\n    feature_shift_decoder: str = \"shuffle\",\n    normalize_with_test: bool = False,\n    average_logits: bool = False,\n    optimize_metric: Literal[\n        \"mse\",\n        \"rmse\",\n        \"mae\",\n        \"r2\",\n        \"mean\",\n        \"median\",\n        \"mode\",\n        \"exact_match\",\n        None,\n    ] = \"rmse\",\n    transformer_predict_kwargs: Optional[Dict] = None,\n    softmax_temperature: Optional[float] = -0.1,\n    use_poly_features=False,\n    max_poly_features=50,\n    remove_outliers=-1,\n    regression_y_preprocess_transforms: Optional[\n        Tuple[\n            None\n            | Literal[\n                \"safepower\", \"power\", \"quantile_norm\"\n            ],\n            ...,\n        ]\n    ] = (None, \"safepower\"),\n    add_fingerprint_features: bool = True,\n    cancel_nan_borders: bool = True,\n    super_bar_dist_averaging: bool = False,\n    subsample_samples: float = -1,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model string is the path to the model.</p> <code>'default'</code> <code>n_estimators</code> <code>int</code> <p>The number of ensemble configurations to use, the most important setting.</p> <code>8</code> <code>preprocess_transforms</code> <code>Tuple[PreprocessorConfig, ...]</code> <p>A tuple of strings, specifying the preprocessing steps to use. You can use the following strings as elements '(none|power|quantile_norm|quantile_uni|quantile_uni_coarse|robust...)_all', where the first part specifies the preprocessing step (see <code>.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()</code>) and the second part specifies the features to apply it to and finally '_and_none' specifies that the original features should be added back to the features in plain. Finally, you can combine all strings without <code>_all</code> with <code>_onehot</code> to apply one-hot encoding to the categorical features specified with <code>self.fit(..., categorical_features=...)</code>.</p> <code>(PreprocessorConfig('quantile_uni', append_original=True, categorical_name='ordinal_very_common_categories_shuffled', global_transformer_name='svd'), PreprocessorConfig('safepower', categorical_name='onehot'))</code> <code>feature_shift_decoder</code> <code>str</code> <p>[\"shuffle\", \"none\", \"local_shuffle\", \"rotate\", \"auto_rotate\"] Whether to shift features for each ensemble configuration.</p> <code>'shuffle'</code> <code>normalize_with_test</code> <code>bool</code> <p>If True, the test set is used to normalize the data, otherwise the training set is used only.</p> <code>False</code> <code>average_logits</code> <code>bool</code> <p>Whether to average logits or probabilities for ensemble members.</p> <code>False</code> <code>optimize_metric</code> <code>Literal['mse', 'rmse', 'mae', 'r2', 'mean', 'median', 'mode', 'exact_match', None]</code> <p>The optimization metric to use.</p> <code>'rmse'</code> <code>transformer_predict_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments to pass to the transformer predict method.</p> <code>None</code> <code>softmax_temperature</code> <code>Optional[float]</code> <p>A log spaced temperature, it will be applied as logits &lt;- logits/exp(softmax_temperature).</p> <code>-0.1</code> <code>use_poly_features</code> <p>Whether to use polynomial features as the last preprocessing step.</p> <code>False</code> <code>max_poly_features</code> <p>Maximum number of polynomial features to use, None means unlimited.</p> <code>50</code> <code>remove_outliers</code> <p>If not 0.0, will remove outliers from the input features, where values with a standard deviation larger than remove_outliers will be removed.</p> <code>-1</code> <code>regression_y_preprocess_transforms</code> <code>Optional[Tuple[None | Literal['safepower', 'power', 'quantile_norm'], ...]]</code> <p>Preprocessing transforms for the target variable. This can be one from <code>.preprocessing.ReshapeFeatureDistributionsStep.get_all_preprocessors()</code>, e.g. \"power\". This can also be None to not transform the targets, beside a simple mean/variance normalization.</p> <code>(None, 'safepower')</code> <code>add_fingerprint_features</code> <code>bool</code> <p>If True, will add one feature of random values, that will be added to the input features. This helps discern duplicated samples in the transformer model.</p> <code>True</code> <code>cancel_nan_borders</code> <code>bool</code> <p>Whether to ignore buckets that are tranformed to nan values by inverting a <code>regression_y_preprocess_transform</code>. This should be set to True, only set this to False if you know what you are doing.</p> <code>True</code> <code>super_bar_dist_averaging</code> <code>bool</code> <p>If we use <code>regression_y_preprocess_transforms</code> we need to average the predictions over the different configurations. The different configurations all come with different bar_distributions (Riemann distributions), though. The default is for us to aggregate all bar distributions using simply scaled borders in the bar distribution, scaled by the mean and std of the target variable. If you set this to True, a new bar distribution will be built using all the borders generated in the different configurations.</p> <code>False</code> <code>subsample_samples</code> <code>float</code> <p>If not None, will use a random subset of the samples for training in each ensemble configuration. If 1 or above, this will subsample to the specified number of samples. If in 0 to 1, the value is viewed as a fraction of the training set size.</p> <code>-1</code>"},{"location":"api/tabpfn-client/TabPFNRegressor/#tabpfn_client.estimator.TabPFNRegressor.fit","title":"fit","text":"<pre><code>fit(X, y)\n</code></pre>"},{"location":"api/tabpfn-client/TabPFNRegressor/#tabpfn_client.estimator.TabPFNRegressor.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre>"},{"location":"api/tabpfn-client/TabPFNRegressor/#tabpfn_client.estimator.TabPFNRegressor.predict_full","title":"predict_full","text":"<pre><code>predict_full(X)\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/","title":"UserAuthenticationClient","text":""},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient","title":"UserAuthenticationClient","text":"<p>             Bases: <code>ServiceClientWrapper</code></p> <p>Wrapper of ServiceClient to handle user authentication, including: - user registration and login - access token caching</p>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.__init__","title":"__init__","text":"<pre><code>__init__(service_client: ServiceClient)\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.is_accessible_connection","title":"is_accessible_connection","text":"<pre><code>is_accessible_connection() -&gt; bool\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.set_token","title":"set_token","text":"<pre><code>set_token(access_token: str)\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.validate_email","title":"validate_email","text":"<pre><code>validate_email(email: str) -&gt; tuple[bool, str]\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.set_token_by_registration","title":"set_token_by_registration","text":"<pre><code>set_token_by_registration(\n    email: str,\n    password: str,\n    password_confirm: str,\n    validation_link: str,\n    additional_info: dict,\n) -&gt; tuple[bool, str]\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.set_token_by_login","title":"set_token_by_login","text":"<pre><code>set_token_by_login(\n    email: str, password: str\n) -&gt; tuple[bool, str]\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.try_reuse_existing_token","title":"try_reuse_existing_token","text":"<pre><code>try_reuse_existing_token() -&gt; bool | tuple[bool, str]\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.get_password_policy","title":"get_password_policy","text":"<pre><code>get_password_policy()\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.reset_cache","title":"reset_cache","text":"<pre><code>reset_cache()\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.retrieve_greeting_messages","title":"retrieve_greeting_messages","text":"<pre><code>retrieve_greeting_messages()\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.send_reset_password_email","title":"send_reset_password_email","text":"<pre><code>send_reset_password_email(email: str) -&gt; tuple[bool, str]\n</code></pre>"},{"location":"api/tabpfn-client/UserAuthenticationClient/#tabpfn_client.service_wrapper.UserAuthenticationClient.send_verification_email","title":"send_verification_email","text":"<pre><code>send_verification_email(\n    access_token: str,\n) -&gt; tuple[bool, str]\n</code></pre>"},{"location":"api/tabpfn-client/UserDataClient/","title":"UserDataClient","text":""},{"location":"api/tabpfn-client/UserDataClient/#tabpfn_client.service_wrapper.UserDataClient","title":"UserDataClient","text":"<p>             Bases: <code>ServiceClientWrapper</code></p> <p>Wrapper of ServiceClient to handle user data, including: - query, or delete user account data - query, download, or delete uploaded data</p>"},{"location":"api/tabpfn-client/UserDataClient/#tabpfn_client.service_wrapper.UserDataClient.__init__","title":"__init__","text":"<pre><code>__init__(service_client=ServiceClient())\n</code></pre>"},{"location":"api/tabpfn-client/UserDataClient/#tabpfn_client.service_wrapper.UserDataClient.get_data_summary","title":"get_data_summary","text":"<pre><code>get_data_summary() -&gt; {}\n</code></pre>"},{"location":"api/tabpfn-client/UserDataClient/#tabpfn_client.service_wrapper.UserDataClient.download_all_data","title":"download_all_data","text":"<pre><code>download_all_data(save_dir: Path = Path('.')) -&gt; Path\n</code></pre>"},{"location":"api/tabpfn-client/UserDataClient/#tabpfn_client.service_wrapper.UserDataClient.delete_dataset","title":"delete_dataset","text":"<pre><code>delete_dataset(dataset_uid: str) -&gt; [str]\n</code></pre>"},{"location":"api/tabpfn-client/UserDataClient/#tabpfn_client.service_wrapper.UserDataClient.delete_all_datasets","title":"delete_all_datasets","text":"<pre><code>delete_all_datasets() -&gt; [str]\n</code></pre>"},{"location":"api/tabpfn-client/UserDataClient/#tabpfn_client.service_wrapper.UserDataClient.delete_user_account","title":"delete_user_account","text":"<pre><code>delete_user_account()\n</code></pre>"},{"location":"api/tabpfn-client/tabpfn_common_utils/RegressionPredictResult/","title":"RegressionPredictResult","text":""},{"location":"api/tabpfn-client/tabpfn_common_utils/RegressionPredictResult/#tabpfn_client.tabpfn_common_utils.regression_pred_result.RegressionPredictResult","title":"RegressionPredictResult","text":""},{"location":"api/tabpfn-client/tabpfn_common_utils/RegressionPredictResult/#tabpfn_client.tabpfn_common_utils.regression_pred_result.RegressionPredictResult.val_type","title":"val_type  <code>property</code>","text":"<pre><code>val_type\n</code></pre>"},{"location":"api/tabpfn-client/tabpfn_common_utils/RegressionPredictResult/#tabpfn_client.tabpfn_common_utils.regression_pred_result.RegressionPredictResult.__init__","title":"__init__","text":"<pre><code>__init__(res: {})\n</code></pre>"},{"location":"api/tabpfn-client/tabpfn_common_utils/RegressionPredictResult/#tabpfn_client.tabpfn_common_utils.regression_pred_result.RegressionPredictResult.to_basic_representation","title":"to_basic_representation  <code>staticmethod</code>","text":"<pre><code>to_basic_representation(\n    res: RegressionPredictResult,\n) -&gt; dict[str, list]\n</code></pre>"},{"location":"api/tabpfn-client/tabpfn_common_utils/RegressionPredictResult/#tabpfn_client.tabpfn_common_utils.regression_pred_result.RegressionPredictResult.from_basic_representation","title":"from_basic_representation  <code>staticmethod</code>","text":"<pre><code>from_basic_representation(\n    basic_repr: dict[str, list]\n) -&gt; dict[str, ndarray]\n</code></pre>"},{"location":"api/tabpfn-client/tabpfn_common_utils/RequestType/","title":"RequestType","text":""},{"location":"api/tabpfn-client/tabpfn_common_utils/RequestType/#tabpfn_client.tabpfn_common_utils.load_test.RequestType","title":"RequestType","text":"<p>             Bases: <code>Enum</code></p>"},{"location":"getting_started/getting_started/","title":"Installation","text":"<p>To install TabPFN, please use the notebooks provided in this review. After review we are going to release our models as a python package on the python repository pypi.</p>"},{"location":"getting_started/getting_started/#example","title":"Example","text":"<p>A simple way to get started with TabPFN using our sklearn interface is demonstrated below. This example shows how to train a classifier on the breast cancer dataset and evaluate its accuracy.</p> <p><pre><code>from sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nfrom tabpfn import TabPFNClassifier\n\n# Load data\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Initialize classifier\nclassifier = TabPFNClassifier(device='cpu', N_ensemble_configurations=32)\n\n# Train classifier\nclassifier.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = classifier.predict(X_test)\nprint('Accuracy:', accuracy_score(y_test, y_pred))\n</code></pre> This example demonstrates the basic workflow of training and predicting with TabPFN models. For more advanced usage, including handling of categorical data, please refer to the Advanced Usage section.</p>"},{"location":"getting_started/install/","title":"Installation","text":"<p>You can access our models through our API (https://github.com/automl/tabpfn-client) or via our user interface built on top of the API (https://www.priorlabs.ai/tabpfn-gui). We will release open weights models soon, currently we are available via api and via our user interface built on top of the API.</p> Python API Client (No GPU, Online)Python Local (GPU) <pre><code>pip install tabpfn-client\n</code></pre> <p>To log into your account, create an account through our website or through the programmatic interface. Then log into that account by setting the token with</p> <pre><code>from tabpfn_client import config\n\n# Retrieve Token\nwith open(config.g_tabpfn_config.user_auth_handler.CACHED_TOKEN_FILE, 'r') as file:\n    token = file.read()\nprint(f\"TOKEN: {token}\")\nfrom tabpfn_client import config\n\n# Set Token\nservice_client = config.ServiceClient()\nconfig.g_tabpfn_config.user_auth_handler = config.UserAuthenticationClient(service_client=service_client)\nuser_auth = config.g_tabpfn_config.user_auth_handler.set_token(token)\n</code></pre> <p>Warning</p> <p>Not released yet</p>"},{"location":"getting_started/intended_use/","title":"Usage tips","text":""},{"location":"getting_started/intended_use/#when-to-use-tabpfn","title":"When to use TabPFN","text":"<p>TabPFN excels in handling small to medium-sized datasets with up to 10,000 samples and 500 features. For larger datasets, approaches such as CatBoost, XGB, or AutoGluon are likely to outperform TabPFN.</p>"},{"location":"getting_started/intended_use/#intended-use-of-tabpfn","title":"Intended Use of TabPFN","text":"<p>While TabPFN provides a powerful drop-in replacement for traditional tabular data models, achieving top performance on real-world problems often requires domain expertise and the ingenuity of data scientists. Data scientists should continue to apply their skills in feature engineering, data cleaning, and problem framing to get the most out of TabPFN.</p>"},{"location":"getting_started/intended_use/#limitations-of-tabpfn","title":"Limitations of TabPFN","text":"<ol> <li>TabPFN's inference speed may be slower than highly optimized approaches like CatBoost.</li> <li>TabPFN's memory usage scales linearly with dataset size, which can be prohibitive for very large datasets.</li> <li>Our evaluation focused on datasets with up to 10,000 samples and 500 features; scalability to larger datasets requires further study.</li> </ol>"},{"location":"getting_started/intended_use/#computational-and-time-requirements","title":"Computational and Time Requirements","text":"<p>TabPFN is computationally efficient and can run on consumer hardware for most datasets. Training on a new dataset is recommended to run on a GPU as this speeds it up significantly. However, TabPFN is not optimized for real-time inference tasks.</p>"},{"location":"getting_started/intended_use/#data-preparation","title":"Data Preparation","text":"<p>TabPFN can handle raw data with minimal preprocessing. Provide the data in a tabular format, and TabPFN will automatically handle missing values, encode categorical variables, and normalize features. While TabPFN works well out-of-the-box, performance can further be improved using dataset-specific preprocessings.</p>"},{"location":"getting_started/intended_use/#interpreting-results","title":"Interpreting Results","text":"<p>TabPFN's predictions come with uncertainty estimates, allowing you to assess the reliability of the results. You can use SHAP to interpret TabPFN's predictions and identify the most important features driving the model's decisions.</p>"},{"location":"getting_started/intended_use/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>TabPFN provides strong performance out-of-the-box without extensive hyperparameter tuning. If you have additional computational resources, you can further optimize TabPFN's performance using random hyperparameter tuning or the Post-Hoc Ensembling (PHE) technique.</p>"},{"location":"integrations/R/","title":"R","text":""},{"location":"integrations/R/#tabpfn-followups","title":"TabPFN Followups","text":"<p>Forecastpfn: Synthetically-trained zero-shot forecasting    Dooley, Khurana, Mohapatra, Naidu, White Advances in Neural Information Processing Systems, 2024, Volume 36.</p> <p>Interpretable machine learning for TabPFN    Rundel, Kobialka, von Crailsheim, Feurer, Nagler, R{\"u}gamer World Conference on Explainable Artificial Intelligence, 2024, Pages 465--476.</p> <p>Scaling tabpfn: Sketching and feature selection for tabular prior-data fitted networks    Feuer, Hegde, Cohen arXiv preprint arXiv:2311.10609, 2023.</p> <p>In-Context Data Distillation with TabPFN    Ma, Thomas, Yu, Caterini arXiv preprint arXiv:2402.06971, 2024.</p> <p>Tokenize features, enhancing tables: the FT-TABPFN model for tabular classification    Liu, Yang, Liang, Pang, Zou arXiv preprint arXiv:2406.06891, 2024.</p> <p>Towards Localization via Data Embedding for TabPFN    Koshil, Nagler, Feurer, Eggensperger NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Enhancing Classification Performance Through the Synergistic Use of XGBoost, TABPFN, and LGBM Models    Prabowo, others 2023 15<sup>th</sup> International Congress on Advanced Applied Informatics Winter (IIAI-AAI-Winter), 2023, Pages 255--259.</p> <p>The Tabular Foundation Model TabPFN Outperforms Specialized Time Series Forecasting Models Based on Simple Features    Hoo, M{\"u}ller, Salinas, Hutter NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabPFGen--Tabular Data Generation with TabPFN    Ma, Dankar, Stein, Yu, Caterini arXiv preprint arXiv:2406.05216, 2024.</p> <p>Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data    Helli, Schnurr, Hollmann, M{\"u}ller, Hutter arXiv preprint arXiv:2411.10634, 2024.</p> <p>TabFlex: Scaling Tabular Learning to Millions with Linear Attention    Zeng, Kang, Mueller NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Retrieval \\&amp; Fine-Tuning for In-Context Tabular Models    Thomas, Ma, Hosseinzadeh, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2406.05207, 2024.</p> <p>TabDPT: Scaling Tabular Foundation Models    Ma, Thomas, Hosseinzadeh, Kamkari, Labach, Cresswell, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2410.18164, 2024.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    Breejen, Bae, Cha, Yun arXiv preprint arXiv:2405.13396, 2024.</p> <p>MotherNet: Fast Training and Inference via Hyper-Network Transformers    Mueller, Curino, Ramakrishnan NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Mixture of In-Context Prompters for Tabular PFNs    Xu, Cirit, Asadi, Sun, Wang arXiv preprint arXiv:2405.16156, 2024.</p> <p>Fast and Accurate Zero-Training Classification for Tabular Engineering Data    Picard, Ahmed arXiv preprint arXiv:2401.06948, 2024.</p> <p>Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning    den Breejen, Bae, Cha, Kim, Koh, Yun NeurIPS 2023 Second Table Representation Learning Workshop, 2023.</p> <p>TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks    Feuer, Schirrmeister, Cherepanova, Hegde, Hutter, Goldblum, Cohen, White arXiv preprint arXiv:2402.11137, 2024.</p> <p>Exploration of autoregressive models for in-context learning on tabular data    Baur, Kim NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabMDA: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting    Margeloiu, Bazaga, Simidjievski, Li{`o}, Jamnik arXiv preprint arXiv:2406.01805, 2024.</p> <p>Large Scale Transfer Learning for Tabular Data via Language Modeling    Gardner, Perdomo, Schmidt arXiv preprint arXiv:2406.12031, 2024.</p> <p>AnnotatedTables: A Large Tabular Dataset with Language Model Annotations    Hu, Fountalis, Tian, Vasiloglou arXiv preprint arXiv:2406.16349, 2024.</p> <p>TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling    Gorishniy, Kotelnikov, Babenko arXiv preprint arXiv:2410.24210, 2024.</p> <p>Pre-Trained Tabular Transformer for Real-Time, Efficient, Stable Radiomics Data Processing: A Comprehensive Study    Jiang, Jia, Zhang, Li 2023 IEEE International Conference on E-health Networking, Application \\&amp; Services (Healthcom), 2023, Pages 276--281.</p> <p>TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik arXiv preprint arXiv:2409.16118, 2024.</p> <p>Augmenting Small-size Tabular Data with Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>FORECASTPFN: ZERO-SHOT LOW-RESOURCE FORECASTING    Khurana, Dooley, Naidu, White, AI No Source, No Year.</p> <p>What exactly has TabPFN learned to do?    McCarter The Third Blogpost Track at ICLR 2024, No Year.</p> <p>Statistical foundations of prior-data fitted networks    Nagler International Conference on Machine Learning, 2023, Pages 25660--25676.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    den Breejen, Bae, Cha, Yun arXiv e-prints, 2024, Pages arXiv--2405.</p>"},{"location":"integrations/R/#tabpfn-application","title":"TabPFN Application","text":"<p>Large-scale chemoproteomics expedites ligand discovery and predicts ligand behavior in cells    Offensperger, Tin, Duran-Frigola, Hahn, Dobner, Ende, Strohbach, Rukavina, Brennsteiner, Ogilvie, others Science, 2024, Volume 384, Issue 6694, Pages eadk5864.</p> <p>Deep learning for cross-selling health insurance classification    Chu, Than, Jo 2024 International Conference on Green Energy, Computing and Sustainable Technology (GECOST), 2024, Pages 453--457.</p> <p>Early fault classification in rotating machinery with limited data using TabPFN    Magad{'a}n, Rold{'a}n-G{'o}mez, Granda, Su{'a}rez IEEE Sensors Journal, 2023.</p> <p>Artificial intelligence-driven predictive framework for early detection of still birth    Alzakari, Aldrees, Umer, Cascone, Innab, Ashraf SLAS technology, 2024, Volume 29, Issue 6, Pages 100203.</p> <p>Prostate Cancer Diagnosis via Visual Representation of Tabular Data and Deep Transfer Learning    El-Melegy, Mamdouh, Ali, Badawy, El-Ghar, Alghamdi, El-Baz Bioengineering, 2024, Volume 11, Issue 7, Pages 635.</p> <p>A machine learning-based approach for individualized prediction of short-term outcomes after anterior cervical corpectomy    Karabacak, Schupper, Carr, Margetis Asian Spine Journal, 2024, Volume 18, Issue 4, Pages 541.</p> <p>Comparing the Performance of a Deep Learning Model (TabPFN) for Predicting River Algal Blooms with Varying Data Composition    Yang, Park Journal of Wetlands Research, 2024, Volume 26, Issue 3, Pages 197--203.</p> <p>Adapting TabPFN for Zero-Inflated Metagenomic Data    Perciballi, Granese, Fall, Zehraoui, Prifti, Zucker NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Comprehensive peripheral blood immunoprofiling reveals five immunotypes with immunotherapy response characteristics in patients with cancer    Dyikanov, Zaitsev, Vasileva, Wang, Sokolov, Bolshakov, Frank, Turova, Golubeva, Gantseva, others Cancer Cell, 2024, Volume 42, Issue 5, Pages 759--779.</p> <p>Predicting dementia in Parkinson's disease on a small tabular dataset using hybrid LightGBM--TabPFN and SHAP    Tran, Byeon Digital Health, 2024, Volume 10, Pages 20552076241272585.</p> <p>Enhancing actuarial non-life pricing models via transformers    Brauer European Actuarial Journal, 2024, Pages 1--22.</p> <p>Machine learning-based diagnostic prediction of minimal change disease: model development study    Noda, Ichikawa, Shibagaki Scientific Reports, 2024, Volume 14, Issue 1, Pages 23460.</p> <p>Using AutoML and generative AI to predict the type of wildfire propagation in Canadian conifer forests    Khanmohammadi, Cruz, Perrakis, Alexander, Arashpour Ecological Informatics, 2024, Volume 82, Pages 102711.</p> <p>Machine learning applications on lunar meteorite minerals: From classification to mechanical properties prediction    Pe{~n}a-Asensio, Trigo-Rodr{'\\i}guez, Sort, Ib{'a}{~n}ez-Insa, Rimola International Journal of Mining Science and Technology, 2024.</p> <p>Data-Driven Prognostication in Distal Medium Vessel Occlusions Using Explainable Machine Learning    Karabacak, Ozkara, Faizy, Hardigan, Heit, Lakhani, Margetis, Mocco, Nael, Wintermark, others American Journal of Neuroradiology, 2024.</p>"},{"location":"integrations/UI/","title":"UI","text":""},{"location":"integrations/UI/#tabpfn-followups","title":"TabPFN Followups","text":"<p>Forecastpfn: Synthetically-trained zero-shot forecasting    Dooley, Khurana, Mohapatra, Naidu, White Advances in Neural Information Processing Systems, 2024, Volume 36.</p> <p>Interpretable machine learning for TabPFN    Rundel, Kobialka, von Crailsheim, Feurer, Nagler, R{\"u}gamer World Conference on Explainable Artificial Intelligence, 2024, Pages 465--476.</p> <p>Scaling tabpfn: Sketching and feature selection for tabular prior-data fitted networks    Feuer, Hegde, Cohen arXiv preprint arXiv:2311.10609, 2023.</p> <p>In-Context Data Distillation with TabPFN    Ma, Thomas, Yu, Caterini arXiv preprint arXiv:2402.06971, 2024.</p> <p>Tokenize features, enhancing tables: the FT-TABPFN model for tabular classification    Liu, Yang, Liang, Pang, Zou arXiv preprint arXiv:2406.06891, 2024.</p> <p>Towards Localization via Data Embedding for TabPFN    Koshil, Nagler, Feurer, Eggensperger NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Enhancing Classification Performance Through the Synergistic Use of XGBoost, TABPFN, and LGBM Models    Prabowo, others 2023 15<sup>th</sup> International Congress on Advanced Applied Informatics Winter (IIAI-AAI-Winter), 2023, Pages 255--259.</p> <p>The Tabular Foundation Model TabPFN Outperforms Specialized Time Series Forecasting Models Based on Simple Features    Hoo, M{\"u}ller, Salinas, Hutter NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabPFGen--Tabular Data Generation with TabPFN    Ma, Dankar, Stein, Yu, Caterini arXiv preprint arXiv:2406.05216, 2024.</p> <p>Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data    Helli, Schnurr, Hollmann, M{\"u}ller, Hutter arXiv preprint arXiv:2411.10634, 2024.</p> <p>TabFlex: Scaling Tabular Learning to Millions with Linear Attention    Zeng, Kang, Mueller NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Retrieval \\&amp; Fine-Tuning for In-Context Tabular Models    Thomas, Ma, Hosseinzadeh, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2406.05207, 2024.</p> <p>TabDPT: Scaling Tabular Foundation Models    Ma, Thomas, Hosseinzadeh, Kamkari, Labach, Cresswell, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2410.18164, 2024.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    Breejen, Bae, Cha, Yun arXiv preprint arXiv:2405.13396, 2024.</p> <p>MotherNet: Fast Training and Inference via Hyper-Network Transformers    Mueller, Curino, Ramakrishnan NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Mixture of In-Context Prompters for Tabular PFNs    Xu, Cirit, Asadi, Sun, Wang arXiv preprint arXiv:2405.16156, 2024.</p> <p>Fast and Accurate Zero-Training Classification for Tabular Engineering Data    Picard, Ahmed arXiv preprint arXiv:2401.06948, 2024.</p> <p>Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning    den Breejen, Bae, Cha, Kim, Koh, Yun NeurIPS 2023 Second Table Representation Learning Workshop, 2023.</p> <p>TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks    Feuer, Schirrmeister, Cherepanova, Hegde, Hutter, Goldblum, Cohen, White arXiv preprint arXiv:2402.11137, 2024.</p> <p>Exploration of autoregressive models for in-context learning on tabular data    Baur, Kim NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabMDA: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting    Margeloiu, Bazaga, Simidjievski, Li{`o}, Jamnik arXiv preprint arXiv:2406.01805, 2024.</p> <p>Large Scale Transfer Learning for Tabular Data via Language Modeling    Gardner, Perdomo, Schmidt arXiv preprint arXiv:2406.12031, 2024.</p> <p>AnnotatedTables: A Large Tabular Dataset with Language Model Annotations    Hu, Fountalis, Tian, Vasiloglou arXiv preprint arXiv:2406.16349, 2024.</p> <p>TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling    Gorishniy, Kotelnikov, Babenko arXiv preprint arXiv:2410.24210, 2024.</p> <p>Pre-Trained Tabular Transformer for Real-Time, Efficient, Stable Radiomics Data Processing: A Comprehensive Study    Jiang, Jia, Zhang, Li 2023 IEEE International Conference on E-health Networking, Application \\&amp; Services (Healthcom), 2023, Pages 276--281.</p> <p>TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik arXiv preprint arXiv:2409.16118, 2024.</p> <p>Augmenting Small-size Tabular Data with Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>FORECASTPFN: ZERO-SHOT LOW-RESOURCE FORECASTING    Khurana, Dooley, Naidu, White, AI No Source, No Year.</p> <p>What exactly has TabPFN learned to do?    McCarter The Third Blogpost Track at ICLR 2024, No Year.</p> <p>Statistical foundations of prior-data fitted networks    Nagler International Conference on Machine Learning, 2023, Pages 25660--25676.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    den Breejen, Bae, Cha, Yun arXiv e-prints, 2024, Pages arXiv--2405.</p>"},{"location":"integrations/UI/#tabpfn-application","title":"TabPFN Application","text":"<p>Large-scale chemoproteomics expedites ligand discovery and predicts ligand behavior in cells    Offensperger, Tin, Duran-Frigola, Hahn, Dobner, Ende, Strohbach, Rukavina, Brennsteiner, Ogilvie, others Science, 2024, Volume 384, Issue 6694, Pages eadk5864.</p> <p>Deep learning for cross-selling health insurance classification    Chu, Than, Jo 2024 International Conference on Green Energy, Computing and Sustainable Technology (GECOST), 2024, Pages 453--457.</p> <p>Early fault classification in rotating machinery with limited data using TabPFN    Magad{'a}n, Rold{'a}n-G{'o}mez, Granda, Su{'a}rez IEEE Sensors Journal, 2023.</p> <p>Artificial intelligence-driven predictive framework for early detection of still birth    Alzakari, Aldrees, Umer, Cascone, Innab, Ashraf SLAS technology, 2024, Volume 29, Issue 6, Pages 100203.</p> <p>Prostate Cancer Diagnosis via Visual Representation of Tabular Data and Deep Transfer Learning    El-Melegy, Mamdouh, Ali, Badawy, El-Ghar, Alghamdi, El-Baz Bioengineering, 2024, Volume 11, Issue 7, Pages 635.</p> <p>A machine learning-based approach for individualized prediction of short-term outcomes after anterior cervical corpectomy    Karabacak, Schupper, Carr, Margetis Asian Spine Journal, 2024, Volume 18, Issue 4, Pages 541.</p> <p>Comparing the Performance of a Deep Learning Model (TabPFN) for Predicting River Algal Blooms with Varying Data Composition    Yang, Park Journal of Wetlands Research, 2024, Volume 26, Issue 3, Pages 197--203.</p> <p>Adapting TabPFN for Zero-Inflated Metagenomic Data    Perciballi, Granese, Fall, Zehraoui, Prifti, Zucker NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Comprehensive peripheral blood immunoprofiling reveals five immunotypes with immunotherapy response characteristics in patients with cancer    Dyikanov, Zaitsev, Vasileva, Wang, Sokolov, Bolshakov, Frank, Turova, Golubeva, Gantseva, others Cancer Cell, 2024, Volume 42, Issue 5, Pages 759--779.</p> <p>Predicting dementia in Parkinson's disease on a small tabular dataset using hybrid LightGBM--TabPFN and SHAP    Tran, Byeon Digital Health, 2024, Volume 10, Pages 20552076241272585.</p> <p>Enhancing actuarial non-life pricing models via transformers    Brauer European Actuarial Journal, 2024, Pages 1--22.</p> <p>Machine learning-based diagnostic prediction of minimal change disease: model development study    Noda, Ichikawa, Shibagaki Scientific Reports, 2024, Volume 14, Issue 1, Pages 23460.</p> <p>Using AutoML and generative AI to predict the type of wildfire propagation in Canadian conifer forests    Khanmohammadi, Cruz, Perrakis, Alexander, Arashpour Ecological Informatics, 2024, Volume 82, Pages 102711.</p> <p>Machine learning applications on lunar meteorite minerals: From classification to mechanical properties prediction    Pe{~n}a-Asensio, Trigo-Rodr{'\\i}guez, Sort, Ib{'a}{~n}ez-Insa, Rimola International Journal of Mining Science and Technology, 2024.</p> <p>Data-Driven Prognostication in Distal Medium Vessel Occlusions Using Explainable Machine Learning    Karabacak, Ozkara, Faizy, Hardigan, Heit, Lakhani, Margetis, Mocco, Nael, Wintermark, others American Journal of Neuroradiology, 2024.</p>"},{"location":"integrations/local/","title":"Local","text":""},{"location":"integrations/local/#tabpfn-followups","title":"TabPFN Followups","text":"<p>Forecastpfn: Synthetically-trained zero-shot forecasting    Dooley, Khurana, Mohapatra, Naidu, White Advances in Neural Information Processing Systems, 2024, Volume 36.</p> <p>Interpretable machine learning for TabPFN    Rundel, Kobialka, von Crailsheim, Feurer, Nagler, R{\"u}gamer World Conference on Explainable Artificial Intelligence, 2024, Pages 465--476.</p> <p>Scaling tabpfn: Sketching and feature selection for tabular prior-data fitted networks    Feuer, Hegde, Cohen arXiv preprint arXiv:2311.10609, 2023.</p> <p>In-Context Data Distillation with TabPFN    Ma, Thomas, Yu, Caterini arXiv preprint arXiv:2402.06971, 2024.</p> <p>Tokenize features, enhancing tables: the FT-TABPFN model for tabular classification    Liu, Yang, Liang, Pang, Zou arXiv preprint arXiv:2406.06891, 2024.</p> <p>Towards Localization via Data Embedding for TabPFN    Koshil, Nagler, Feurer, Eggensperger NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Enhancing Classification Performance Through the Synergistic Use of XGBoost, TABPFN, and LGBM Models    Prabowo, others 2023 15<sup>th</sup> International Congress on Advanced Applied Informatics Winter (IIAI-AAI-Winter), 2023, Pages 255--259.</p> <p>The Tabular Foundation Model TabPFN Outperforms Specialized Time Series Forecasting Models Based on Simple Features    Hoo, M{\"u}ller, Salinas, Hutter NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabPFGen--Tabular Data Generation with TabPFN    Ma, Dankar, Stein, Yu, Caterini arXiv preprint arXiv:2406.05216, 2024.</p> <p>Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data    Helli, Schnurr, Hollmann, M{\"u}ller, Hutter arXiv preprint arXiv:2411.10634, 2024.</p> <p>TabFlex: Scaling Tabular Learning to Millions with Linear Attention    Zeng, Kang, Mueller NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Retrieval \\&amp; Fine-Tuning for In-Context Tabular Models    Thomas, Ma, Hosseinzadeh, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2406.05207, 2024.</p> <p>TabDPT: Scaling Tabular Foundation Models    Ma, Thomas, Hosseinzadeh, Kamkari, Labach, Cresswell, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2410.18164, 2024.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    Breejen, Bae, Cha, Yun arXiv preprint arXiv:2405.13396, 2024.</p> <p>MotherNet: Fast Training and Inference via Hyper-Network Transformers    Mueller, Curino, Ramakrishnan NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Mixture of In-Context Prompters for Tabular PFNs    Xu, Cirit, Asadi, Sun, Wang arXiv preprint arXiv:2405.16156, 2024.</p> <p>Fast and Accurate Zero-Training Classification for Tabular Engineering Data    Picard, Ahmed arXiv preprint arXiv:2401.06948, 2024.</p> <p>Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning    den Breejen, Bae, Cha, Kim, Koh, Yun NeurIPS 2023 Second Table Representation Learning Workshop, 2023.</p> <p>TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks    Feuer, Schirrmeister, Cherepanova, Hegde, Hutter, Goldblum, Cohen, White arXiv preprint arXiv:2402.11137, 2024.</p> <p>Exploration of autoregressive models for in-context learning on tabular data    Baur, Kim NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabMDA: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting    Margeloiu, Bazaga, Simidjievski, Li{`o}, Jamnik arXiv preprint arXiv:2406.01805, 2024.</p> <p>Large Scale Transfer Learning for Tabular Data via Language Modeling    Gardner, Perdomo, Schmidt arXiv preprint arXiv:2406.12031, 2024.</p> <p>AnnotatedTables: A Large Tabular Dataset with Language Model Annotations    Hu, Fountalis, Tian, Vasiloglou arXiv preprint arXiv:2406.16349, 2024.</p> <p>TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling    Gorishniy, Kotelnikov, Babenko arXiv preprint arXiv:2410.24210, 2024.</p> <p>Pre-Trained Tabular Transformer for Real-Time, Efficient, Stable Radiomics Data Processing: A Comprehensive Study    Jiang, Jia, Zhang, Li 2023 IEEE International Conference on E-health Networking, Application \\&amp; Services (Healthcom), 2023, Pages 276--281.</p> <p>TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik arXiv preprint arXiv:2409.16118, 2024.</p> <p>Augmenting Small-size Tabular Data with Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>FORECASTPFN: ZERO-SHOT LOW-RESOURCE FORECASTING    Khurana, Dooley, Naidu, White, AI No Source, No Year.</p> <p>What exactly has TabPFN learned to do?    McCarter The Third Blogpost Track at ICLR 2024, No Year.</p> <p>Statistical foundations of prior-data fitted networks    Nagler International Conference on Machine Learning, 2023, Pages 25660--25676.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    den Breejen, Bae, Cha, Yun arXiv e-prints, 2024, Pages arXiv--2405.</p>"},{"location":"integrations/local/#tabpfn-application","title":"TabPFN Application","text":"<p>Large-scale chemoproteomics expedites ligand discovery and predicts ligand behavior in cells    Offensperger, Tin, Duran-Frigola, Hahn, Dobner, Ende, Strohbach, Rukavina, Brennsteiner, Ogilvie, others Science, 2024, Volume 384, Issue 6694, Pages eadk5864.</p> <p>Deep learning for cross-selling health insurance classification    Chu, Than, Jo 2024 International Conference on Green Energy, Computing and Sustainable Technology (GECOST), 2024, Pages 453--457.</p> <p>Early fault classification in rotating machinery with limited data using TabPFN    Magad{'a}n, Rold{'a}n-G{'o}mez, Granda, Su{'a}rez IEEE Sensors Journal, 2023.</p> <p>Artificial intelligence-driven predictive framework for early detection of still birth    Alzakari, Aldrees, Umer, Cascone, Innab, Ashraf SLAS technology, 2024, Volume 29, Issue 6, Pages 100203.</p> <p>Prostate Cancer Diagnosis via Visual Representation of Tabular Data and Deep Transfer Learning    El-Melegy, Mamdouh, Ali, Badawy, El-Ghar, Alghamdi, El-Baz Bioengineering, 2024, Volume 11, Issue 7, Pages 635.</p> <p>A machine learning-based approach for individualized prediction of short-term outcomes after anterior cervical corpectomy    Karabacak, Schupper, Carr, Margetis Asian Spine Journal, 2024, Volume 18, Issue 4, Pages 541.</p> <p>Comparing the Performance of a Deep Learning Model (TabPFN) for Predicting River Algal Blooms with Varying Data Composition    Yang, Park Journal of Wetlands Research, 2024, Volume 26, Issue 3, Pages 197--203.</p> <p>Adapting TabPFN for Zero-Inflated Metagenomic Data    Perciballi, Granese, Fall, Zehraoui, Prifti, Zucker NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Comprehensive peripheral blood immunoprofiling reveals five immunotypes with immunotherapy response characteristics in patients with cancer    Dyikanov, Zaitsev, Vasileva, Wang, Sokolov, Bolshakov, Frank, Turova, Golubeva, Gantseva, others Cancer Cell, 2024, Volume 42, Issue 5, Pages 759--779.</p> <p>Predicting dementia in Parkinson's disease on a small tabular dataset using hybrid LightGBM--TabPFN and SHAP    Tran, Byeon Digital Health, 2024, Volume 10, Pages 20552076241272585.</p> <p>Enhancing actuarial non-life pricing models via transformers    Brauer European Actuarial Journal, 2024, Pages 1--22.</p> <p>Machine learning-based diagnostic prediction of minimal change disease: model development study    Noda, Ichikawa, Shibagaki Scientific Reports, 2024, Volume 14, Issue 1, Pages 23460.</p> <p>Using AutoML and generative AI to predict the type of wildfire propagation in Canadian conifer forests    Khanmohammadi, Cruz, Perrakis, Alexander, Arashpour Ecological Informatics, 2024, Volume 82, Pages 102711.</p> <p>Machine learning applications on lunar meteorite minerals: From classification to mechanical properties prediction    Pe{~n}a-Asensio, Trigo-Rodr{'\\i}guez, Sort, Ib{'a}{~n}ez-Insa, Rimola International Journal of Mining Science and Technology, 2024.</p> <p>Data-Driven Prognostication in Distal Medium Vessel Occlusions Using Explainable Machine Learning    Karabacak, Ozkara, Faizy, Hardigan, Heit, Lakhani, Margetis, Mocco, Nael, Wintermark, others American Journal of Neuroradiology, 2024.</p>"},{"location":"research/papers/","title":"Papers","text":""},{"location":"research/papers/#tabpfn-followups","title":"TabPFN Followups","text":"<p>Forecastpfn: Synthetically-trained zero-shot forecasting    Dooley, Khurana, Mohapatra, Naidu, White Advances in Neural Information Processing Systems, 2024, Volume 36.</p> <p>Interpretable machine learning for TabPFN    Rundel, Kobialka, von Crailsheim, Feurer, Nagler, R{\"u}gamer World Conference on Explainable Artificial Intelligence, 2024, Pages 465--476.</p> <p>Scaling tabpfn: Sketching and feature selection for tabular prior-data fitted networks    Feuer, Hegde, Cohen arXiv preprint arXiv:2311.10609, 2023.</p> <p>In-Context Data Distillation with TabPFN    Ma, Thomas, Yu, Caterini arXiv preprint arXiv:2402.06971, 2024.</p> <p>Tokenize features, enhancing tables: the FT-TABPFN model for tabular classification    Liu, Yang, Liang, Pang, Zou arXiv preprint arXiv:2406.06891, 2024.</p> <p>Towards Localization via Data Embedding for TabPFN    Koshil, Nagler, Feurer, Eggensperger NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Enhancing Classification Performance Through the Synergistic Use of XGBoost, TABPFN, and LGBM Models    Prabowo, others 2023 15<sup>th</sup> International Congress on Advanced Applied Informatics Winter (IIAI-AAI-Winter), 2023, Pages 255--259.</p> <p>The Tabular Foundation Model TabPFN Outperforms Specialized Time Series Forecasting Models Based on Simple Features    Hoo, M{\"u}ller, Salinas, Hutter NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabPFGen--Tabular Data Generation with TabPFN    Ma, Dankar, Stein, Yu, Caterini arXiv preprint arXiv:2406.05216, 2024.</p> <p>Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data    Helli, Schnurr, Hollmann, M{\"u}ller, Hutter arXiv preprint arXiv:2411.10634, 2024.</p> <p>TabFlex: Scaling Tabular Learning to Millions with Linear Attention    Zeng, Kang, Mueller NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Retrieval \\&amp; Fine-Tuning for In-Context Tabular Models    Thomas, Ma, Hosseinzadeh, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2406.05207, 2024.</p> <p>TabDPT: Scaling Tabular Foundation Models    Ma, Thomas, Hosseinzadeh, Kamkari, Labach, Cresswell, Golestan, Yu, Volkovs, Caterini arXiv preprint arXiv:2410.18164, 2024.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    Breejen, Bae, Cha, Yun arXiv preprint arXiv:2405.13396, 2024.</p> <p>MotherNet: Fast Training and Inference via Hyper-Network Transformers    Mueller, Curino, Ramakrishnan NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Mixture of In-Context Prompters for Tabular PFNs    Xu, Cirit, Asadi, Sun, Wang arXiv preprint arXiv:2405.16156, 2024.</p> <p>Fast and Accurate Zero-Training Classification for Tabular Engineering Data    Picard, Ahmed arXiv preprint arXiv:2401.06948, 2024.</p> <p>Fine-Tuning the Retrieval Mechanism for Tabular Deep Learning    den Breejen, Bae, Cha, Kim, Koh, Yun NeurIPS 2023 Second Table Representation Learning Workshop, 2023.</p> <p>TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks    Feuer, Schirrmeister, Cherepanova, Hegde, Hutter, Goldblum, Cohen, White arXiv preprint arXiv:2402.11137, 2024.</p> <p>Exploration of autoregressive models for in-context learning on tabular data    Baur, Kim NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>TabMDA: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting    Margeloiu, Bazaga, Simidjievski, Li{`o}, Jamnik arXiv preprint arXiv:2406.01805, 2024.</p> <p>Large Scale Transfer Learning for Tabular Data via Language Modeling    Gardner, Perdomo, Schmidt arXiv preprint arXiv:2406.12031, 2024.</p> <p>AnnotatedTables: A Large Tabular Dataset with Language Model Annotations    Hu, Fountalis, Tian, Vasiloglou arXiv preprint arXiv:2406.16349, 2024.</p> <p>TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling    Gorishniy, Kotelnikov, Babenko arXiv preprint arXiv:2410.24210, 2024.</p> <p>Pre-Trained Tabular Transformer for Real-Time, Efficient, Stable Radiomics Data Processing: A Comprehensive Study    Jiang, Jia, Zhang, Li 2023 IEEE International Conference on E-health Networking, Application \\&amp; Services (Healthcom), 2023, Pages 276--281.</p> <p>TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik arXiv preprint arXiv:2409.16118, 2024.</p> <p>Augmenting Small-size Tabular Data with Class-Specific Energy-Based Models    Margeloiu, Jiang, Simidjievski, Jamnik NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>FORECASTPFN: ZERO-SHOT LOW-RESOURCE FORECASTING    Khurana, Dooley, Naidu, White, AI No Source, No Year.</p> <p>What exactly has TabPFN learned to do?    McCarter The Third Blogpost Track at ICLR 2024, No Year.</p> <p>Statistical foundations of prior-data fitted networks    Nagler International Conference on Machine Learning, 2023, Pages 25660--25676.</p> <p>Why In-Context Learning Transformers are Tabular Data Classifiers    den Breejen, Bae, Cha, Yun arXiv e-prints, 2024, Pages arXiv--2405.</p>"},{"location":"research/papers/#tabpfn-application","title":"TabPFN Application","text":"<p>Large-scale chemoproteomics expedites ligand discovery and predicts ligand behavior in cells    Offensperger, Tin, Duran-Frigola, Hahn, Dobner, Ende, Strohbach, Rukavina, Brennsteiner, Ogilvie, others Science, 2024, Volume 384, Issue 6694, Pages eadk5864.</p> <p>Deep learning for cross-selling health insurance classification    Chu, Than, Jo 2024 International Conference on Green Energy, Computing and Sustainable Technology (GECOST), 2024, Pages 453--457.</p> <p>Early fault classification in rotating machinery with limited data using TabPFN    Magad{'a}n, Rold{'a}n-G{'o}mez, Granda, Su{'a}rez IEEE Sensors Journal, 2023.</p> <p>Artificial intelligence-driven predictive framework for early detection of still birth    Alzakari, Aldrees, Umer, Cascone, Innab, Ashraf SLAS technology, 2024, Volume 29, Issue 6, Pages 100203.</p> <p>Prostate Cancer Diagnosis via Visual Representation of Tabular Data and Deep Transfer Learning    El-Melegy, Mamdouh, Ali, Badawy, El-Ghar, Alghamdi, El-Baz Bioengineering, 2024, Volume 11, Issue 7, Pages 635.</p> <p>A machine learning-based approach for individualized prediction of short-term outcomes after anterior cervical corpectomy    Karabacak, Schupper, Carr, Margetis Asian Spine Journal, 2024, Volume 18, Issue 4, Pages 541.</p> <p>Comparing the Performance of a Deep Learning Model (TabPFN) for Predicting River Algal Blooms with Varying Data Composition    Yang, Park Journal of Wetlands Research, 2024, Volume 26, Issue 3, Pages 197--203.</p> <p>Adapting TabPFN for Zero-Inflated Metagenomic Data    Perciballi, Granese, Fall, Zehraoui, Prifti, Zucker NeurIPS 2024 Third Table Representation Learning Workshop, No Year.</p> <p>Comprehensive peripheral blood immunoprofiling reveals five immunotypes with immunotherapy response characteristics in patients with cancer    Dyikanov, Zaitsev, Vasileva, Wang, Sokolov, Bolshakov, Frank, Turova, Golubeva, Gantseva, others Cancer Cell, 2024, Volume 42, Issue 5, Pages 759--779.</p> <p>Predicting dementia in Parkinson's disease on a small tabular dataset using hybrid LightGBM--TabPFN and SHAP    Tran, Byeon Digital Health, 2024, Volume 10, Pages 20552076241272585.</p> <p>Enhancing actuarial non-life pricing models via transformers    Brauer European Actuarial Journal, 2024, Pages 1--22.</p> <p>Machine learning-based diagnostic prediction of minimal change disease: model development study    Noda, Ichikawa, Shibagaki Scientific Reports, 2024, Volume 14, Issue 1, Pages 23460.</p> <p>Using AutoML and generative AI to predict the type of wildfire propagation in Canadian conifer forests    Khanmohammadi, Cruz, Perrakis, Alexander, Arashpour Ecological Informatics, 2024, Volume 82, Pages 102711.</p> <p>Machine learning applications on lunar meteorite minerals: From classification to mechanical properties prediction    Pe{~n}a-Asensio, Trigo-Rodr{'\\i}guez, Sort, Ib{'a}{~n}ez-Insa, Rimola International Journal of Mining Science and Technology, 2024.</p> <p>Data-Driven Prognostication in Distal Medium Vessel Occlusions Using Explainable Machine Learning    Karabacak, Ozkara, Faizy, Hardigan, Heit, Lakhani, Margetis, Mocco, Nael, Wintermark, others American Journal of Neuroradiology, 2024.</p>"},{"location":"tutorials/cheat_sheet/","title":"Cheat Sheet / Best practices","text":"<p>Look at Autogluon cheat sheet [https://auto.gluon.ai/stable/cheatsheet.html]</p>"},{"location":"tutorials/classification/","title":"Classification","text":"<p>TabPFN provides a powerful interface for handling classification tasks on tabular data. The <code>TabPFNClassifier</code> class can be used for binary and multi-class classification problems.</p>"},{"location":"tutorials/classification/#example","title":"Example","text":"<p>Below is an example of how to use <code>TabPFNClassifier</code> for a multi-class classification task:</p> Python API Client (No GPU, Online)Python Local (GPU) <pre><code>from tabpfn_client import TabPFNClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train classifier\nclassifier = TabPFNClassifier(device='cuda', N_ensemble_configurations=10)\nclassifier.fit(X_train, y_train)\n\n# Evaluate\ny_pred = classifier.predict(X_test)\nprint('Test Accuracy:', accuracy_score(y_test, y_pred))\n</code></pre> <pre><code>from tabpfn import TabPFNClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train classifier\nclassifier = TabPFNClassifier(device='cuda', N_ensemble_configurations=10)\nclassifier.fit(X_train, y_train)\n\n# Evaluate\ny_pred = classifier.predict(X_test)\nprint('Test Accuracy:', accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"tutorials/classification/#example-with-autotabpfnclassifier","title":"Example with AutoTabPFNClassifier","text":"<p>Abstract</p> <p>The AutoTabPFNClassifier and AutoTabPFNRegressor automatically run a hyperparameter search and build an ensemble of strong hyperparameters. You can control the runtime using \u00b4max_time\u00b4 and need to make no further adjustments to get best results.</p> <pre><code>from tabpfn.scripts.estimator.post_hoc_ensembles import AutoTabPFNClassifier, AutoTabPFNRegressor\n# we refer to the PHE variant of TabPFN as AutoTabPFN in the code\nclf = AutoTabPFNClassifier(device='auto', max_time=30)\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict_proba(X_test)\ny_eval = np.argmax(preds, axis=1)\n\nprint('ROC AUC: ',  sklearn.metrics.roc_auc_score(y_test, preds[:,1], multi_class='ovr'), 'Accuracy', sklearn.metrics.accuracy_score(y_test, y_eval))\n</code></pre>"},{"location":"tutorials/distshift/","title":"Cheat Sheet / Best practices","text":"<p>Look at Autogluon</p>"},{"location":"tutorials/example_competitions/","title":"Cheat Sheet / Best practices","text":"<p>Look at Autogluon</p>"},{"location":"tutorials/regression/","title":"Regression","text":"<p>TabPFN can also be applied to regression tasks using the <code>TabPFNRegressor</code> class. This allows for predictive modeling of continuous outcomes.</p>"},{"location":"tutorials/regression/#example","title":"Example","text":"<p>An example usage of <code>TabPFNRegressor</code> is shown below:</p> Python API Client (No GPU, Online)Python Local (GPU) <pre><code>from tabpfn_client import TabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = TabPFNRegressor(device='auto')\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre> <pre><code>from tabpfn import TabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = TabPFNRegressor(device='auto')\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre> <p>This example demonstrates how to train and evaluate a regression model. For more details on TabPFNRegressor and its parameters, refer to the API Reference section.</p>"},{"location":"tutorials/regression/#example-with-autotabpfnregressor","title":"Example with AutoTabPFNRegressor","text":"<pre><code>from tabpfn.scripts.estimator.post_hoc_ensembles import AutoTabPFNRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport sklearn\n\nreg = AutoTabPFNRegressor(device='auto\u2019, max_time=30)\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg.fit(X_train, y_train)\npreds = reg.predict(X_test)\n\nprint('Mean Squared Error (MSE): ', sklearn.metrics.mean_squared_error(y_test, preds))\nprint('Mean Absolute Error (MAE): ', sklearn.metrics.mean_absolute_error(y_test, preds))\nprint('R-squared (R^2): ', sklearn.metrics.r2_score(y_test, preds))\n</code></pre>"},{"location":"tutorials/reproduction/","title":"Experimental Reproduction","text":"<p>Our code is currently stored in a private repository on GitHub, to avoid indexing at this point. We do not share this links on this public website. To access our code and any example notebooks, please use the notebook provided in the links to our submission via the \u201clinktree\u201d in our main paper or the code submission checklist. </p>"},{"location":"tutorials/timeseries/","title":"Cheat Sheet / Best practices","text":"<p>Look at Autogluon</p>"},{"location":"tutorials/unsupervised/","title":"Unsupervised functionalities","text":"<pre><code>from tabpfn.scripts.estimator import TabPFNUnsupervisedModel, TabPFNClassifier, TabPFNRegressor\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nmodel_unsupervised = TabPFNUnsupervisedModel(TabPFNClassifier(), TabPFNRegressor())\n\n# Load the Iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n\nmodel_unsupervised.fit(X_train, y_train)\nembeddings = model_unsupervised.get_embeddings(X_test)\nX_outliers = model_unsupervised.outliers(X_test)\nX_synthetic = model_unsupervised.generate_synthetic_data(n_samples=100)\n</code></pre>"}]}